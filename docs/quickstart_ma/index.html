<!DOCTYPE html>
<html>
<head>
<title>APT Multi-Animal Quick Start Guide</title>
<link rel="stylesheet" type="text/css" charset="utf-8" media="all"
href="../styles/common.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="screen"
href="../styles/screen.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="print"
href="../styles/print.css">
<link rel="stylesheet" type="text/css" charset="utf-8"
media="projection" href="../styles/projection.css">

<style type="text/css">
strong.regular-font {
  font-family: Arial, Lucida Grande, sans-serif;
  font-style: italic;
  font-size: 0.9em;
}
</style>

</head>

<body>
<h1><a href="index.html">APT</a> - Multi-Animal Quick Start Guide</h1>

<br>

<p>APT is an interactive animal pose estimation labeling and training application. In APT, users can train deep learning based pose estimation models to track multiple body landmarks of animals in videos. With its interactive and intuitive labeling interface, users can train highly accurate trackers with few labels.</p>
<p>Labels are the single most important factor in training accurate trackers. In fact, even a not so advanced algorithms can perform better than state-of-the-art algorithms if provided with more labels. However, simply labeling all examples is actually wasteful for training trackers. What is usually helpful is to label unusual or difficult examples. But how can <i>we</i> tell what is easy or difficult for the <i>tracker</i>? We cannot by simply looking at the examples. A practical approach is to train a tracker with the current set of labels, review its performance, and label the examples where it predicts incorrectely which are the difficult examples for it currently. By iteratively labeling and training, we will add increasingly more difficult labels to continuously improve the performance. </p>
<p>In this quick-start guide I'll create a tracker that tracks landmarks on individual flies in videos of freely moving and interacting flies. I'll be tracking four landmarks -- tip of the head, end of the abdomen, and the two wing tips.</p>

<hr class="h2-divider"/>
 <h2><a id="Setup">Setup</a></h2>
Before we get started, you'll have to setup APT. For this,
<ul>
<li> Install MATLAB with the <a href="index.html/#Requirements">required toolboxes</a>.
<li> Download APT from the <a href="https://github.com/kristinbranson/APT"> APT git repository</a>.
<li> Setup GPU on the <a href="LocalBackEnd.html">local backend</a> or any other <a href="index.html/#BackEnd">backends</a>. This guide uses local Docker backend but the steps are identical for all the backends.

<hr class="h2-divider"/>

<h2><a id="newproj">APT Project</a></h2>

To get started once done with the setup, start MATLAB and change directories to the location of your APT git repository directory. Launch APT from the MATLAB command line.
<br>
<br>
<center><a href="APTStart.gif"><img style='border:2px solid #000000' src="APTStart.gif" width="90%"></a></center>
<br/>
Start a new APT Project.
<br>
<!--
<br>
<center><a href="newproject.jpg"><img style='border:2px solid #000000' src="newproject.jpg" width="90%"></a></center>
<br/>
-->
<br>
<center><a href="new_ma_project.png"><img style='border:2px solid #000000' src="new_ma_project.png" width="20%"></a></center>
<br/>
<p>Name your project. I'm calling it <i>flyWings</i>. Select the number of points we will be tracking. In our case it is 4. Since the videos have multiple flies, select the option <b>Multiple Animals</b>  </p>
<br>
<p>A <i>Manage Movies</i> window will appear once you create the project. Add the movies you want to work with. If you have to add a large number of movies, you can do so using <b>File &rarr; Add List</b> in the <i>Manage Movies</i> window. The movie list file should have one movie per line. If you close the <i> Manage Movies</i> window, you can access it again using <b>File &rarr; Manage Movies</b> or <b>Go &rarr; Switch Movies</b>.</p>
<br>
<center><a href="addMovie.png"><img style='border:2px solid #000000' src="addMovie.png" width="40%"></a></center>
<br/>

<p>Once a movie is loaded, you can explore the movie in time by
<ul>
<li> Pressing the left or right arrow keys to navigate one frame backwards or forwards. Use Ctrl + arrow to jump by 10 frames at a time.
<li> Clicking on the timeline
<li> Using the play button below video on the left
<li> Sliding the frame slider
<li> Entering a frame number in the frame number box
</ul></p>

<p>You can zoom in and out in space using the mouse scroll buttons. You can use Ctrl + drag to pan in space. You can also zoom and pan in space using the magnifying glass and hand tools in the tool bar that is displayed when you hover over main labeling window. At any time, you can zoom back to the whole video using <b>View &rarr; Zoom out/full image(s)</b> or the <b>Ctrl + f</b> shortcut.</p>

<p>You can also modify the brightness, contrast and gamma correction for the videos using <b>View &rarr; Adjust brightness/contrast</b> and <b>View &rarr; Gamma Correct</b>. The playback speed can be adjusted using <b>Go &rarr; Navigation Preferences</b>.</p>
<br>
  <br>
  <center><a href="toolsHover.gif"><img style='border:2px solid #000000' src="toolsHover.gif" width="90%"><div style='width:"80%"; text-align:center;'>Hovering over video will bring up the toolbar.</div></a></center>
  <br>


<hr class="h2-divider"/>

<h2><a id="Labeling">Labeling</a></h2>

<p>Once you are familiar with navigation in space and time, we can start labeling. The labeling mode in multi-animal mode has 3 parts.
  <ul>
<li> <b>Start</b> Click on <b>New Target</b> button to initiate labeling.
<li> <b>Align</b> Roughly click the head and the tail location of the animal. This will  rotate and zoom the video so that the animal is oriented to face up. Orienting the animal in a standard direction helps in identifying the landmarks quickly, reduces the left-right ambiguity and labeling errors. This essentially saves you from craning your neck for each label. You can turn this feature off at any time by switching off <b>Two-click Animal Alignment</b> from the <b>Label</b> menu.
  <li> <b>Label</b> Click on the landmarks of the animal. If you happen to click the landmark in a wrong location, you can undo the label using <b>Ctrl + z</b>. Once you click on all the landmarks, the new label will be accepted.
</ul>
</p>
<br>

<center><a href="firstLabel.gif"><img style='border:2px solid #000000' src="firstLabel.gif" width="90%"></a></img></center>
<br/>

<p>After you label the first frame in a project, a preview of that animal with labeled landmark will be displayed in the <i>Reference frame</i> window in the upper-left corner of APT. This serves as a useful reminder about which label number corresponds to which body part to reduce labeling errors. You can change the labeled animal displayed in the reference frame by selecting the labeled animal and then clicking the <i>Freeze</i> button above the reference frame.</p>

<p>Once you label an animal, a bounding-box will be displayed around the animal and its label. The number displayed at the top-left corner of this bounding-box gives the <i>target number</i> or the label number for this animal. When you label multiple animals in a frame, this number is used to identify each animal's labels. You can select an animal's labels by clicking on the animal's number displayed on the bounding box. You can also select the animal's labels by clicking on the entry corresponding to its number in the <i>Targets</i> table on the left below the <i>Reference frame</i>.</p>
  <br>
  <center><a href="moreLabels.png"><img style='border:2px solid #000000' src="moreLabels.png" width="90%">
</a>
  </center>
  <br>

<p>Labels can be edited and deleted after they have been added. To edit or correct an existing label, first select the label and Then you can drag the landmark(s) to  their correct locations. You can also edit the selected label using the keyboard. First, select a landmark by typing that landmark's number and then use the arrow keys to move the landmark. Use shift key for larger movements. Typing the landmark's number again will end the editing to accept the labels. If you want to delete a label, click on the <b> Remove Target</b> button after selecting the label.</p>

<p>At times, you need to see the past or future frames to label a landmark.However, if you change frames in the middle of labeling a frame, the locations of the points you have clicked so far will be <b>lost</b>. To see temporal context in the middle of labeling, you can click the <a href="index.html#GUI"><b>Play context</b></a> buttons. Backward play context will show the frames before the current frame while the forward play context button will show the frames after the current frame but reversed so that for both the buttons you end up on the current frame in the end. </p>
<br>
<center><a href="labelWithContextCrop.gif"><img style='border:2px solid #000000' src="labelWithContextCrop.gif" type="video/mp4" width="90%"></a></center>
<br>
<p>You can label a point that is not clearly visible as <a href="index.html/#Occluded"><b>occluded</b></a> by holding the <b>shift</b> key while clicking. Alternatively, you can select the point and type 'o'. Occluded points are indicated with an 'o' marker. We recommend that you label a point in this manner if it is occluded but its location can be estimated. If a point is occluded and its location <b>cannot</b> be estimated or falls outside the video, then use the <b>View &rarr; Show occluded points box</b>. Instead of labeling within the video, click inside this rectangle and the point will be marked as heavily occluded. </p>
<br>
<center><a href="label_occluded_box.gif"><img style='border:2px solid #000000' src="label_occluded_box.gif" width="90%"><div style='width:"80%"; text-align:center;'>Labeling heavily occluded point. This example is only for illustration because we can estimate the location of the right wing in this case and hence shouldn't be labeled as heavily occluded.</div></a></img></center>
<br/>

<p>It is also useful to specify details of the landmarks like how to connect them to create a skeleton and which landmarks should be used as head and tail in order to align the animals. In APT you can do this using <b>Landmark Parameters</b> from the <i>Track</i> menu.</p>
<br>
<center><a href="landmarks.gif"><img style='border:2px solid #000000' src="landmarks.gif"width="50%"></a></img></center>
<br/>
<p>Details that you need to specify are:
<ul>
  <li><b>Skeleton</b> Skeletons are often helpful to percieve the labels and predictions faster and to disambiguate which landmarks belong to which animal when the animals are close. Also, certain algorithms like <i>OpenPose</i> need the skeleton to detect the animals. At times it might be advantageous to define an asymmetric skeleton to differentiate <i>e.g.</i> the left and right sides of an animal. <b>View &rarr; Show Skeleton </b> controls whether the skeleton is displayed or not.
<li> <b>Head and Tail landmarks</b> These are useful for certain algorithms and to align the animals when doing identity tracking. These landmarks need not necessarily be head or tail of the animal. These can be any two landmarks that can be used to align the animal.
<li> <a id="FlipLandmarkPairings"><b>Swap Pairs</b></a> Here you define the landmarks that are symmetric either along the horizontal or vertical body axis. These are used when augmenting the training set by mirroring the animal and its labels.
</ul>
 </p>
<!--<p>Another tool that can help find labeling errors is <b>Label &rarr; Label Overlay Montage</b> which superimposes all labels on a single image for review. Often a significant number of labeling errors will stand out as outliers in this tool. When you click on a label that is out of place, the movie and frame number of that label in displayed in the plot title. </p>
<br>
<center><a href="labelMontage.jpg"><img style='border:2px solid #000000' src="labelMontage.jpg" width="90%"></a></center>
<br/>
-->

<hr class="h2-divider"/>
<h2><a id="Mask">Labeling Select Animals</a></h2>
<p>When training a multi-animal tracker, all animals present in the frame need to be labeled. Any animal left unlabeled will confound the tracker because during training the tracker will try to predict that an animal at the location of the unlabeled animal exists. However, it would be penalized for doing so because based on the labels no animal exists at that location.</p>
<p>Labeling all the animals though can be extremely tedious particularly if you are tracking more than a few landmarks or have many animals. Additionally, the labeling all the animals also doesn't always provide useful information for training because most animal's poses in the frame are going to be easy for the tracker to predict.</p>

<p>To reduce this labeling burden, we allow users to label selected animals. in this mode, while training only the parts of the image within the <i>bounding-box</i> around the animal are exposed to the learning algorithm.</p>
<p>The drawback, however, is that since only the parts around labeled animal is shown, the tracker doesn't have any examples of what the background (parts that are <i>not</i>-animal) looks like and often ends up predicting animals where there are no animals present. To counter this, we provide <i>Label Box</i> labels to add background to the training set. Using Label-boxes, we can add rectangular regions of the frame to the training set that have <b><i>no un-labeled</i></b> animals. That is, label-boxes can enclose parts that have only background. Or, parts that have labeled animals. But they should <i>not</i> include any unlabeled animals. The label boxes can be added and edited using the <b>New Label Box</b> and <b>Edit Label Boxes</b> buttons. To delete a label box, enter the edit label box mode using <b>Edit Label Boxes</b> button, right click on the label box to bring up the context-menu, hit <b>Delete rectange</b>, and then quit the edit mode by clicking on the <b>Edit Label Boxes</b> button again. </p>

<p>An important point in this mode is that if you label an animal and that animal's bounding box <i><b>overlaps</b></i> significantly with another animal (i.e., covers more than 10-20% of the animal's body) then you should always label the other animal as well. The reason to do is same as earlier -- unlabeled animals or their parts that are visible during training data will result in algorithm to optimize incorrect loss.</p>

<br>
<center><a href="labelBox1Tick.png"><img style='border:2px solid #000000' src="labelBox1Tick.png" width="30%"></img></a>
  <a href="labelBox2Tick.png"><img style='border:2px solid #000000' src="labelBox2Tick.png" width="30%"></img></a>
  <a href="labelBoxWrongTick.png"><img style='border:2px solid #000000' src="labelBoxWrongTick.png" width="30%"></img></a></center>
<br/>

<p>To enable or disable this mode, select the <i>Unlabeled Animals Present</i> parameter from the <i>Tracking Parameters</i> window, that can be accessed using the <i>Set training parameters...</i> option from the <i>Tracking</i> menu. By default this mode is switched <b>on</b>. If for your project it is more convenient to label all the animals, do remember to disable this mode!</p>

<p>After finishing labeling your first frame, browse through the movie to label more frames. While browsing, skipping frames using the Ctrl + arrow keys is useful here. It is usually not a good idea to label immediately adjacent frames because they will be similar in appearance and won't give the learning algorithm much new information.</p>

<p>The frames that have been labeled for the current movie are listed in the <i>Labeled Frames</i> table on the left below the reference frame window. Clicking on any row in the table will take you to that frame.</p>

<!--
<br>
<center><a href="LabelMore.gif"><img style='border:2px solid #000000' src="LabelMore.gif" width="90%"></a></img></center>
<br/>
-->


<hr class="h2-divider"/>
<h2><a id="Saving">Saving and Loading</a></h2>

<p>Now that we have added a few labels, it is important to save the project so that it can recovered later. You can save the project by using <i>File &rarr; Save Project</i> or by hitting Ctrl+s. The project will be saved into <i>.lbl</i> file. When you load a saved project, APT will return to the same state when the project was saved. The saved project can also be shared across systems. However, the movie files are not stored in the project file. When you open the project on a new computer, APT will try to locate the movie files in the same location as the original computer. If it is unable to find it, it'll ask the user to locate the movies. APT shows the save status and other details of the project below the frame table.</p>
<hr class="h2-divider"/>
<h2><a id="Training">Training</a></h2>

<p>Now that we have saved the project, we can train! We suggest labeling at least 20-30 animals in at least 10-15 frames before you train your first tracker. One of APT's advantages is that APT provides multiple cutting-edge algorithms for training and users can seamlessly switch between them. You can switch between algorithms by selecting them from <b>Track &rarr; Tracking Algorithm</b>. The default <b>Multianimal GRONe</b> algorithm trains accurate trackers, is fast, and efficient in terms GPU memory and is a good algorithm to start with.</p>
<p>Before we train we need to set up the GPU backend. For now, we will select the Docker backend. For Windows, you would select Conda. After you select the backend, test it using <b>Test Backend Configuration</b>.</p>
<br>
<center><a href="testBackend.gif"><img style='border:2px solid #000000' src="testBackend.gif" width="90%"></a></img></center>
<br/>

<p>We also need to set the training parameters. Most deep learning based algorithms require users to set a lot of parameters, which is often difficult for non machine learning experts (in all honesty, they are difficult to set for practictioners as well).  APT makes it easy for users to intuitively to set these and others parameters. Users can access the parametes using <i>Tracking Parameters</i> window using <b>Track &rarr; Set training parameters</b>. </p>
<p>Most of the parameters that usually need to be set are related to updating the training data augmentation parameters. Deep learning algorithms overfit and fail to track accurately on new video frames if the same inputs, or a relatively narrow range of inputs, are presented to it during learning. In such cases, the algorithm can fail to learn invariances in the data. For movies capturing a top-down view of a freely-behaving mouse, for example, an algorithm must learn to track the mouse in any orientation, even if the training data only consists of mice labeled in certain oreintations.
To address this issue, training images are augmented by rotating, translating and applying other image processing techniques by random amounts that modify the image while preserving its visual information. To reduce user's effort and confusion on what the optimal values are, APT estimates these parameters based on the user's labels. As user add more labels, these estimates can change. When they do change, APT informs and asks the users whether to do update the parameters or not. Usually, you should let APT use the updated parameters.</p>
<br>
<center><a href="parameterUpdate.png"><img style='border:2px solid #000000' src="parameterUpdate.png" type="image" width="30%"></a></center>
<br/>

<p>APT also makes reduces the complexity of setting the parameters by categorizing the parameters by levels of expertise that users should have to modify them. You can select the levels from the drop-down menu at the bottom of the Parameters window. By default, it'll show only the important parameters that should be usually set by the users.</p>

<p>The most important set of parameters you need to set are related to GPU memory usage. They are important because training can crash if the memory required for training is more than the available GPU memory. The GPU memory required during training is proportional to the size of the image used for training (i.e. the number of pixels in the input image) and the batch size. You can control the image size used during training by setting the <i>Downsample factor</i> which specifies the degree of downsampling applied to video frames. When this parameter is selected, the <i>Parameter Visualization</i> displays the estimated GPU memory required for varying levels of downsampling. Select a downsampling factor such that the required GPU memory is less the memory available on your GPU. Note that downsampling by a large amount can impact tracking accuracy because heavily downsampled images can lose essential details in the images. </p>
<p>The other parameter which impacts GPU memory usage is the <i>Training batch size</i>. The GPU memory required scales linearly with batch size. However, a lower batch size can degrade tracking accuracy and we suggest keeping the batch size as large as possible. The GPU memory usage shown is an estimate; the actual GPU memory usage may be higher or lower. The surest way to know if you can train is to actually train a network. If training crashes, your project is unaffected. You can just update the parameters and train again.</p>
<br>
<center><a href="trainingParamsGPUMem.png"><img style='border:2px solid #000000' src="trainingParamsGPUMem.png" width="90%"></a></center>
<br>

<p>The other important parameters are:</p>
<li> <b>Flip Horizontally</b> - Augment by flipping the image and labels vertically (left to right). Use this if the animal is symmetric along the vertical axis. If selected, remember to set the <a href=index.html/#FlipLandmarkPairings> swap landmark pairs</a>. Note that only one of <i>Flip Horizontally</i> or <i>Flip Vertically</i> (below) should be set.
<li> <b>Flip Vertically</b> - Augment by flipping the image and labels horizontally (top to bottom). Use this if the animal is symmetric along the horizontal axis. If selected, remember to set the <a href=index.html/#FlipLandmarkPairings> swap landmark pairs</a>.  Note that only one of <i>Flip Horizontally</i> or <i>Flip Vertically</i> should be set.
<li> <b>Minimum number of animals</b> - The minimum number of animals that will always be present in any frame of the video. More precisely, though, it is the minimum number of detections that you would like the tracker to return for each frame. The difference is clearer if you consider a case where one animal could completely occlude another animal. In this case, it would be better if the tracker returned one less detection than the total number of animals. This isn't used for training and is used only for tracking.
<li> <b>Maximum number of animals</b> - The maximum number of animals that can ever be present in any frame of the video. Though again, more precisely, it is the maximum number of detections that you would like the tracker to return for each frame. For this case, there isn't any difference between the two definitions.
</ul>
<p>Of course, we have already talked about <i>Unlabeled animals present</i> parameter. Ensure it is set appropriately. <!--More details on the augmentation parameters and other training parameters is available in the <a href=index.html/#Configure tracking parameters>documentation</a></p>. -->

<!--
<p>The next set of important parameters to consider are the data augmentation parameters. Deep learning algorithms overfit and fail to track accurately on new video frames if the same inputs, or a relatively narrow range of inputs, are presented to it during learning. In such cases, the algorithm can fail to learn invariances in the data. For movies capturing a top-down view of a freely-behaving mouse, for example, an algorithm must learn to track the mouse in any orientation, even if the training data only consists of mice labeled in certain oreintations.</p>

<p>To address this issue, training images are augmented by rotating, translating and applying other image processing techniques by random amounts that modify the image while preserving its visual information. To help you set these parameters, APT provides a unique visualization of the data augmentation that will be applied during training. When you select or update any of the augmentation parameters (listed below), the UI displays a set of sample training images generated using the current augmentation parameters.</p>
<br>
<center><a href="trainVisualization1.jpg"><img style='border:2px solid #000000' src="trainVisualization1.jpg" type="video/mp4" width="90%"></a></center>
<br/>

<p>The goal while setting the augmentation parameters should be to set them such that the augmented images look like the images that will be encountered during tracking (or a slightly distorted version of them). For example, if the rotation range is set to 100, many images of vertical horses are generated. We do not need a tracker that predicts well on such images because we are not going to encounter such examples during tracking. The rotation range should therefore be set to a smaller value. </p>
<br>
<center><a href="trainVisualization_high.jpg"><img style='border:2px solid #000000' src="trainVisualization_high.jpg" type="video/mp4" width="90%"></a></center>
<br/>

<p>Note that to compute the visualization, APT has to communicate with the backend and as a result the visualization can be slow to update. If you change the parameters while the visualization is being updated, the visualization may not reflect the new parameters.</p>

<p>While the default augmentation parameters will usually work well, it is a good idea to adjust them to get the best performance out of your tracker. More information about data augmentation in APT can be found <a href="index.html/#Configure tracking parameters">here</a>. The important data augmentation parameters (in brief) that you can set in APT are:

<ul>
<li> Rotate Range - The range in degrees through which the image could be rotated.
<li> Translation Range - The range in pixels that the image could be translated in the x and y directions (independently).
<li> Scale factor Range - The range by which the image could be scaled.
<li> Brightness Range - The range by which to modify the image's brightness.
<li> Contrast Range - The range by which to modify the image's contrast.
<li> Flip Horizontally - Augment by flipping the image and labels vertically (left to right). Use this if the animal is symmetric along the vertical axis. If selected, remember to set the <a href=index.html/#FlipLandmarkPairings> flip landmark pairs</a>.
<li> Flip Vertically - Augment by flipping the image and labels horizontally (top to bottom). Use this if the animal is symmetric along the horizontal axis. If selected, remember to set the <a href=index.html/#FlipLandmarkPairings> flip landmark pairs</a>.
</ul>
More details on the augmentation parameters and other training parameters is available in the <a href=index.html/#Configure tracking parameters>documentation</a>.
-->

<p>Now it is time to train! Hit the train button! After you start the training, APT processess the training images and launches the <a href="index.html/#Training Monitor"><i>Training Monitor</i></a> window which updates you on the training status. The <i>Training Monitor</i> will display what stage the training is in (Initialization, building the image database or training) and if it is training it will show the current training iteration and the loss. For some algorithms it will also display the prediction accuracy as the average distance in pixels between the predicted landmarks and the labeled landmarks. The duration of training depends on the number of iterations and the size of the input image.</p>
<br>
<center><a href="trainMonitor.png"><img style='border:2px solid #000000' src="trainMonitor.png" type="video/mp4" width="50%"></a></center>
<br/>
<p>While you wait for training to finish, you can use the pop-up menu at the bottom of the <i>Training Monitor</i> to look at various other information about the training process. An important option is <i>Show sample training images</i> which shows the samples of images that are used for training with the current data augmentation parameters, masking to use only the labeled animals for training and cropping to use smaller patches instead of whole images. This option is a good sanity check that the training data is being generated properly. In the sample images, the blue overlay shows the mask that is used when computing the loss. And at the top the first two numbers indicate the movie number and the frame number of the example that was used to generate the training sample.</p>
<br>
<center><a href="trainingSamples.png"><img style='border:2px solid #000000' src="trainingSamples.png" type="video/mp4" width="90%"></a></center>

<p>Training will usually take a bit of time. You can however keep busy and continue labeling. The labels though will not get used for the current training. You can also <a href=index.html/#Tracking>Track</a> even before training ends if you have multiple GPUs. APT will use the latest model that has been saved for tracking. Usually, the model that have been trained for less than 5000-10000 iterations will have very poor performance, so it isn't a great idea to use them for tracking.</p>

<p>At the end of training APT will be asked to save the project to ensure that you don't end up losing the model accidentally after waiting for so long for it to train.</p>
<!--
<br>
<center><a href="trainEnd.png"><img style='border:2px solid #000000' src="trainEnd.png" type="video/mp4" width="90%"></a></center>
<br/>
-->
<p>You can change the Tracking Algorithm at any stage. When you do this, you don't lose your previously trained tracker. For example, if you train an GRONe tracker, and then change the algorithm to Openpose and train a new tracker, the GRONe tracker will still be a part of your project. You can access it at any time by re-selecting GRONe in the <i>Tracking Algorithm</i> menu. However, if you train an GRONe tracker and then re-train a new GRONe tracker, the old GRONe tracker will be replaced and will not be available. Information about the currently active model is displayed on the left above the Clear button.</p>

<hr class="h2-divider"/>

<h2><a id="Tracking">Tracking and Relabeling</a></h2>

<p>Time to track! Use the <i>Frames to track</i> pop-up menu below the Track button to select the frames you want to track. Usually it is a good idea to track just a few frames at first. To do this, select the <i>+/-100 fr</i> option which will track the 200 closest frames around the current frame. </p>
<br>
<center><a href="framesPopup.png"><img style='border:2px solid #000000' src="framesPopup.png" type="video/mp4" width="90%"></a></center>
<br/>

<p>Similar to training, once you start tracking, a <i>Tracking Monitor</i> will be launched that will update you about the tracking job's status. The main indicator is the colored bar in the middle that will inform you about the number of frames have been tracked.</p>
<br>
<center><a href="trackEndCrop.png"><img style='border:2px solid #000000' src="trackEndCrop.png" type="video/mp4" width="50%"></a></center>
<br/>
<br>
<p>If tracking results aren't visible once tracking finishes, confirm that <b>View &rarr; Hide Predictions</b> is unset.</p>
<p>Now you can start exploring how the tracker is performing by moving around the video. Since we labeled only a few frames in this demo, the tracker is likely to make quite a few mistakes while still performing pretty well overall. In the initial stages of a project, where only a few frames have been labeled, you'll find a lot of frames where tracking fails. These are the frames that you should label to improve your tracker.</p>
<br>
<center><a href="firstResultsCrop.gif"><img style='border:2px solid #000000' src="firstResultsCrop.gif" type="video/mp4" width="90%"></a></center>
<br/>
<p>When reviewing the results, there are 4 types of detection errors that you will encounter. In the first type, the animal is not detected at all while in the second type is that the animal is detected but its pose is incorrect. The third is that there are multiple predictions for the same animal. For these three types of errors, you should label the animal. The fourth type of error is false positive detection i.e., an animal is predicted where there is no animal. To correct this error, in case you are not labeling all the animals in the frame, you should add a <i>label box</i> to enclose the false detection. It is better to have a label box that liberally covers the false detection rather than a snug one. If you are labeling all the animals in the frame, then you should just label all the animals in the current frame so that the current frame will get used for training in the next round.</p>
<br>
<center><a href="predErrorLabeledCrop.png"><img style='border:2px solid #000000' src="predErrorLabeledCrop.png" type="video/mp4" width="90%">
<div style='width:"80%"; text-align:center;'>Add label boxes to correct false positive errors.</div></a>
</center>
<br>
<p>To reduce the labeling burden, you can add any prediction as label by clicking on that animal's trajectory and hitting Ctrl-l (l for label). This is handy for many instances. If only a few landmarks are incorrect for an animal, you can add the prediction as a label and then correct the landmarks (use of arrow keys to correct the labels will reduce your carpal-tunnel stress). Or, if the animal that you labeled overlaps with another animal that has correct prediction then you can add the prediction on the overlapping animal as label. Or, if you are labeling all the animals in a frame, then you manually label only the animals that have incorrect predictions and for the rest add the predictions as labels.</p>
<br>
<center><a href="relabel_ctr_l.gif"><img style='border:2px solid #000000' src="relabel_ctr_l.gif" type="video/mp4" width="90%">
<div style='width:"80%"; text-align:center;'>Add overlapping animal's predictions as predictions.</div></a></center>
<br/>
<p>Once you have corrected a good number of errors (around 50-60) you can train again. When you start training, APT will estimate some of the training parameters again and if they are significantly different than previous values, it'll ask you if the new ones should be used for training. It is recommended to use the new ones. And if there is a previously trained tracker for the selected algorithm, you'll be asked whether you want use that model for initialization. It is recommended to use the always use the previous model as this will train a more accurate tracker.</p>
<p>If there are frames that were tracked using the previous model, APT will delete these tracking results when training a new tracker as keeping them around could create confusion. APT will ask whether you want the frames previously tracked to be tracked again once the training is over.</p>
<br>
<center><a href="retrain_quest.png"><img style='border:2px solid #000000' src="retrain_quest.png" type="video/mp4" width="90%"></a></center>
<br>
<p>Deep learning algorithms will learn a reasonable tracker with few labels. However, the accuracy requirement for a usable tracker is much higher. Suppose you have a reasonably good tracker that has a 5% error rate and if you were to track a video that has a frame rate of 60 frames per second (FPS), you will end up with an error rate of 3 errors per second! </p>

<p>To get an accurate tracker that is useful in practice, you'll have to iteratitvely label and train. And the more diverse the data you label, the more robust the tracker will be. For this, you should review a wide variety of movies with many differing conditions like lighting, genetic lines, or camera angles. You wouldn't trust a self-driving car that is trained and tested on California roads to work well in Europe, can you?</p>

<p>In APT's interactive framework of labeling, the training set that is created by correcting the errors, is as powerful to the training set we would have got if we had labeled every frame that was reviewed while correcting the errors. In fact it is more powerful. In the normal case, when every frame is labeled, very little is achieved in most training iterations as the tracker already knows how to predict on the examples used for training. The interactive labeling framework thus reduces the number of training iterations required as well by ensuring most training examples provide new information.</p>

<hr class="h2-divider"/>

<h2><a id="Identity Tracking">Identity Tracking</a></h2>

<p>Tracking in a multi-animal project has 2 steps - 1) Detection, and 2) Linking. We have so far concentrated on the detection step, in which we detect the animals and their pose in each frame. The linking of the individual detections into trajectories is done in the linking step.</p>
<p>Linking in APT also has 2 steps. The first step is called tracklet linking and in this linking step we create tracklets by connecting detections in in adjacent frames only if there is very little ambiguity. That is if there is any doubt that detections in adjacent frames might come from different animals we won't link them. What sort of doubts? Doubts like if there is a sudden large movement or if two animals are close to each other and it is hard to tell which detections belong to which animal. Detections also are not linked if there is a missing detection in the adjacent frames. In the second <i>linking</i> step these tracklets are stitched to produce longer trajectories.</p>

<p>When we track using the <i>Track</i> button we always show the results of this linking step. There are three reasons for this. First, the <i>Track</i> button is for quick review of the tracking and waiting the second step will slow it down. Second, in the <i>linking</i> step in order to produce longer useful trajectories, some detections might get deleted. This can create confusion, because now we don't know whether an animal is missing due to an error during detection or because of linking stage optimization. And finally, as the breaks in the tracklets are likely to be caused by incorrect detections, they can be handy for finding difficult examples to label. The breaks are also useful to get a sense of all the instances where linking is likely to go wrong. </p>

<p>To stitch the tracklets in the the second stage of linking, APT provides two options: 1) fast and easy method that uses animal's motion, or 2) slower but more accurate method that uses animal's identity. The second method is far more robust to trajectory switches from one animal to another in which we train a deep-learning network to tell each invididual animals apart similar to the way face recognition is used to identify humans. By default, identity tracking is switched off and tracklets are stitched using motion.</p>
<p>To enable identity tracking, open <i>Tracking Parameters</i> from <i>Tracking</i> menu and then select <i>Identity Linking</i> and thats it. You do not have add any new type of labels! The training data set for training the identity detector is derived using the tracklets. The core network of identity detector compares a pair of images and predicts whether they belong to the same animal or not. To train the network, we give images from same tracklet as examples of pairs that are of the same animal. Images from overlapping tracklets are given as negative examples <i>i.e.</i>, pairs that do not belong to the same animal.</p>
<p>To train a good identity detector, it is necessary that the most tracklets be <b>not short</b>. The ideal length of course varies from dataset to dataset but identity tracking will work best if the tracklets are longer than 100 frames in general. So to get a useful identity tracker it is necessary to get the detection stage working well to ensure that the tracklets are of sufficient length.</p>
<p>The second stage is applied when you track the whole movie or movies using the options (<i>Track current movie, Track multiple videos, Track all videos in project</i>) in the <i>Track</i> menu. When you use these tracking options, the results are stores in <i>.trk</i> files. Once the tracking completes, you can load the results using <i>File &rarr; Import/Export &rarr; Import predictions from Trk file (current movie)</i>.</p>
<p>The results that are loaded are called <i>imported</i> predictions and they will be shown alongwith the predictions that you may already have from using the <i>Track</i> button. You can hide the imported and/or normal predictions using the options in the <i>View</i> menu as displaying both can create clutter.</p>
<!--
<br>
<center><a href="relabel_updated.gif"><img style='border:2px solid #000000' src="relabel_updated.gif"width="90%"></a></img></center>
<br/>
-->

<hr class="h2-divider"/>
<h2><a id="MovieTracking">Batch Tracking and Reviewing</a></h2>

<p>APT makes it easy to add a lot of movies to the project and review the tracking on them. You can track the current movie (<b>Track &rarr; Track current movie</b>) or all the movies in the current project (<b>Track &rarr; Track all movies in project</b>). You can also track videos without adding them to the project (<b>Track &rarr; Track Multiple Videos</b>) and review the tracking later. To make it easy to explore and find interesting frames, APT provides a <a href="index.html/#GUI">property timeline</a> which is located below the main video labeling window. Using the property selector pop-up menu you can display different properties of the tracking results and use them to find interesting frames. For example, if you select velmag (velocity magnitude) the property timeline will show how much each landmark moved in the current frame compared to the previous frame. This will help you in finding frames where there are sudden jumps in a landmarks location, which could be caused by tracking errors. You can also jump to frames where the velmag value exceeds a specified threshold using the Shift + arrow keys. To do this, go to (<b>Go &rarr; Navigation Preferences</b>) and set the desired threshold value, along with setting <i>Shift-left/right seeks</i> to <i>Next where timeline stat exceeds</i> and the appropriate comparison operator (> or <). Another property that is useful is the trackers confidence. You'll often find that frames that have tracking errors have low confidence. You can view the confidences for the predictions by selecting <i>conf_mdn</i> for the MDN tracker.</p>

<hr class="h2-divider"/>
<h2><a id="topdown">Faster Tracking with Two-stage Methods</a></h2>
<p>Tracking speed is directly proportional to the number of pixels that are input into the deep-learning networks. To speed up tracking using multi-animal pose estimation algorithms like GRONe we can downsample the input. However, there is a limit to how much the frames can be downsampled because downsampling heavily will make it difficult for the algorithm to localize the landmarks accurately. The main cause of this limit on downsampling is that these algorithms detect the animals as well as localize the landmarks at the same time. For datasets where animals occupy a small part (less than 1/3 or 1/4) of the video, it is possible to speed up tracking by separating animal detection from landmark localization into different stages. In this setup, the animals are first detected on low resolution downsampled images and the landmarks are then localization at high resolution but only on cropped patches around the detections. The landmarks are localized in the second stage using a single animal pose estimation algorithm. This two stage process reduces the number of pixels that are processed by the deep learning networks to speed up the tracking.</p>

<p>APT provides two classes of methods to detect animals in the first stage - 1) Object detection based and 2) Head-tail estimation based. In the object detection based methods, the first stage algorithm output bouding boxes around all the detected animals. Patches centered around the bounding boxes are then input to the second stage pose estimation algorithm to get the landmarks for all the animals.
</p>
<p>In head-tail estimation based detection methods, we detect only <a href="index.html/#landmarks.gif">the head and the tail landmarks</a> for all the animals in the first stage using multi-animal pose estimation algorithms. In the first stage since we mainly want to detect the animals, we don't have to localize the head and the tail landmarks accurately and hence the input can be downsampled heavily to speed it up. In the second stage, we crop patches around the detected head and tail landmark pairs. We also use the landmakrs to orient the patch so that the animal always face in the same direction. These patches are then input to the single animal pose estimation algorithm to predict the landmarks.</p>
<p>For the both the classes of methods, the second stage is similar except that when using object detection the animals are not oriented in the same direction while they are when we do head and tail detection. Whether the animals are orientied or not didn't affect the performance in our experiments.</p>
<p>The main difference between the two classes of methods are the likely source of errors. For object detection based methods, if two animals are very close to each other, their bounding boxes will overlap significantly. As a result the when patches are cropped for the second stage, they are likely to include both the animals and the second stage might predict landmarks for the same animal for both detections, or may predict a mix of landmarks from both the animals. The head tail based detection method alleviates the problem in most cases because the second stage is trained to predict the landmarks only for animals that are oriented in a certain direction.</p>
<p>The drawback of head-tail based methods is that if an animal's head or tail landmark isn't visible at all, it might not a detect an animal altogther. Also, it requires that you label the head and the tail landmarks which may not be the landmarks you want to track.</p>
<p>And while using these methods, speeds up tracking, the common drawback is that training could be slower because two networks for each stage have to be trained. However, for our datasets we observed that training is not significantly slower.</p>
<p>To select object detection based two-stage methods, you need to select <i>Top-down (bbox): Object detection + GRONe</i> algorithm from the <i>Track &rarr; Tracking algorithm</i> menu. The bbox in the algorithm name indicates that we are detecting the bounding-boxes in the first stage. To use head and tail based detection method, you need to select <i>Top-Down (head-tail): Multianimal GRONe + GRONe</i> algorithm. These two stage methods are called top-down methods in computer vision because we first detect top-level objects (<i>i.e.,</i> the animals) and then find the finer details (<i>i.e.,</i> the landmarks). For both the methods, we use the GRONe algorithm that has been adapted for single animal pose estimation for the second stage. For object detection, we use DeTR object detection algorithm while for head and tail detection, we use multi-animal GRONe as the first stage algorithm.</i>
<p>When using two-stage algorithms, the training parameters GUI shows parameters for both the stages. The parameters for the first-stage are under <b>Deep Learning (Detection)</b> while the parameters for second-stage are under <b>Deep Learning (Pose)</b>. To speed up tracking downsampling for the detection stage should be set to a high value like 2 or 4 while for the second stage should be set to 1 or 2 depending upon the GPU memory usage. If you desire higher landmark tracking accuracy you can even set the downsampling to 0.5 for second stage if GPU memory usage permits it. To determine the downsample that will best work for the first stage, you will have to balance the trade-off between animal detection accuracy and speed which could require a bit of trial and error.</p>

<br>
<center><a href="twoStageParametersEdited.png"><img style='border:2px solid #000000' src="twoStageParametersEdited.png" type="video/mp4" width="30%"></a></center>
<br>
<p>Switching and trying different algorithms is extremely easy in APT. We suggest starting out with multi-animal pose estimation algorithm like GRONe to begin with and once you have a tracker that is close to usable, you can switch to two-stage networks to speed up the tracking. Of course, once you switch algorithms you should again review the performance because different algorithm's find different instances to be difficult.</p>

<hr class="h2-divider"/>
<h2><a id="conclusion">Training your own trackers</a></h2>

<p>This introductory guide illustrates how APT provides a straight-forward, convenient and flexible interface to train your own trackers. We highly recommend that you read the <a href="index.html/#How to">How to train a part tracker</a> guide and <a href="index.html">APT documentation</a> for further details.</p>
<p>It is important to recongnize that the trackers are machine-learning models and have their shortcomings. The most important being that they are extremely complex and are are like a black-box. There are no good answers to why they fail or on what cases they'll fail. Other than reviewing a lot of data, the other useful way to counter this is to design the experiments or analyze the data in such a way that they are robust to some amount of error.
</p>
<p>Equally important but a subtle point is that machine learning models are designed for <i>prediction</i>. That is, they can <i>at best</i> do what humans can do but can do so at much larger scale. It is unreasonable to assume to expect them to do tasks where humans fail. For instance, if two animals are so entwined the even we cannot tell which landmark belongs to which animal then it is unlikely that the trained model will do any better.
</p>

<hr class="h2-divider"/>

<h2><a id="Useful Tools">Useful Tools in APT</a></h2>
<!--<h3><a id="groundtruth">Evaluating Performance</a></h3>
Reviewing tracking results on a large number of the videos is definitely the most useful way to know how the tracker is performing. But the reviewing is inevitably subjective and can bias you about your trackers performance. To provide a more objective measure of the tracker's performance, APT provides multiple way to <a href="index.html/#Evaluating performance">evaluate the tracker's performance</a>.
-->
<h3><a id="shortctus">Keyboard Shortcuts</a></h3>
APT has a host of keyboard shortcuts to speed up the labeling. You can access the list using <b>Help &rarr; Labeling Actions </b>.
<!--<h3><a id="switch targets">Labeling Details</a></h3>
APT provides the details about the number of labels alongwith other important details for each movie in <b>Go &rarr; Switch targets</b>
-->
<h3><a id="track multiple">Tracking multile movies</a></h3>
You can track a large number of movies using <b>Track &rarr; Track multiple videos </b>. You can load a list of movies using the <i>Load</i> button. And you can also define the trk output file names using macros $name$, $dir$ and $path$. For example, if the movie name is "/example/path/to/movie.avi", then $name$ will be "movie", $dir$ will be "to" and $path$ will be "/example/path".
<h3><a id="export labels">Exporting and Importing Labels</a></h3>
Labels can be <a href="index.html/#Exporting manual labels">imported and exported</a> as trk files or mat files using <b>File &rarr; Import/Export</b>.
<h3><a id="cosmetics">Change Label appearance</a></h3>
Label's appearance like color and marker can be changed using <b>View &rarr; Landmark Cosmetics</b>.
<h3><a id="command line">Command line tools</a></h3>
APT exposes a rich command line API. To access these launch APT using "lObj = StartAPT;". The lObj is the handle to the Labeler object defined in <i>APT/matlab/Labeler.m</i>. Users can update and modify this object to interact with APT.
<h3><a id="property timeline">Property Timeline</a></h3>
Users can display different properties of Labels, Predictions, and Imported tracking in the Property timeline. The property to be displayed can be selected using the two pop-up menus at the bottom right.

<footer>
<hr class="h1-divider">
<center>
<a href="index.html">APT Documentation Home</a> | <a href="https://www.janelia.org/lab/branson-lab">Branson Lab</a> | <i>Last Updated August 30, 2022</i>
</center>
</footer>


</body>
