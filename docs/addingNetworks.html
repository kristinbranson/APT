<!DOCTYPE html>
<html>
<head>
<title>APT Documentation</title>
<link rel="stylesheet" type="text/css" charset="utf-8" media="all"
href="styles/common.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="screen"
href="styles/screen.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="print"
href="styles/print.css">
<link rel="stylesheet" type="text/css" charset="utf-8"
media="projection" href="styles/projection.css">

<style type="text/css">
strong.regular-font {
  font-family: Arial, Lucida Grande, sans-serif;
  font-style: italic;
  font-size: 0.9em;
}
</style>

</head>

<body>

<h1><a id="adding_networks">Adding Networks to APT</a></h1>
<br>
<p>APT's backend has a modular structure to enable easier integration of different networks. To add a network <i>mynet</i>, user has to add file <code>Pose_mynet.py</code> to the deepnet directory and define a class with the same name (i.e., <code>Pose_mynet</code>) in the file. For tensorflow 1.x based networks, <code>Pose_mynet</code> can inherit from <code>PoseBase</code> or <code>PoseBaseGeneral</code> classes. These classes define all the training data-loading, model saving and loading to appropriate locations and other procecures and the user has to define only the networks and maybe the training procedures.
 
<p> APT front end (GUI) interacts with APT backend (DL algorithms) through <code>APT_interface.py</code>. During training when user selects <i>mynet</i> as the network to use, <code>APT_interface.py</code> creates the training databases, initializes the <code>Pose_mynet</code> class with the configuration settings object (<a href=#conf><code>conf</code></a>) and then calls the <code> train_wrapper</code> function to start the training. APT requires that the information about training's status be stored in the json format to <code>traindata.json</code> file to enable APT front-end to update the training progress and that the model file name have the format <code>deepnet-<i>step_num</i></code> so that APT can bundle the model files into the APT project at the end of the training. The files <code>traindata.json</code> and <code>deepnet-*</code> are constantly polled by APT front-end in order to update the user about training's status.
<p>
During tracking, <code>APT_interface.py</code> calls <code>get_pred_fn</code> after initilializing the <code>Pose_mynet</code> object. This function should return a handle to <i>prediction</i> function which when given a batch of input images as numpy array as input   returns the landmark predictions as <i>x,y</i>  locations, and if possible prediction scores or confidences. 
<p>
Example implementation of PyTorch networks can be found in <code>Pose_multi_mdn_joint_torch.py</code> and its super class <code>PoseCommon_pytorch.py</code>. In particular, <code>PoseCommon_pytorch.create_data_gen</code> function can be used to create the data loaders quickly. 
<p> There are multiple ways of adding new networks to APT:</p>
<ul>
<li> <a href="#pytorch">Based on code outline</a>: This is the only way to add PyTorch based networks. Understanding code outline is also helpful for adding networks based on other methods as well. 
<li> For Tensorflow 1.x based networks: We provide the base class <a href="#posebase">PoseBase</a> which when inherited the user only has to define only the network. 
<li> If more flexiblity is required for your Tensorflow 1.x based networks then you can inherit <a href="#posebasegeneral">PoseBaseGeneral</a>. In this case, the code will provide a tensorflow dataset object which can be used to generate the augmented training examples (images and labeled landmark location).
</ul>


<h2><a id="pytorch">Code Outline</a></h2>

<p> Outline of <code>Pose_<i>mynet</i>.py</code> file that implements a PyTorch based network that can be added to APT</p>
<pre><code>
  import PoseTools # If you want to use APTs default image augmentation method.
  import json
  import torch
  import logging # Use logging to print out information. 
  
  <b>class Pose_mynet(object)</b>:
      
      <b>def __init__(self,conf,**kwargs)</b>:
          # <a href="#conf">conf is a poseConfig</a> object that has the configuration properties
          # kwargs can be ignored.
          self.conf = conf
          self.model = None
          
      # --- helper fuctions ----
         
      def create_dataloader(self, train_info_file):
          # create dataloaders that will return images etc for training
          # An example pytorch based data loader can be found in PoseCommon_pytorch.py. 
          
      def create_model(self):
          # Create the model
          
      def get_saved_model_info(self):
          # Find the last model file that was saved and the iteration at which it was saved.
          
      def loss(self,preds,labels):
          # Compute the loss
          
      def load_model(self, model_file):
          # Load the model weights, optimizer state etc from the model_file
          
      def preprocess(self,ims):
          # Preprocess the images, eg: normalize.
          
          
      <b>def train_wrapper(self, restore=False):</b>
          # This is the main training function that is called by APT_interface to start the training.
          # When restore is set to True, the training should restart from the last saved model.
          
          conf = self.conf
          self.model = self.create_model()          
          # Also create the optimizer and loss function as required. 
          
          <b>train_info_file = os.path.join(conf.cachedir,conf.trainfilename + '.json') </b>
          # train_info_file has the training data information in coco format. The <i>image_file</i> path in train_info_file is relative to the conf.cachedir
          
          data_loader = self.create_dataloader(train_info_file)
          # An example pytorch based data loader can be found in PoseCommon_pytorch.py. 
          
          # The ability to restart the training is optional, but preferred.
          if restore: 
              # restore is set to True if user wants to continue from previously paused or discontinued training
              prev_step, model_file = self.get_saved_model_info()
              self.load_model(model_file)
          else:
              prev_step = 0
          
          <b>output_model_file_name = os.path.join(conf.cachedir,'deepnet-{}')</b>
          train_info_dict = {'step':[],'train_loss':[]}
          
          for step in range(prev_step,conf.dl_steps):
              img, labels = next(data_loader) 
              # For TF based networks, the whole dataset pipeline needs to be setup appropriately. 
              
              aug_img, aug_labels = PoseTools.preprocess_ims(img,labels,conf,scale=conf.rescale,distort=True)              
              # For PyTorch based networks, it is better to do the image augmentation in data_loader.
              # img and batches should be batched when calling preprocess_ims. The shape of img should be conf.batch_size x conf.imsz[0] x conf.imsz[1] x img_dim. For single animal, locs shape should be conf.batch_size x conf.n_classes x 2. For multi-animal, locs shape should be conf.batch_size x conf.max_n_animals x conf.n_classes x 2. The size of the aug_img will conf.batch_size x int(conf.imsz[0]/conf.rescale) x int(conf.imsz[1]/conf.rescale) x conf.img_dim. Shape of aug_locs is same as locs. Labeled locs that are missing annotations should have values either np.nan or -100000 (very large negative value).
              # Networks can implement their augmentation methods. The augmentation parameters set by user are availabel in self.conf.
              
              # Actual training steps.
               model_out = self.model(aug_img)
               loss = self.loss(model_out,aug_locs)
               loss.backward()
               optimizer.step()
               
               <b>if (step % conf.save_step == 0) and (step>0):</b>
                  # every conf.save_step save the model to f'deepnet-{step}' file.
                  model_param_dict = {'step':step,'model_state_params':self.model.state_dict(),...}
                  # Add optimizer parameters to the dict etc to help with restart of the training                  
                  <b>torch.save(model_param_dict,output_model_file_name.format(step))</b>
                  
               <b>if step % conf.display_step == 0:</b>
                  # every display_step save training information details. This information is displayed by APT front end in training monitor GUI during training.
                  train_loss = loss.cpu().numpy() 
                  # to ensure that the GPU memory gets collected by the gc.
                  # Loss can also be computed fresh batch of training inputs.
                  train_info_dict['train_loss'].append(train_loss)
                  train_info_dict['step'].append(step)
                  <b>train_data_file = os.path.join(conf.cachedir,'traindata.json')
                  with open(train_data_file,'w') as f:
                      json.dump(train_info_dict,f)</b>
                      
                  logging.info(f'step:{step}, train loss:{train_loss}')
                      
          # save the final model
          model_param_dict = {'step':conf.dl_steps,'model_state_params':self.model.state_dict(),...}
          torch.save(model_param_dict,output_model_file_name.format(conf.dl_steps))
                      
          
      <b>def get_pred_fn(self,model_file=None)</b>:
          # This is the function that is called to setup the model to start predicting. If model_file is not None, then the weights should be loaded from the model_file. If model_file is None, then load the weights from that latest model file. 
          # This function should return --
          # pred_fn: Function that predicts on a batch of input images
          # close_fn: Function that clears up stuff like releasing GPU memory etc.
          # model_file: Model file from which the weights were used. 
          
          self.model = self.create_model()
          
          # Load weights from latest trained model file if model_file is None.
          if model_file is None:
              step, model_file_used = self.get_saved_model_info()
          else:
              model_file_used = model_file
          self.load_model(model_file_used)
          
          <b>def pred_fn(ims):</b>
              # ims is a numpy array of a batch input images of shape conf.batch_size x conf.imsz[0] x conf.imsz[1] x conf.img_dim
              
              preprocessed_ims = self.preprocess(ims)
              # preprocess the inputs images to normalize them. The range of values of ims will be form 0 to 255 even though ims might not be a uint8 array. 
              
              # Do the prediction.
              preds = self.model(preprocessed_ims)
              
              # If the outputs are heatmaps, conver them to x,y loctions.
              pred_locs, pred_scores = self.convert_heatmaps_to_xy(preds)
              # For single animal networks: pred_locs and pred_scores shape must be conf.batch_size x conf.n_classes x 2 and conf.batch_size x conf.n_classes 
              # For multi-animal networks: pred_locs and pred_scores shape must be conf.batch_size x conf.max_n_animals x conf.n_classes x 2 and conf.batch_size x conf.max_n_animals x conf.n_classes
              
              <b>return {'locs':pred_locs, 'conf': pred_scores}</b>
              
          <b>def close_fn():</b>
              # Close any open files, clear GPU memory etc. 
              torch.cuda.empty_cache()
        
          <b>return pred_fn, close_fn, model_file_used</b>
      
</code></pre>

<p>
<h2><a id="conf">Configuration Object</a></h2>
During a network's initialization, the configuration settings defined by the user in the APT GUI as are given as an <code>PoseConfig (conf)</code> object during initialization. Listed below are its important configuration fields.</p>
<ul>
    <li> cachedir: Directory where the model files and training information must be saved.
    <li> imsz: 2 element tuple specifying the size of the input image.
    <li> img_dim: Number of channels in input image. 1 for grayscale, 3 for color.
    <li> batch_size: Batch size defined by user.
    <li> rescale: How much to downsample the input image before feeding into the network.
    <li> dl_steps: Number of steps to run training for.
    <li> n_classes: Number of pose landmarks.
    <li> is_multi: Whether the project is multi-animal or single-animal.
    <li> max_n_animals: For multi-animal networks, the number of maximum animals in an image.
    <li> trainfilename: Name of the file that has the training data.
    <li> save_step: Number of steps after which to save models.
    <li> maxckpt: Maximum number of latest models to save.
    <li> display_step: Number of steps after which to display and save the training information.
    <li> rrange: Amount in degree suggested by the user by which images can be rotated for augmentation. 
    <li> trange: Amount in pixels suggested by the user by which the image can be translated in both x and y directions for augmentation. 
    <li> scale_range_factor: Scaling amount suggested by for augmentation. Images can be scaled between [1/scale_factor_range,scale_factor_range].
    <li> flip_landmark_matches: Dict specifying landmark indices mapping to be used when images are flipped for augmentation.
    <li>horz_flip: Boolean specifying if images can be flipped horizontally for augmentation. 
    <li>vert_flip: Boolean specifying if images can be flipped vertically for augmentation. 
    
</ul>
<p>Details of other settings can be found  in <code>APT/tracker/dt/params_deeptrack.yaml</code>.</p>

<p>To expose parameters specific to your network to the user through the <code>APT&#8594;Tracking&#8594;Set Tracking Parameters</code> interface when they select <i>mynet</i> in APT, create a new configuration settings file <code>APT/trackers/dt/params_<i>mynet</i>.yaml</code>. Details of its format can be found in <code>APT/tracker/dt/params_deeptrack.yaml</code>.

<h2><a id="pytorch">Adding TensorFlow 1.x networks</a></h2>
<p> 

<h2><a id="posebase">Inheriting PoseBase</a></h2>

    <p>This is quickest way to add tensorflow 1.x based networks. If the network generates heatmaps for each landmark, then the user only needs to override create_network function and supply the appropriate hmaps_downsample to the __init__ function. For example in the case of openpose network, the final heatmap output is downsampled by a factor of 8. In this case, during initializiation <code>self.hmaps_downsample</code> should be set to 8.</p>

    <p>If your networks produces output other than only heatmaps, you'll have to override</p>
    <ul>
        <li> <a href="#pb_create_network">create_network</a> - Function that creates the network.
        <li> <a href="#pb_convert_locs_to_targets">convert_locs_to_targets</a> - Convert x,y locations to network target e.g., from x,y locations to heatmaps.
        <li> <a href="#pb_convert_preds_to_locs">convert_preds_to_locs</a> - Convert the networks output (eg heatmaps) to x,y landmark locations.
        <li> <a href="#pb_conf">get_conf_from_preds</a> - Get prediction confidence (or scores) from predictions. Eg., Heatmap pixel value at each landmarks predicted location.
        <li> <a href="#pb_loss">loss</a> - Define the loss function to be optimized.
    </ul>
    <p>In addition, if you want to change the training procedure, you'll have to override</p>
    <ul>
        <li> <a href="#pb_train">train</a> -If you want to different learning rate schedule or optimizer.
        <li> <a href="#pb_get_pred_fn">get_pred_fn</a> - To create a prediction function that will take a batch of input images and return the predictions on them. Override this function if you override train function.
        <li> <a href="#pb_preproc">pre_proc</a> - How image preprocessing is done.
    </ul>

    <h3><a id="pb_create_network"> Defining create_network function:</a></h3>
        Inputs<br>
        <br>
        <!-- <ul>
          <li> None
        </ul> -->
        Outputs
        <ul>
          <li> List of prediction tensors (e.g. heatmaps).
        </ul>
        <p>The tensors containing the training examples are provided as a list in self.inputs.
        By default, for batch size B, and image size H x W x C:</p>
        <ul>
            <li> self.inputs[0] tensor has the images. If the image downsample factor is s then the size of this tensor is [B, H//s, W//s, C].
            <li> self.inputs[1] tensor has the locations in an array of [B, N, 2]
            <li> self.inputs[2] tensor has the [movie_id, frame_id, animal_id] information. Mostly useful for debugging.
            <li> self.inputs[3] tensor has the heatmaps, which in most cases will be used for computing the loss.
        </ul>
        <p>Information about whether it is training phase or test phase for batch norm is available in self.ph['phase_train']
        If preproc function is overridden, then self.inputs will have the outputs of preproc function.
        This function must return the network's output such as the predicted heatmaps.</p>

        <p>If the network output's heatmap and this is the only function that is overridden, then the network will trained using l2 loss between the target and output heatmaps. The network will be trained using Adam optimizer with an exponetially decaying learning rate.</p>
        <pre><code>
        def create_network(self):
            in = self.inputs[0]
            l1 = tf.layers.Conv2D(64,3)(in)
            l2 = tf.layers.Conv2D(64,3)(l2)
            return l2
        </code></pre>

    <h3><a id="pb_convert_locs_to_targets"> Defining convert_locs_to_target function </a></h3>
        Inputs
        <ul>
            <li>locs: Labeled part locations as B x N x 2
        </ul>
        Outputs
        <ul>
            <li> List of targets.
        </ul>
        <p>Override this function to change how labels are converted into targets. You can use PoseTools.create_label_images to generate the target heatmaps. You can use PoseTools.create_affinity_labels to generate the target part affinity field heatmaps.</p>

        <p>Both the inputs and outputs are numpy arrays. This function will be injected into tensorflow dataset pipeline using tf.dataset.map.</p>

        <p>Return the results as a list. This list will be supplied as the first argument to the loss function.</p>
        <pre><code>
          def convert_locs_to_targets(self,locs):
              conf = self.conf
              hmaps_rescale = self.hmaps_downsample
              hsz = [ math.ceil( (i // conf.rescale)/hmaps_rescale) for i in conf.imsz]
              # Creates target heatmaps by placing gaussians with sigma label_blur_rad at location locs.
              hmaps = PoseTools.create_label_images(locs/hmaps_rescale, hsz, 1, conf.label_blur_rad)
              return [hmaps]
    </code></pre>

    <h3><a id="pb_convert_preds_to_locs"> Defining convert_preds_to_locs function </a></h3>
        Inputs<br>
        <ul>
            <li> pred: Output of network as python/numpy arrays
        </ul>
        Outputs<br>
        <ul>
            <li>locs: 2D landmark locations as B x N x 2
        </ul>

        Override this function to write your function to convert the networks output (as numpy array) to locations. Note the locations should be in input images scale i.e., not downsampled by self.rescale
        <pre><code>
          def convert_preds_to_locs(self, pred):
              return PoseTools.get_pred_locs(pred)*self.hmaps_downsample
        </code></pre>

        <h3><a id="pb_conf"> Defining get_conf_from_preds function:</a></h3>
            Inputs<br>
            <ul>
                <li>pred: Has the output of network created in define_network. It'll be a list if networks output was a list.
            </ul>
            Outputs<br>
            <ul>
                <li>conf: A numpy array of size [B,N]
            </ul>
            <p>APT uses confidence values are postprocessing and for browsing difficult examples.</p>

            <pre><code>
              def get_conf_from_preds(self, pred):
                  # if pred are heatmaps
                  return np.max(pred, axis=(1, 2))
            </code></pre>


    <h3><a id="pb_loss"> Defining loss function:</a></h3>
        Inputs<br>
        <ul>
            <li>targets: Has the targets (e.g hmaps/pafs) created in convert_locs_to_targets
            <li>pred: Has the output of network created in define_network. It'll be a list if networks output was a list.
        </ul>
        Outputs<br>
        <ul>
            <li>loss: The loss function to be optimized.
        </ul>
        <p>Override this function to define your own loss function.</p>
        <pre><code>
          def loss(self,targets, pred):
              hmap_loss = tf.sqrt(tf.nn.l2_loss(targets[0] - self.pred))/ self.conf.n_classes/ self.conf.batch_size
              return hmap_loss
        </code></pre>

    <h3><a id="pb_train">Defining train  function</a></h3>
        Inputs<br>
        <ul>
            <li>restore: Whether to start training from previously saved model or start from scratch.
        </ul>
        Outputs<br>
        <br>

        <p>By default this function trains the network my minimizing the loss function using Adam optimizer along with gradient norm clipping. The learning rate schedule is exponential decay: lr = conf.learning_rate*(conf.gamma**(float(cur_step)/conf.decay_steps))</p>

        <p>Override this function to implement a different training function. If you override this, you need to override the get_pred_fn as well. For saving the models, call self.create_saver() after creating the network but before creating a session to setup the saver. Call self.save(sess, step_no)  every self.conf.save_step to save intermediate models. At the end of training, again call self.save(sess, self.conf.dl_steps) to save the final model. During prediction (get_pred_fn), you can restore the latest model by calling self.create_saver() after creating the network but before creating a session, and then calling self.restore(sess). In most cases, if you use self.create_saver(), then you don't need to override get_pred_fn().<p>

        <p>If you want to use your own saver, the train function should save models to self.conf.cachedir every self.conf.save_step. Also save a final model at the end of the training with step number self.conf.dl_steps. APT expects the model files to be named 'deepnet-<step_no>' (i.e., follow the format used by tf.train.Saver for saving models e.g. deepnet-10000.index). If you write your own saver, then you'll have to override get_pred_fn() for restoring the model.</p>

        <p>Before each training step call self.fd_train() which will setup the data generator to generate augmented input images in self.inputs from training database during the next call to sess.run. This also sets the self.ph['phase_train'] to True for batch norm. Use self.fd_val() will generate non-augmented inputs from training DB and to set the self.ph['phase_train'] to false for batch norm.</p>

        <p>To view updated training status in APT, call self.update_and_save_td(step,sess) after each training step. Note update_and_save_td uses the output of loss function to find the loss and convert_preds_to_locs function to find the distance between prediction and labeled locations.</p>

    <h3><a id="pb_train">Defining get_pred_fn</a></h3>
        Inputs<br>
        <ul>
            <li>model_file: Model_file to use. If not specified the latest trained should be used.
        </ul>
        Outputs:<br>
        <ul>
            <li> pred_fn: Create a prediction function that takes a batch of images of size [B,H,W,C] and returns the pose prediction as a python array of size [B, N,2]. The pose prediction must be returned as dictionary with key 'locs'. The input images are raw and need to be preprocessed (eg. Downsampled or contrast adjusted).
            <li> close_fn: Function to clean up at the end. For example, to close tensorflow sessions.
            <li>model_file_used: Returns the model file that is used for prediction.
        </ul>

        <p>Create a prediction function that takes a batch of images of size [B,H,W,C] and returns the pose prediction as a python array of size [B, N,2]. This function should create the network, start a tensorflow session and load the latest model. If you used self.create_saver() for saving the models, you can restore the latest model by calling self.create_saver() after creating the network but before creating a session, and then calling self.restore(sess). Example:</p>
        <pre><code>
def get_pred_fn(self, model_file=None):
    sess, latest_model_file = self.restore_net_common(model_file=model_file)
    conf = self.conf

    def pred_fn(all_f):
        # all_f will have size [B,H,W,C]
        bsize = conf.batch_size
        xs, locs_in = PoseTools.preprocess_ims(all_f, in_locs=np.zeros([bsize, self.conf.n_classes, 2]), conf=self.conf, distort=False, scale=self.conf.rescale)
        self.fd[self.inputs[0]] = xs
        self.fd_val()
        pred = sess.run(self.pred, self.fd)
        base_locs = self.convert_preds_to_locs(pred)
        base_locs = base_locs * conf.rescale
        ret_dict = {}
        ret_dict['locs'] = base_locs
        return ret_dict
</code></pre>

<h3><a id="pb_preproc">Defining preproc_func function (Optional)</a></h3>
    Inputs<br>
    <ul>
        <li>ims: Input image as B x H x W x C
        <li>locs: Labeled part locations as B x N x 2
        <li>info: Information about the input as B x 3. (:,0) is the movie number, (:,1) is the frame number and (:,2) is the animal number (if the project has trx).
        <li>distort: Whether to augment the data or not.
    </ul>
    Output (Default)<br>
    <ul>
        <li> List with augmented images, augmented labeled locations, input information, heatmaps.
    </ul>
    <p>Override this function to change how images are preprocessed. Ensure that the return objects are float32. This function is added into tensorflow dataset pipeline using tf.dataset.map. The outputs returned by this function are available as tf tensors in self.inputs list in the same order.</p>

    <pre><code>
      import PoseTools
      def preproc_func(self, ims, locs, info, distort):
          conf = self.conf
          # Scale and augment the training image and labels
          ims, locs = PoseTools.preprocess_ims(ims, locs, conf, distort, conf.rescale)
          out = self.convert_locs_to_targets(locs)
          # Return the results as float32.
          out_32 = [o.astype('float32') for o in out]
          return [ims.astype('float32'), locs.astype('float32'), info.astype('float32')] + out_32

    </code></pre>

<p>For more control over training, you can also override the following functions:</p>


<h2><a id="posebasegeneral"> Inheriting PoseBaseGeneral</a></h2>
    <p>This class provides more flexibility for adding tensorflow 1.x based networks to APT, where you have more control over the training procedure.</p>
    <p>In this case, the function that need to overridden are:</p>
    <ul>
        <li><a href="#pbg_convert_locs">convert_locs_to_target</a> - create target outputs (e.g. heatmaps) from x,y locations that you want the network the predict.
          <li><a href="#pbg_convert_preds">convert_preds_to_locs</a> - convert the output of network to x,y locations E.g From output heatmaps to x,y predictions.
        <li><a href="#pbg_train">train</a> - create the network, train and save trained models.
        <li><a href="#pbg_load">load_model</a> - setup the network for prediction by restoring the network.
    </ul>
    <p>We use the tensorflow dataset pipeline to read and process the input images for faster training. For this, image preprocessing and target creation (e.g. generating heatmaps) functions have to be defined individually so that they can be injected into the dataset pipeline.</p>

    <p>Override <a href="pbg_preprocess">preprocess_ims</a> if you want to define your own image pre-processing function. By default, it'll down sample the input image (if specified by the user), augment the images using the augmentation parameters, and normalize contrast using CLAHE (again if specified by the user).</p>

    <h3><a id="pbg_convert_locs">Defining convert_locs_to_target</a></h3>
        Inputs<br>
        <ul>
            <li>locs: A numpy array of size B x N x 2, where B is the batch size, N is the number of landmarks and locs[:,:,0] are x locations, while locs[:,:,1] are the y locations of the landmarks.
        </ul>
        Outputs
        <ul>
            <li>target_list: The targets also should be numpy arrays, and the first dimension should correspond to the batch.
        </ul>
        <p>Override this function to convert labels into targets (e.g. heatmaps). You can use PoseTools.create_label_images to generate the target heatmaps. You can use PoseTools.create_affinity_labels to generate the target part affinity field heatmaps. Return the results as a list. This list will be available as tensors self.inputs[3], self.inputs[4] and so on for computing the loss.</p>
        <pre><code>
          def convert_locs_to_targets(self,locs):
              conf = self.conf
              hmaps_rescale = self.hmaps_downsample
              hsz = [ math.ceil( (i // conf.rescale)/hmaps_rescale) for i in conf.imsz]
              # Creates target heatmaps by placing gaussians with sigma label_blur_rad at location locs.
              hmaps = PoseTools.create_label_images(locs/hmaps_rescale, hsz, 1, conf.label_blur_rad)
              return [hmaps]
    </code></pre>


    <h3><a id="pbg_convert_preds">Defining convert_preds_to_locs</a></h3>
        Inputs
        <ul>
            <li>preds: Output of the network. Eg Heatmaps.
        </ul>
        Outputs
        <ul>
        <li> numpy array of size [B,N,2] with x,y locations where B is the batch_size, N is the number of landmarks. [:,:,0] should be the x location, while [:,:,1] should be the y locations.
        </ul>

          <p>Convert the output prediction of network to x,y locations. The output should be in the same scale as input image. If you downsampled the input image, then the x,y location should for the downsampled image. Eg. From heatmap output to x,y locations.</p>

          <pre><code>
            def convert_preds_to_locs(self, pred):
                return PoseTools.get_pred_locs(pred)*downsample_factor
          </code></pre>

    <h3><a id="pbg_train">Defining train function</a></h3>
        Inputs<br>
        Outputs<br>

        <p>Implement network creation and and its training in this function. The input and output tensors are in available in self.inputs.
        self.inputs[0] has the downsampled, preprocessed and augmented images as B x H//s x W//s x C, where s is the downsample factor.
        self.inputs[1] has the landmark positions as B x N x 2
        self.inputs[2] has information about the movie number, frame number and trx number as B x 3
        self.inputs[3] onwards has the outputs that are produced by convert_locs_to_targets
        The train function should save models to self.conf.cachedir every self.conf.save_step. Also save a final model at the end of the training with step number self.conf.dl_steps. APT expects the model files to be named 'deepnet-<step_no>' (i.e., follow the format used by tf.train.Saver for saving models e.g. deepnet-10000.index).
        To view updated training metrics in APT training update window, call self.append_td(step,train_loss,train_dist) every self.conf.display_step. train_loss is the current training loss, while train_dist is the mean pixel distance between the predictions and labels.</p>

    <h3><a id="pbg_load">Defining load_model</a></h3>
        Inputs
        <ul>
            <li>im_input: Input tensor that will have the preprocessed image for prediction.
            <li>mode_file: Model file to be used for prediction. If None, then use the latest model. If not supplied then set the default value to None.
        </ul>
        Outputs
        <ul>
        <li> network_out: Network's prediction tensor. The value of this tensor is given as input to convert_preds_to_locs to get the x,y pose location.
        <li> sess: Current TF session.
        <li> model_file_used: Model file used for initializing  the network.
        </ul>

      <p>Setup up prediction function.
          During prediction, the input image (after preprocessing) will be available in the im_input tensor. Return the prediction/output tensor and the TF session object, and the model file that was used to load the model. (Return model_file if that is used to load the model)</p>

          <p>In this function, setup the network, load the saved weights from the model_file. If model_file is None load the weights from the latest model. The placeholders (if any) used during training, should be converted to constant tensors.</p>


<p> To use pretrained weights (eg. Imagenet weights) for TF 1.x based networks, put the location of the pretrained weights in self.pretrained_weights in the __init__ function. When the weights are initialized, for all variables whose names and sizes match the variables saved in the pretrained weights file will get their weights initialized to the ones saved in the pretrained file. </p>
<pre><code>
    def __init__(self,conf):
        PoseBase.__init__(self,conf)
        script_dir = os.path.dirname(os.path.realpath(__file__))
        wt_dir = os.path.join(script_dir, 'pretrained')
        self.pretrained_weights =  os.path.join(wt_dir,'resnet_v1_50.ckpt')
</code></pre>

</body>
