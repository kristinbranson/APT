# How to add a deep network to APT
#
# 1. Add an entry here with top-level field <net>.
#    <net> should match the Py net-type.
# 2. Add <net> to the enumeration block in DLNetType
# 3. Add a params_deeptrack_<net>.yaml
#
#
# How to remove a deep network from APT
# 1. Remove the entry here.
# 2. Remove <net> from the enumeration block in DLNetType.
# 3. Remove params_deeptrack_<net>.yaml.
# 4. Search/grep code for any hardcoded references to removed net (eg 
#    DLNetType.leap) or that net type ('leap'); remove that code!
#
# Notes on names
# - The top-level field <net> matches the Python deepnet nettype.
# - shortString is a single-world nickname. Currently unused.
# - displayString is a pretty string for UIs. It can contain whitespace.
# - The string used in parameters structures is specified in the
#   params_deeptrack_<net>.yaml.

mdn_joint_fpn:   
  shortString: grone
  displayString: GRONe
  modelCheckpointPat: deepnet-%d  
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*]
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 1
  isMultiAnimal: 0
  description: >
    Grid Regression Output Network (GRONe) is a convolutional network for pose recognition
    developed specifically for lab-animal pose tracking by the developers of
    <a href="https://kristinbranson.github.io/APT/index.html">APT</a>, and achieved
    best or near-best performance for all tested datasets. It excels at achieving subpixel
    resolution and disambiguating close, social interactions. It combines heatmap-based and regression-based
    approaches. 
  
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

mdn:
  shortString: mdn  
  displayString: MDN (Deprecated)
  modelCheckpointPat: deepnet-%d.index  
  modelGlobs: [deepnet-%d.*, deepnet_ckpt, traindata*]  # matching files are considered important and saved to Lbl
  trkAuxFields: [pTrkConf] #, pTrkConf_unet]
  trkAuxLabels: [conf_mdn] #, conf_unet]
  doesOccPred: 1
  isMultiAnimal: 0
  description: >
    Former name for GRONe, depricated. 
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

unet:
  shortString: unet
  displayString: Unet
  modelCheckpointPat: deepnet-%d.index  
  modelGlobs: [deepnet-%d.*, deepnet_ckpt, traindata*]  # matching files are considered important and saved to Lbl
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    Unet is a single-animal pose recognition algorithm based on the simple U-Net convolutional network architecture. It is a heatmap
    based approach, and works best when there are not many occlusions.
    
    <i>Ronneberger, O., Fischer, P., and Brox, T., 2015. "U-net: Convolutional networks for biomedical image segmentation", MICCAI.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

mmpose:   
  shortString: mspn
  displayString: MSPN
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*, '*json']
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    Multi-Stage Pose Network (MSPN) is a convolutional network designed for human pose recogntition. It is the winner of the 2018 COCO human keypoint recognition challenge and implemented by the <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.

    <i>Li, W., et al., 2019. "Rethinking on multi-stage networks for human pose estimation", arXiv.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

hrformer:
  shortString: hrformer
  displayString: HRFormer
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*, '*json']
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    High-Resolution Transformer (HRFormer) is a large, transformer-based method. It is a leading algorithm for human pose recognition when used as the final stage of a multi-stage algorithm. We use the implementation in the <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.
    
    <i>Yuan, Y. et al., 2021. "HRFormer: High-resolution vision transformer for dense predict", NeurIPS.</i>
    
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

hrnet:
  shortString: hrnet
  displayString: HRNet
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*, '*json']
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    High-Resolution Net (HRNet) is a convolutional network designed for pixel-level resolution in
    human pose recognition. It is implemented by the
    <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.
    
    <i>Sun, K., et al., 2019. "Deep high-resolution representation learning for human pose estimation", CVPR.</i>

  
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

deeplabcut:   
  shortString: dlc
  displayString: DeepLabCut
  modelCheckpointPat: deepnet-%d.index  
  modelGlobs: [deepnet-%d.*, deepnet_ckpt, traindata*]
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    DeepLabCut is convolutional network developed for animal pose tracking. Our implementation uses code from the original
    DeepLabCut repository. 
    
    <i>Mathis, A., et al., 2018. "DeepLabCut: markerless pose estimation of user-defined body parts with deep learning", Nature neuroscience.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

dpk:   
  shortString: dpk
  displayString: DeepPoseKit
  modelCheckpointPat: deepnet-%08d.h5
  modelGlobs: [deepnet-%08d.h5, deepnet.conf.pickle, traindata*]
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    DeepPoseKit is a convolutional network developed for animal pose tracking. Our implementation uses code from the
    original DeepPoseKit repository.

    <i>Graving, J.M. et al., 2019. "DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning", elife.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

openpose:   
  shortString: openpose
  displayString: OpenPose
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, traindata*]
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 0
  description: >
    OpenPose is a light-weight, convolutional-network, bottom-up algorithm for multi-person pose recognition. It was the
    first real-time capable pose recognition algorithm for multi-person pose tracking.
    
    <i> Cao, Z., et al., 2017. "Realtime multi-person 2d pose estimation using part affinity fields", CVPR.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

multi_mdn_joint_torch:
  shortString: magrone
  displayString: MultiAnimal GRONe
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d*, deepnet_ckpt, traindata*, 'traindata.json'] 
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    Grid Regression Output Network (GRONe) is a multi-target, bottom-up, convolutional-network-based algorithm for pose recognition
    developed specifically for lab-animal pose tracking by the developers of
    <a href="https://kristinbranson.github.io/APT/index.html">APT</a>, and achieved
    best or near-best performance for all tested datasets. It excels at achieving subpixel
    resolution and disambiguating close, social interactions. It combines heatmap-based and regression-based
    approaches. 

#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

multi_openpose:
  shortString: maopenpose
  displayString: MultiAnimal OpenPose
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d.*, traindata*] 
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    OpenPose is a light-weight, bottom-up, convolutional-network-based algorithm for multi-person pose recognition. It was
    the first real-time capable
    pose recognition algorithm for multi-person pose tracking. It is implemented by the <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.
    
    <i> Cao, Z., et al., 2017. "Realtime multi-person 2d pose estimation using part affinity fields", CVPR.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

multi_cid:
  shortString: multi_cid
  displayString: CiD
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*, '*json']
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    Contextual Instance Decoupling (CID) is a bottom-up, convolutional-network-based, mutli-target tracking algorithm, designed for human pose recognition. It
    achieves state-of-the-art results on the CrowdPose dataset. It is implemented by the <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.
    
    <i>Wang, D. and Zhang, S., 2022. "Contextual instance decoupling for robust multi-person pose estimation", CVPR.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

multi_dekr:
  shortString: multi_dekr
  displayString: DeKR
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, deepnet_ckpt, traindata*, '*json']
  trkAuxFields: [pTrkConf]
  trkAuxLabels: [confidence]
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    Disentangled Keypoint Regression (DEKR) is a bottom-up mutli-target tracking algorithm, designed for human pose recognition. It
    achieves state-of-the-art results on the COCO keypoint and CrowdPose dataset. It is implemented by the <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a> library.
    
    <i>Geng, Z., et al., 2021. "Bottom-up human pose estimation via disentangled keypoint regression", CVPR.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

detect_mmdetect:
  shortString: mmdetect
  displayString: DeTR Object Detection
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, traindata*] 
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    Detection TRansformer (DeTR) is a transformer-based algorithm for generic object detection.
    It is implemented by the <a href="https://mmdetection.readthedocs.io/en/latest/overview.html>MMDetection</a> library. 
    
    <i>Carion, N. et al., 2020. "End-to-end object detection with transformers", ECCV.</i>

#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif

detect_frcnn:
  shortString: frcnn
  displayString: FRCNN Object Detection
  modelCheckpointPat: deepnet-%d
  modelGlobs: [deepnet-%d, traindata*] 
  trkAuxFields: []
  trkAuxLabels: []
  doesOccPred: 0
  isMultiAnimal: 1
  description: >
    Faster R-CNN is a popular, efficient, convolutional network for generic object detection. It is implemented by the
    <a href="https://mmdetection.readthedocs.io/en/latest/overview.html>MMDetection</a> library.
    
    <i>Ren, S., et al., 2016. "Faster R-CNN: Towards real-time object detection with region proposal networks", PAMI.</i>
#  docker: tf23_mmdetection
#  sing: /groups/branson/bransonlab/apt/sif/apt_tf23_mmdetection_20210708.sif
