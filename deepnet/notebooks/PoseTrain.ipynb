{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Mayank Feb 3 2016\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import os,sys,shutil\n",
    "import tempfile,copy,re\n",
    "from enum import Enum\n",
    "import localSetup\n",
    "\n",
    "# import caffe,lmdb\n",
    "# import caffe.proto.caffe_pb2\n",
    "# from caffe.io import datum_to_array\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as manimation\n",
    "import math,cv2,scipy,time,pickle\n",
    "import numpy as np\n",
    "\n",
    "import PoseTools,myutils,multiResData\n",
    "\n",
    "import convNetBase as CNB\n",
    "from batch_norm import batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PoseTrain(object):\n",
    "    \n",
    "    Nets = Enum('Nets','Base Joint Fine')\n",
    "    TrainingType = Enum('TrainingType','Base MRF Fine All')\n",
    "    DBType = Enum('DBType','Train Val')\n",
    "   \n",
    "    \n",
    "    def __init__(self,conf):\n",
    "        self.conf = conf\n",
    "        self.feed_dict = {}\n",
    "    \n",
    "    # ---------------- DATABASE ---------------------\n",
    "    \n",
    "    def openDBs(self):\n",
    "#         lmdbfilename =os.path.join(self.conf.cachedir,self.conf.trainfilename)\n",
    "#         vallmdbfilename =os.path.join(self.conf.cachedir,self.conf.valfilename)\n",
    "#         self.env = lmdb.open(lmdbfilename, readonly = True)\n",
    "#         self.valenv = lmdb.open(vallmdbfilename, readonly = True)\n",
    "        if self.trainType == 0:\n",
    "            trainfilename =os.path.join(self.conf.cachedir,self.conf.trainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(self.conf.cachedir,self.conf.valfilename) + '.tfrecords'\n",
    "            self.train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            self.val_queue = tf.train.string_input_producer([valfilename])\n",
    "        else:\n",
    "            trainfilename =os.path.join(self.conf.cachedir,self.conf.fulltrainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(self.conf.cachedir,self.conf.fulltrainfilename) + '.tfrecords'\n",
    "            self.train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            self.val_queue = tf.train.string_input_producer([valfilename])\n",
    "            \n",
    "\n",
    "    def openHoldoutDBs(self):\n",
    "        lmdbfilename =os.path.join(self.conf.cachedir,self.conf.holdouttrain)\n",
    "        vallmdbfilename =os.path.join(self.conf.cachedir,self.conf.holdouttest)\n",
    "        self.env = lmdb.open(lmdbfilename, readonly = True)\n",
    "        self.valenv = lmdb.open(vallmdbfilename, readonly = True)\n",
    "\n",
    "    def createCursors(self, sess, txn=None, valtxn=None):\n",
    "#         if txn is None:\n",
    "#             txn = self.env.begin()\n",
    "#             valtxn = self.valenv.begin()\n",
    "#         self.train_cursor = txn.cursor(); \n",
    "#         self.val_cursor = valtxn.cursor()\n",
    "        train_ims,train_locs,train_info = multiResData.read_and_decode(self.train_queue,self.conf)\n",
    "        val_ims,val_locs,val_info = multiResData.read_and_decode(self.val_queue,self.conf)\n",
    "        self.train_data = [train_ims,train_locs,train_info]\n",
    "        self.val_data = [val_ims,val_locs,val_info]\n",
    "        coord = tf.train.Coordinator()\n",
    "        self.threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "        \n",
    "    def closeCursors(self):\n",
    "        self.env.close()\n",
    "        self.valenv.close()\n",
    "        \n",
    "    def readImages(self,dbType,distort,sess):\n",
    "        conf = self.conf\n",
    "#         curcursor = self.val_cursor if (dbType == self.DBType.Val) \\\n",
    "#                     else self.train_cursor\n",
    "#         xs, locs = PoseTools.readLMDB(curcursor,\n",
    "#                          conf.batch_size,conf.imsz,multiResData)\n",
    "        cur_data = self.val_data if (dbType==self.DBType.Val) \\\n",
    "                else self.train_data\n",
    "        xs = []; locs = []; info = []\n",
    "        for ndx in range(conf.batch_size):\n",
    "            [curxs,curlocs,curinfo] = sess.run(cur_data)\n",
    "            if np.ndim(curxs)<3:\n",
    "                xs.append(curxs[np.newaxis,:,:])\n",
    "            else:\n",
    "                xs.append(curxs)\n",
    "            locs.append(curlocs)\n",
    "            info.append(curinfo)\n",
    "        xs = np.array(xs)    \n",
    "        locs = np.array(locs)\n",
    "        locs = multiResData.sanitizelocs(locs)\n",
    "        if distort:\n",
    "            if conf.horzFlip:\n",
    "                xs,locs = PoseTools.randomlyFlipLR(xs,locs)\n",
    "            if conf.vertFlip:\n",
    "                xs,locs = PoseTools.randomlyFlipUD(xs,locs)\n",
    "            xs,locs = PoseTools.randomlyRotate(xs,locs,conf)\n",
    "            xs = PoseTools.randomlyAdjust(xs,conf)\n",
    "        \n",
    "        self.xs = xs\n",
    "        self.locs = locs\n",
    "        self.info = info\n",
    "        \n",
    "    def createPH(self):\n",
    "        x0,x1,x2,y,keep_prob = CNB.createPlaceHolders(self.conf.imsz,\n",
    "                               self.conf.rescale,self.conf.scale,self.conf.pool_scale,\n",
    "                                self.conf.n_classes)\n",
    "        locs_ph = tf.placeholder(tf.float32,[self.conf.batch_size,\n",
    "                                             self.conf.n_classes,2])\n",
    "        learning_rate_ph = tf.placeholder(tf.float32,shape=[],name='learning_r')\n",
    "        phase_train_base = tf.placeholder(tf.bool, name='phase_train_base')                 \n",
    "        phase_train_fine = tf.placeholder(tf.bool, name='phase_train_fine')                 \n",
    "        self.ph = {'x0':x0,'x1':x1,'x2':x2,\n",
    "                     'y':y,'keep_prob':keep_prob,'locs':locs_ph,\n",
    "                     'phase_train_base':phase_train_base,\n",
    "                     'phase_train_fine':phase_train_fine,\n",
    "                     'learning_rate':learning_rate_ph}\n",
    "\n",
    "    def createFeedDict(self):\n",
    "        self.feed_dict = {self.ph['x0']:[],\n",
    "                          self.ph['x1']:[],\n",
    "                          self.ph['x2']:[],\n",
    "                          self.ph['y']:[],\n",
    "                          self.ph['keep_prob']:1.,\n",
    "                          self.ph['learning_rate']:1,\n",
    "                          self.ph['phase_train_base']:False,\n",
    "                          self.ph['phase_train_fine']:False,\n",
    "                          self.ph['locs']:[]}\n",
    "\n",
    "    def updateFeedDict(self,dbType,distort,sess):\n",
    "        conf = self.conf\n",
    "        self.readImages(dbType,distort,sess)\n",
    "        x0,x1,x2 = PoseTools.multiScaleImages(self.xs.transpose([0,2,3,1]),\n",
    "                                              conf.rescale, conf.scale,\n",
    "                                              conf.l1_cropsz,conf)\n",
    "\n",
    "        labelims = PoseTools.createLabelImages(self.locs,\n",
    "                                   self.conf.imsz,\n",
    "                                   self.conf.pool_scale*self.conf.rescale,\n",
    "                                   self.conf.label_blur_rad)\n",
    "        self.feed_dict[self.ph['x0']] = x0\n",
    "        self.feed_dict[self.ph['x1']] = x1\n",
    "        self.feed_dict[self.ph['x2']] = x2\n",
    "        self.feed_dict[self.ph['y']] = labelims\n",
    "        self.feed_dict[self.ph['locs']] = self.locs\n",
    "        \n",
    "\n",
    "    # ---------------- NETWORK ---------------------\n",
    "    \n",
    "    def createBaseNetwork(self,doBatch):\n",
    "        pred,layers = CNB.net_multi_conv(self.ph['x0'],self.ph['x1'],\n",
    "                                         self.ph['x2'],self.ph['keep_prob'],\n",
    "                                         self.conf,doBatch,\n",
    "                                         self.ph['phase_train_base']\n",
    "                                        )\n",
    "        self.basePred = pred\n",
    "        self.baseLayers = layers\n",
    "\n",
    "    def createACNetwork(self,doBatch,jointTraining=False):\n",
    "        \n",
    "        n_classes = self.conf.n_classes\n",
    "        with tf.variable_scope('AC_'):\n",
    "            self.createMRFNetwork(doBatch,jointTraining)\n",
    "        \n",
    "        mrfpred = self.mrfPred if jointTraining else tf.stop_gradient(self.mrfPred)\n",
    "        \n",
    "        layer7_init = self.baseLayers['conv7']\n",
    "        layer7 = layer7_init if jointTraining else tf.stop_gradient(layer7_init)\n",
    "        \n",
    "        with tf.variable_scope('AC_'):\n",
    "            ac_weights = tf.get_variable(\"weights\", \n",
    "                [1,1,self.conf.nfcfilt,self.conf.n_classes],\n",
    "                initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "            ac_biases = tf.get_variable(\"biases\", self.conf.n_classes,\n",
    "                initializer=tf.constant_initializer(0))\n",
    "            ac_out = tf.nn.conv2d(layer7, ac_weights,\n",
    "                strides=[1, 1, 1, 1], padding='SAME') + ac_biases\n",
    "        self.acPred = ac_out + mrfpred\n",
    "        \n",
    "    def createMRFNetwork(self,doBatch,jointTraining=False):\n",
    "        \n",
    "        n_classes = self.conf.n_classes\n",
    "        bpred = self.basePred if jointTraining else tf.stop_gradient(self.basePred)\n",
    "        mrf_weights = PoseTools.initMRFweights(self.conf).astype('float32')\n",
    "        mrf_weights = mrf_weights/n_classes\n",
    "        \n",
    "        baseShape = tf.Tensor.get_shape(bpred).as_list()[1:3]\n",
    "        mrf_sz = mrf_weights.shape[0:2]\n",
    "\n",
    "        bpred = tf.nn.relu((bpred+1)/2)\n",
    "        sliceEnd = [0,0]\n",
    "        pad = False\n",
    "        if mrf_sz[0] > baseShape[0]:\n",
    "            dd1 = int(math.ceil(float(mrf_sz[0]-baseShape[0])/2))\n",
    "            sliceEnd[0] = mrf_sz[0]-dd1\n",
    "            pad = True\n",
    "        else:\n",
    "            dd1 = 0\n",
    "            sliceEnd[0] = baseShape[0]\n",
    "            \n",
    "        if mrf_sz[1] > baseShape[1]:\n",
    "            dd2 = int(math.ceil(float(mrf_sz[1]-baseShape[1])/2))\n",
    "            sliceEnd[1] = mrf_sz[1]-dd2\n",
    "            pad = True\n",
    "        else:\n",
    "            dd2 = 0\n",
    "            sliceEnd[1] = baseShape[1]\n",
    "            \n",
    "        if pad:    \n",
    "            print('Padding base prediction by %d,%d. Filter shape:%d,%d Base shape:%d,%d'%(dd1,dd2,mrf_sz[0],mrf_sz[1],baseShape[0],baseShape[1]))\n",
    "            bpred = tf.pad(bpred,[[0,0],[dd1,dd1],[dd2,dd2],[0,0]])\n",
    "\n",
    "        ksz = math.sqrt(mrf_weights.shape[0]*mrf_weights.shape[1])\n",
    "        with tf.variable_scope('mrf'):\n",
    "            conv_std = (1./n_classes)/ksz\n",
    "            weights = tf.get_variable(\"weights\", initializer=tf.constant(mrf_weights))\n",
    "            biases = tf.get_variable(\"biases\", n_classes,\n",
    "                                     initializer=tf.constant_initializer(0))\n",
    "            conv = tf.nn.conv2d(bpred, weights,\n",
    "                               strides=[1, 1, 1, 1], padding='SAME')\n",
    "            mrfout = conv+biases\n",
    "        self.mrfPred = mrfout[:,dd1:sliceEnd[0],dd2:sliceEnd[1],:]\n",
    "        \n",
    "#   BELOW is from Chris bregler's paper where they try to do MRF style inference.\n",
    "#   This didn't work so well and the performance was always worse than base.\n",
    "#         mrf_conv = 0\n",
    "#         conv_out = []\n",
    "#         all_wts = []\n",
    "#         for cls in range(n_classes):\n",
    "#             with tf.variable_scope('mrf_%d'%cls):\n",
    "#                 curwt = 10*mrf_weights[:,:,cls:cls+1,:]-3\n",
    "#                 #the scaling is so that zero values are close to zero after softplus\n",
    "#                 weights = tf.get_variable(\"weights\",dtype = tf.float32,\n",
    "#                               initializer=tf.constant(curwt))\n",
    "#                 biases = tf.get_variable(\"biases\", mrf_weights.shape[-1],dtype = tf.float32,\n",
    "#                               initializer=tf.constant_initializer(-1))\n",
    "\n",
    "#             sweights = tf.nn.softplus(weights)\n",
    "#             sbiases = tf.nn.softplus(biases)\n",
    "            \n",
    "#             if doBatch:\n",
    "#                 curBasePred = batch_norm(bpred[:,:,:,cls:cls+1],trainPhase)\n",
    "#                 curBasePred = tf.maximum(curBasePred,0.0001)\n",
    "#             else:\n",
    "#                 curBasePred = tf.maximum(bpred[:,:,:,cls:cls+1],0.0001)\n",
    "                \n",
    "#             curconv = tf.nn.conv2d(curBasePred,sweights,strides=[1, 1, 1, 1], \n",
    "#                                    padding='SAME')+sbiases\n",
    "#             conv_out.append(tf.log(curconv))\n",
    "#             mrf_conv += tf.log(curconv)\n",
    "#             all_wts.append(sweights)\n",
    "#         mrfout = tf.exp(mrf_conv)\n",
    "#         self.mrfPred = mrfout[:,dd:sliceEnd,dd:sliceEnd,:]\n",
    "#    Return the value below when we used MRF kind of stuff\n",
    "#         return conv_out,all_wts\n",
    "        \n",
    "    def createFineNetwork(self,doBatch,jointTraining=False):\n",
    "        if self.conf.useMRF:\n",
    "            if not jointTraining:\n",
    "                pred = tf.stop_gradient(self.mrfPred)\n",
    "            else:\n",
    "                pred = self.mrfPred\n",
    "        else:\n",
    "            if not jointTraining:\n",
    "                pred = tf.stop_gradient(self.basePred)\n",
    "            else:\n",
    "                pred = self.basePred\n",
    "\n",
    "        # Construct fine model\n",
    "        labelT  = PoseTools.createFineLabelTensor(self.conf)\n",
    "        layers = self.baseLayers\n",
    "        \n",
    "        if not jointTraining:\n",
    "            layer1_1 = tf.stop_gradient(layers['base_dict_0']['conv1'])\n",
    "            layer1_2 = tf.stop_gradient(layers['base_dict_0']['conv2'])\n",
    "            layer2_1 = tf.stop_gradient(layers['base_dict_1']['conv1'])\n",
    "            layer2_2 = tf.stop_gradient(layers['base_dict_1']['conv2'])\n",
    "        else:\n",
    "            layer1_1 = layers['base_dict_0']['conv1']\n",
    "            layer1_2 = layers['base_dict_0']['conv2']\n",
    "            layer2_1 = layers['base_dict_1']['conv1']\n",
    "            layer2_2 = layers['base_dict_1']['conv2']\n",
    "            \n",
    "        curfine1_1 = CNB.extractPatches(layer1_1,pred,self.conf,1,4)\n",
    "        curfine1_2 = CNB.extractPatches(layer1_2,pred,self.conf,2,2)\n",
    "        curfine2_1 = CNB.extractPatches(layer2_1,pred,self.conf,2,2)\n",
    "        curfine2_2 = CNB.extractPatches(layer2_2,pred,self.conf,4,1)\n",
    "        curfine1_1u = tf.unpack(tf.transpose(curfine1_1,[1,0,2,3,4]))\n",
    "        curfine1_2u = tf.unpack(tf.transpose(curfine1_2,[1,0,2,3,4]))\n",
    "        curfine2_1u = tf.unpack(tf.transpose(curfine2_1,[1,0,2,3,4]))\n",
    "        curfine2_2u = tf.unpack(tf.transpose(curfine2_2,[1,0,2,3,4]))\n",
    "        finepred = CNB.fineOut(curfine1_1u,curfine1_2u,curfine2_1u,curfine2_2u,\n",
    "                               curfine7u,self.conf,doBatch,\n",
    "                               self.ph['phase_train_fine'])    \n",
    "        limgs = PoseTools.createFineLabelImages(self.ph['locs'],\n",
    "                                                pred,self.conf,labelT)\n",
    "        self.finePred = finepred\n",
    "        self.fine_labels = limgs\n",
    "\n",
    "    # ---------------- SAVING/RESTORING ---------------------\n",
    "\n",
    "    def createBaseSaver(self):\n",
    "        self.basesaver = tf.train.Saver(var_list = PoseTools.getvars('base'),\n",
    "                                        max_to_keep=self.conf.maxckpt)\n",
    "        \n",
    "    def createACSaver(self):\n",
    "        self.acsaver = tf.train.Saver(var_list = PoseTools.getvars('AC_'),\n",
    "                                       max_to_keep=self.conf.maxckpt)\n",
    "        \n",
    "    def createMRFSaver(self):\n",
    "        self.mrfsaver = tf.train.Saver(var_list = PoseTools.getvars('mrf'),\n",
    "                                       max_to_keep=self.conf.maxckpt)\n",
    "        \n",
    "    def createFineSaver(self):\n",
    "        self.finesaver = tf.train.Saver(var_list = PoseTools.getvars('fine'),\n",
    "                                        max_to_keep=self.conf.maxckpt)\n",
    "        \n",
    "    def createJointSaver(self):\n",
    "        vlist = PoseTools.getvars('fine') + \\\n",
    "                PoseTools.getvars('mrf') + \\\n",
    "                PoseTools.getvars('base') \n",
    "        self.jointsaver = tf.train.Saver(var_list = vlist,\n",
    "                                        max_to_keep=self.conf.maxckpt)\n",
    "        \n",
    "    def loadBase(self,sess,iterNum):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.baseoutname)\n",
    "        ckptfilename = '%s-%d'%(outfilename,iterNum)\n",
    "        print('Loading base from %s'%(ckptfilename))\n",
    "        self.basesaver.restore(sess,ckptfilename)\n",
    "            \n",
    "    def restoreBase(self,sess,restore):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.baseoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.basedataname)\n",
    "        latest_ckpt = tf.train.get_checkpoint_state(self.conf.cachedir,\n",
    "                                            latest_filename = self.conf.baseckptname)\n",
    "        if not latest_ckpt or not restore:\n",
    "            self.basestartat = 0\n",
    "            self.basetrainData = {'train_err':[], 'val_err':[], 'step_no':[],\n",
    "                                  'train_dist':[], 'val_dist':[] }\n",
    "            sess.run(tf.variables_initializer(PoseTools.getvars('base')),feed_dict=self.feed_dict)\n",
    "            print(\"Not loading base variables. Initializing them\")\n",
    "            return False\n",
    "        else:\n",
    "            self.basesaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "            matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "            self.basestartat = int(matchObj.group(1))+1\n",
    "            with open(traindatafilename,'rb') as tdfile:\n",
    "                inData = pickle.load(tdfile)\n",
    "                if not isinstance(inData,dict):\n",
    "                    self.basetrainData, loadconf = inData\n",
    "                    print('Parameters that dont match for base:')\n",
    "                    PoseTools.compareConf(self.conf, loadconf)\n",
    "                else:\n",
    "                    print(\"No config was stored for base. Not comparing conf\")\n",
    "                    self.basetrainData = inData\n",
    "            print(\"Loading base variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "            return True\n",
    "            \n",
    "    def restoreMRF(self,sess,restore):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.mrfoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.mrfdataname)\n",
    "        latest_ckpt = tf.train.get_checkpoint_state(self.conf.cachedir,\n",
    "                                            latest_filename = self.conf.mrfckptname)\n",
    "        if not latest_ckpt or not restore:\n",
    "            self.mrfstartat = 0\n",
    "            self.mrftrainData = {'train_err':[],'val_err':[],'step_no':[],\n",
    "                                'train_base_err':[],'val_base_err':[],\n",
    "                                 'train_dist':[],'val_dist':[],\n",
    "                                'train_base_dist':[],'val_base_dist':[]}\n",
    "            sess.run(tf.variables_initializer(PoseTools.getvars('mrf')))\n",
    "            print(\"Not loading mrf variables. Initializing them\")\n",
    "            return False\n",
    "        else:\n",
    "            self.mrfsaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "            matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "            self.mrfstartat = int(matchObj.group(1))+1\n",
    "            with open(traindatafilename,'rb') as tdfile:\n",
    "                inData = pickle.load(tdfile)\n",
    "                if not isinstance(inData,dict):\n",
    "                    self.mrftrainData, loadconf = inData\n",
    "                    print('Parameters that dont match for mrf:')\n",
    "                    PoseTools.compareConf(self.conf, loadconf)\n",
    "                else:\n",
    "                    print(\"No config was stored for mrf. Not comparing conf\")\n",
    "                    self.mrftrainData = inData\n",
    "            print(\"Loading mrf variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "            return True\n",
    "            \n",
    "    def restoreFine(self,sess,restore):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.fineoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.finedataname)\n",
    "        latest_ckpt = tf.train.get_checkpoint_state(self.conf.cachedir,\n",
    "                                            latest_filename = self.conf.fineckptname)\n",
    "        if not latest_ckpt or not restore:\n",
    "            self.finestartat = 0\n",
    "            self.finetrainData = {'train_err':[],'val_err':[],'step_no':[],\n",
    "                                'train_mrf_err':[],'val_mrf_err':[],\n",
    "                                'train_base_err':[],'val_base_err':[],\n",
    "                                'train_dist':[],'val_dist':[],\n",
    "                                'train_mrf_dist':[],'val_mrf_dist':[],\n",
    "                                'train_base_dist':[],'val_base_dist':[]}\n",
    "            sess.run(tf.initialize_variables(PoseTools.getvars('fine')))\n",
    "            print(\"Not loading fine variables. Initializing them\")\n",
    "            return False\n",
    "        else:\n",
    "            self.finesaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "            matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "            self.finestartat = int(matchObj.group(1))+1\n",
    "            with open(traindatafilename,'rb') as tdfile:\n",
    "                inData = pickle.load(tdfile)\n",
    "                if not isinstance(inData,dict):\n",
    "                    self.finetrainData, loadconf = inData\n",
    "                    print('Parameters that dont match for fine:')\n",
    "                    PoseTools.compareConf(self.conf, loadconf)\n",
    "                else:\n",
    "                    print(\"No conf was stored for fine. Not comparing conf\")\n",
    "                    self.finetrainData = inData\n",
    "            print(\"Loading fine variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "            return True\n",
    "\n",
    "    def restoreJoint(self,sess,restore):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.jointoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.jointdataname)\n",
    "        latest_ckpt = tf.train.get_checkpoint_state(self.conf.cachedir,\n",
    "                                latest_filename = self.conf.jointckptname)\n",
    "        if not latest_ckpt or not restore:\n",
    "            self.finestartat = 0\n",
    "            self.jointtrainData = {'train_err':[],'val_err':[],'step_no':[],\n",
    "                                'train_fine_err':[],'val_fine_err':[],\n",
    "                                'train_mrf_err':[],'val_mrf_err':[],\n",
    "                                'train_base_err':[],'val_base_err':[],\n",
    "                                'train_dist':[],'val_dist':[],\n",
    "                                'train_fine_dist':[],'val_fine_dist':[],\n",
    "                                'train_mrf_dist':[],'val_mrf_dist':[],\n",
    "                                'train_base_dist':[],'val_base_dist':[]}\n",
    "            print(\"Not loading joint variables. Initializing them\")\n",
    "            return False\n",
    "        else:\n",
    "            self.jointsaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "            print(\"Loading joint variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "            matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "            self.jointstartat = int(matchObj.group(1))+1\n",
    "            with open(traindatafilename,'rb') as tdfile:\n",
    "                inData = pickle.load(tdfile)\n",
    "                if not isinstance(inData,dict):\n",
    "                    self.jointtrainData, loadconf = inData\n",
    "                    print('Parameters that dont match for joint:')\n",
    "                    PoseTools.compareConf(self.conf, loadconf)\n",
    "                else:\n",
    "                    print(\"No conf was stored for joint. Not comparing conf\")\n",
    "                    self.jointtrainData = inData\n",
    "            return True\n",
    "\n",
    "        \n",
    "    def saveBase(self,sess,step):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.baseoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.basedataname)\n",
    "        self.basesaver.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = self.conf.baseckptname)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        with open(traindatafilename,'wb') as tdfile:\n",
    "            pickle.dump([self.basetrainData,self.conf],tdfile)\n",
    "            \n",
    "    def saveAC(self,sess,step):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.acoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.acdataname)\n",
    "        self.acsaver.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = self.conf.acckptname)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        with open(traindatafilename,'wb') as tdfile:\n",
    "            pickle.dump([self.actrainData,self.conf],tdfile)\n",
    "\n",
    "    def saveMRF(self,sess,step):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.mrfoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.mrfdataname)\n",
    "        self.mrfsaver.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = self.conf.mrfckptname)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        with open(traindatafilename,'wb') as tdfile:\n",
    "            pickle.dump([self.mrftrainData,self.conf],tdfile)\n",
    "\n",
    "    def saveFine(self,sess,step):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.fineoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.finedataname)\n",
    "        self.finesaver.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = self.conf.fineckptname)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        with open(traindatafilename,'wb') as tdfile:\n",
    "            pickle.dump([self.finetrainData,self.conf],tdfile)\n",
    "\n",
    "    def saveJoint(self,sess,step):\n",
    "        outfilename = os.path.join(self.conf.cachedir,self.conf.jointoutname)\n",
    "        traindatafilename = os.path.join(self.conf.cachedir,self.conf.jointdataname)\n",
    "        self.finesaver.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = self.conf.jointckptname)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        with open(traindatafilename,'wb') as tdfile:\n",
    "            pickle.dump([self.jointtrainData,self.conf],tdfile)\n",
    "\n",
    "    def initializeRemainingVars(self,sess):\n",
    "        varlist = tf.global_variables()\n",
    "        for var in varlist:\n",
    "            try:\n",
    "                sess.run(tf.assert_variables_initialized([var]))\n",
    "            except tf.errors.FailedPreconditionError:\n",
    "                sess.run(tf.initialize_variables([var]))\n",
    "                print('Initializing variable:%s'%var.name)\n",
    "                \n",
    "                \n",
    "    # ---------------- OPTIMIZATION/LOSS ---------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    def createOptimizer(self):\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate= \\\n",
    "                          self.ph['learning_rate']).minimize(self.cost)\n",
    "        self.read_time = 0.\n",
    "        self.opt_time = 0.\n",
    "\n",
    "    def doOpt(self,sess,step,learning_rate):\n",
    "        excount = step*self.conf.batch_size\n",
    "        cur_lr = learning_rate * \\\n",
    "                self.conf.gamma**math.floor(excount/self.conf.step_size)\n",
    "        self.feed_dict[self.ph['learning_rate']] = cur_lr\n",
    "        self.feed_dict[self.ph['keep_prob']] = self.conf.dropout\n",
    "        r_start = time.clock()\n",
    "        self.updateFeedDict(self.DBType.Train,distort=True,sess=sess)\n",
    "        r_end = time.clock()\n",
    "        sess.run(self.opt, self.feed_dict)\n",
    "        o_end = time.clock()\n",
    "        \n",
    "        self.read_time += r_end-r_start\n",
    "        self.opt_time += o_end-r_end\n",
    "\n",
    "    def computeLoss(self,sess,costfcns):\n",
    "        self.feed_dict[self.ph['keep_prob']] = 1.\n",
    "        loss = sess.run(costfcns,self.feed_dict)\n",
    "        loss = [x/self.conf.batch_size for x in loss]\n",
    "        return loss\n",
    "    \n",
    "    def computePredDist(self,sess,predfcn):\n",
    "        self.feed_dict[self.ph['keep_prob']] = 1.\n",
    "        pred = sess.run(predfcn,self.feed_dict)\n",
    "        bee = PoseTools.getBaseError(self.locs,pred,self.conf)\n",
    "        dee = np.sqrt(np.sum(np.square(bee),2))\n",
    "        return dee\n",
    "        \n",
    "    def computeFinePredDist(self,sess,predfcn):\n",
    "        self.feed_dict[self.ph['keep_prob']] = 1.\n",
    "        pred = sess.run(predfcn,self.feed_dict)\n",
    "        base_ee,fine_ee = PoseTools.getFineError(self.locs,pred[0],pred[1],self.conf)\n",
    "        base_dist = np.sqrt(np.sum(np.square(base_ee),2))\n",
    "        fine_dist = np.sqrt(np.sum(np.square(fine_ee),2))\n",
    "        return fine_dist\n",
    "\n",
    "    def updateBaseLoss(self,step,train_loss,val_loss,trainDist,valDist):\n",
    "        print \"Iter \" + str(step) + \\\n",
    "             \", Train = \" + \"{:.3f},{:.1f}\".format(train_loss[0],trainDist[0]) + \\\n",
    "             \", Val = \" + \"{:.3f},{:.1f}\".format(val_loss[0],valDist[0])\n",
    "#         nstep = step-self.basestartat\n",
    "#         print \"  Read Time:\" + \"{:.2f}, \".format(self.read_time/(nstep+1)) + \\\n",
    "#               \"Opt Time:\" + \"{:.2f}\".format(self.opt_time/(nstep+1)) \n",
    "        self.basetrainData['train_err'].append(train_loss[0])      \n",
    "        self.basetrainData['val_err'].append(val_loss[0])        \n",
    "        self.basetrainData['step_no'].append(step)        \n",
    "        self.basetrainData['train_dist'].append(trainDist[0])        \n",
    "        self.basetrainData['val_dist'].append(valDist[0])        \n",
    "\n",
    "    def updateMRFLoss(self,step,train_loss,val_loss,trainDist,valDist):\n",
    "        print \"Iter \" + str(step) + \\\n",
    "             \", Train = \" + \"{:.3f},{:.1f}\".format(train_loss[0],trainDist[0]) + \\\n",
    "             \", Val = \" + \"{:.3f},{:.1f}\".format(val_loss[0],valDist[0]) + \\\n",
    "             \" ({:.1f},{:.1f}),({:.1f},{:.1f})\".format(train_loss[1],val_loss[1],\n",
    "                                                      trainDist[1],valDist[1])\n",
    "        self.mrftrainData['train_err'].append(train_loss[0])        \n",
    "        self.mrftrainData['val_err'].append(val_loss[0])        \n",
    "        self.mrftrainData['train_base_err'].append(train_loss[1])        \n",
    "        self.mrftrainData['val_base_err'].append(val_loss[1])        \n",
    "        self.mrftrainData['train_dist'].append(trainDist[0])        \n",
    "        self.mrftrainData['val_dist'].append(valDist[0])        \n",
    "        self.mrftrainData['train_base_dist'].append(trainDist[1])        \n",
    "        self.mrftrainData['val_base_dist'].append(valDist[1])        \n",
    "        self.mrftrainData['step_no'].append(step)        \n",
    "\n",
    "    def updateFineLoss(self,step,train_loss,val_loss,trainDist,valDist):\n",
    "        print \"Iter \" + str(step) + \\\n",
    "             \", Train = \" + \"{:.3f},{:.1f}\".format(train_loss[0],trainDist[0]) + \\\n",
    "             \", Val = \" + \"{:.3f},{:.1f}\".format(val_loss[0],valDist[0]) + \\\n",
    "             \" (MRF:{:.1f},{:.1f},{:.1f},{:.1f})\".format(train_loss[1],val_loss[1],trainDist[1],valDist[1]) + \\\n",
    "             \" (Base:{:.1f},{:.1f},{:.1f},{:.1f})\".format(train_loss[2],val_loss[2],trainDist[2],valDist[2])\n",
    "        self.finetrainData['train_err'].append(train_loss[0])        \n",
    "        self.finetrainData['val_err'].append(val_loss[0])        \n",
    "        self.finetrainData['train_mrf_err'].append(train_loss[1])        \n",
    "        self.finetrainData['val_mrf_err'].append(val_loss[1])        \n",
    "        self.finetrainData['train_base_err'].append(train_loss[2])        \n",
    "        self.finetrainData['val_base_err'].append(val_loss[2])        \n",
    "        self.finetrainData['step_no'].append(step)        \n",
    "        self.finetrainData['train_dist'].append(trainDist[0])        \n",
    "        self.finetrainData['val_dist'].append(valDist[0])        \n",
    "        self.finetrainData['train_mrf_dist'].append(trainDist[1])        \n",
    "        self.finetrainData['val_mrf_dist'].append(valDist[1])        \n",
    "        self.finetrainData['train_base_dist'].append(trainDist[2])        \n",
    "        self.finetrainData['val_base_dist'].append(valDist[2])        \n",
    "\n",
    "    def updateJointLoss(self,step,train_loss,val_loss):\n",
    "        print \"Iter \" + str(step) + \\\n",
    "             \", Train = \" + \"{:.3f}\".format(train_loss[0]) + \\\n",
    "             \", Val = \" + \"{:.3f}\".format(val_loss[0]) + \\\n",
    "             \" (Fine:{:.1f},{:.1f})\".format(train_loss[1],val_loss[1]) + \\\n",
    "             \" (MRF:{:.1f},{:.1f})\".format(train_loss[2],val_loss[2]) + \\\n",
    "             \" (Base:{:.1f},{:.1f})\".format(train_loss[3],val_loss[3])\n",
    "        self.jointtrainData['train_err'].append(train_loss)        \n",
    "        self.jointftrainData['val_err'].append(val_loss[0])        \n",
    "        self.jointtrainData['train_fine_err'].append(train_loss[1])        \n",
    "        self.jointtrainData['val_fine_err'].append(val_loss[1])        \n",
    "        self.jointtrainData['train_mrf_err'].append(train_loss[2])        \n",
    "        self.jointtrainData['val_mrf_err'].append(val_loss[2])        \n",
    "        self.jointtrainData['train_base_err'].append(train_loss[3])        \n",
    "        self.jointtrainData['val_base_err'].append(val_loss[3])        \n",
    "        self.jointtrainData['step_no'].append(step)        \n",
    "\n",
    "        \n",
    "\n",
    "    # ---------------- TRAINING ---------------------\n",
    "\n",
    "    \n",
    "    \n",
    "    def baseTrain(self, restore=True, trainPhase=True,trainType=0):\n",
    "        self.createPH()\n",
    "        self.createFeedDict()\n",
    "        self.feed_dict[self.ph['phase_train_base']] = trainPhase\n",
    "        self.feed_dict[self.ph['keep_prob']] = 0.5\n",
    "        self.trainType = trainType\n",
    "        doBatchNorm = self.conf.doBatchNorm\n",
    "        \n",
    "        with tf.variable_scope('base'):\n",
    "            self.createBaseNetwork(doBatchNorm)\n",
    "        self.cost = tf.nn.l2_loss(self.basePred-self.ph['y'])\n",
    "        self.openDBs()\n",
    "        self.createOptimizer()\n",
    "        self.createBaseSaver()\n",
    "\n",
    "#         with self.env.begin() as txn,\\\n",
    "#                  self.valenv.begin() as valtxn,\\\n",
    "        with tf.Session() as sess:\n",
    "                    \n",
    "            self.createCursors(sess)\n",
    "            self.updateFeedDict(self.DBType.Train,sess=sess,distort=True)\n",
    "            self.restoreBase(sess,restore)\n",
    "            self.initializeRemainingVars(sess)\n",
    "            \n",
    "            for step in range(self.basestartat,self.conf.base_training_iters+1):\n",
    "                self.feed_dict[self.ph['keep_prob']] = 0.5\n",
    "                self.doOpt(sess,step,self.conf.base_learning_rate)\n",
    "                if step % self.conf.display_step == 0:\n",
    "                    self.updateFeedDict(self.DBType.Train,sess=sess,distort=True)\n",
    "                    self.feed_dict[self.ph['keep_prob']] = 1.\n",
    "                    train_loss = self.computeLoss(sess,[self.cost])\n",
    "                    tt1 = self.computePredDist(sess,self.basePred)\n",
    "                    trainDist = [tt1.mean()]\n",
    "                    numrep = int(self.conf.numTest/self.conf.batch_size)+1\n",
    "                    val_loss = np.zeros([2,])\n",
    "                    valDist = [0.]\n",
    "                    for rep in range(numrep):\n",
    "                        self.updateFeedDict(self.DBType.Val,distort=False,sess=sess)\n",
    "                        val_loss += np.array(self.computeLoss(sess,[self.cost]))\n",
    "                        tt1 = self.computePredDist(sess,self.basePred)\n",
    "                        valDist = [valDist[0]+tt1.mean()]\n",
    "                    val_loss = val_loss/numrep\n",
    "                    valDist = [valDist[0]/numrep]\n",
    "                    self.updateBaseLoss(step,train_loss,val_loss,trainDist,valDist)\n",
    "                if step % self.conf.save_step == 0:\n",
    "                    self.saveBase(sess,step)\n",
    "            print(\"Optimization Finished!\")\n",
    "            self.saveBase(sess,step)\n",
    "    \n",
    "    \n",
    "    def mrfTrain(self,restore=True,trainType=0):\n",
    "        self.createPH()\n",
    "        self.createFeedDict()\n",
    "        doBatchNorm = self.conf.doBatchNorm\n",
    "        self.feed_dict[self.ph['keep_prob']] = 0.5\n",
    "        self.feed_dict[self.ph['phase_train_base']] = False\n",
    "        self.trainType = trainType\n",
    "        \n",
    "        with tf.variable_scope('base'):\n",
    "            self.createBaseNetwork(doBatchNorm)\n",
    "\n",
    "        with tf.variable_scope('mrf'):\n",
    "            self.createMRFNetwork(doBatchNorm)\n",
    "\n",
    "        self.createBaseSaver()\n",
    "        self.createMRFSaver()\n",
    "\n",
    "        mod_labels = tf.maximum((self.ph['y']+1.)/2,0.01)\n",
    "# the labels shouldn't be zero because the prediction is an output of\n",
    "# exp. And it seems a lot of effort is wasted to make the prediction goto\n",
    "# zero rather than match the location.\n",
    "        \n",
    "#         self.cost = tf.nn.l2_loss(self.mrfPred-mod_labels)\n",
    "        self.cost = tf.nn.l2_loss(2*(self.mrfPred-0.5)-self.ph['y'])\n",
    "        basecost  = tf.nn.l2_loss(self.basePred-self.ph['y'])\n",
    "        \n",
    "        if self.conf.useHoldout:\n",
    "            self.openHoldoutDBs()\n",
    "        else:\n",
    "            self.openDBs()\n",
    "\n",
    "        self.createOptimizer()\n",
    "\n",
    "#         self.env.begin() as txn,self.valenv.begin() as valtxn,\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            self.restoreBase(sess,restore=True)\n",
    "            self.restoreMRF(sess,restore)\n",
    "            self.initializeRemainingVars(sess)\n",
    "            self.createCursors(sess)\n",
    "            \n",
    "            for step in range(self.mrfstartat,self.conf.mrf_training_iters+1):\n",
    "                self.feed_dict[self.ph['keep_prob']] = 0.5\n",
    "                self.doOpt(sess,step,self.conf.mrf_learning_rate)\n",
    "                if step % self.conf.display_step == 0:\n",
    "                    self.feed_dict[self.ph['keep_prob']] = 1.0\n",
    "                    self.updateFeedDict(self.DBType.Train,sess=sess,distort=True)\n",
    "                    train_loss = self.computeLoss(sess,[self.cost,basecost])\n",
    "                    tt1 = self.computePredDist(sess,self.mrfPred)\n",
    "                    tt2 = self.computePredDist(sess,self.basePred)\n",
    "                    trainDist = [tt1.mean(),tt2.mean()]\n",
    "\n",
    "                    numrep = int(self.conf.numTest/self.conf.batch_size)+1\n",
    "                    val_loss = np.zeros([2,])\n",
    "                    valDist = [0.,0.]\n",
    "                    for rep in range(numrep):\n",
    "                        self.updateFeedDict(self.DBType.Val,sess=sess,distort=False)\n",
    "                        val_loss += np.array(self.computeLoss(sess,[self.cost,basecost]))\n",
    "                        tt1 = self.computePredDist(sess,self.mrfPred)\n",
    "                        tt2 = self.computePredDist(sess,self.basePred)\n",
    "                        valDist = [valDist[0]+tt1.mean(),valDist[1]+tt2.mean()]\n",
    "                        \n",
    "                    val_loss = val_loss/numrep\n",
    "                    valDist = [valDist[0]/numrep,valDist[1]/numrep]\n",
    "                    self.updateMRFLoss(step,train_loss,val_loss,trainDist,valDist)\n",
    "                if step % self.conf.save_step == 0:\n",
    "                    self.saveMRF(sess,step)\n",
    "            print(\"Optimization Finished!\")\n",
    "            self.saveMRF(sess,step)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def fineTrain(self, restore=True,trainPhase=True,trainType=0):\n",
    "        self.createPH()\n",
    "        self.createFeedDict()\n",
    "        self.openDBs()\n",
    "        self.feed_dict[self.ph['phase_train_fine']] = trainPhase\n",
    "        self.feed_dict[self.ph['phase_train_base']] = False\n",
    "        doBatchNorm = self.conf.doBatchNorm\n",
    "        self.trainType = trainType\n",
    "        \n",
    "        with tf.variable_scope('base'):\n",
    "            self.createBaseNetwork(doBatchNorm)\n",
    "        with tf.variable_scope('mrf'):\n",
    "            self.createMRFNetwork(doBatchNorm)\n",
    "        with tf.variable_scope('fine'):\n",
    "            self.createFineNetwork(doBatchNorm)\n",
    "\n",
    "        self.createBaseSaver()\n",
    "        self.createMRFSaver()\n",
    "        self.createFineSaver()\n",
    "\n",
    "        mod_labels = tf.maximum((self.ph['y']+1.)/2,0.01)\n",
    "        # the labels shouldn't be zero because the prediction is an output of\n",
    "        # exp. And it seems a lot of effort is wasted to make the prediction goto\n",
    "        # zero rather than match the location.\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss(self.finePred-tf.to_float(self.fine_labels))\n",
    "        mrfcost = tf.nn.l2_loss(self.mrfPred-mod_labels)\n",
    "        basecost =  tf.nn.l2_loss(self.basePred-self.ph['y'])\n",
    "        self.createOptimizer()\n",
    "        if self.conf.useMRF:\n",
    "            predPair = [self.mrfPred,self.finePred]\n",
    "        else:\n",
    "            predPair = [self.basePred,self.finePred]\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            self.restoreBase(sess,True)\n",
    "            self.restoreMRF(sess,True)\n",
    "            self.restoreFine(sess,restore)\n",
    "            self.initializeRemainingVars(sess)\n",
    "            self.createCursors(sess)\n",
    "            \n",
    "            for step in range(self.finestartat,self.conf.fine_training_iters+1):\n",
    "                self.doOpt(sess,step,self.conf.fine_learning_rate)\n",
    "                if step % self.conf.display_step == 0:\n",
    "                    self.updateFeedDict(self.DBType.Train,distort=True,sess=sess)\n",
    "                    train_loss = self.computeLoss(sess,[self.cost,mrfcost,basecost])\n",
    "                    tt1 = self.computePredDist(sess,self.basePred)\n",
    "                    tt2 = self.computePredDist(sess,self.mrfPred)\n",
    "                    tt3 = self.computeFinePredDist(sess,predPair)\n",
    "\n",
    "                    trainDist = [tt3.mean(),tt2.mean(),tt1.mean()]\n",
    "\n",
    "                    numrep = int(self.conf.numTest/self.conf.batch_size)+1\n",
    "                    val_loss = np.zeros([3,])\n",
    "                    valDist = [0.,0.,0.]\n",
    "                    for rep in range(numrep):\n",
    "                        self.updateFeedDict(self.DBType.Val,distort=False,sess=sess)\n",
    "                        val_loss += np.array(self.computeLoss(sess,[self.cost,mrfcost,basecost]))\n",
    "                        tt1 = self.computePredDist(sess,self.basePred)\n",
    "                        tt2 = self.computePredDist(sess,self.mrfPred)\n",
    "                        tt3 = self.computeFinePredDist(sess,predPair)\n",
    "                        valDist = [valDist[0]+tt3.mean(),\n",
    "                                   valDist[1]+tt2.mean(),\n",
    "                                   valDist[2]+tt1.mean()]\n",
    "                        \n",
    "                    val_loss = val_loss/numrep\n",
    "                    valDist = [valDist[0]/numrep,valDist[1]/numrep,valDist[2]/numrep]\n",
    "                    self.updateFineLoss(step,train_loss,val_loss,trainDist,valDist)\n",
    "                if step % self.conf.save_step == 0:\n",
    "                    self.saveFine(sess,step)\n",
    "            print(\"Optimization Finished!\")\n",
    "            self.saveFine(sess,step)\n",
    "\n",
    "    def jointTrain(self, restore=True):\n",
    "        self.createPH()\n",
    "        self.createFeedDict()\n",
    "        self.openDBs()\n",
    "        self.feed_dict[self.ph['phase_train_base']]=True\n",
    "        self.feed_dict[self.ph['phase_train_fine']]=True\n",
    "        doBatchNorm = self.conf.doBatchNorm\n",
    "        \n",
    "        with tf.variable_scope('base'):\n",
    "            self.createBaseNetwork(doBatchNorm)\n",
    "        with tf.variable_scope('mrf'):\n",
    "            self.createMRFNetwork(doBatchNorm)\n",
    "        with tf.variable_scope('fine'):\n",
    "            self.createFineNetwork(doBatchNorm)\n",
    "\n",
    "        self.createBaseSaver()\n",
    "        self.createMRFSaver()\n",
    "        self.createACSaver()\n",
    "        self.createFineSaver()\n",
    "        self.createJointSaver()\n",
    "\n",
    "        mod_labels = tf.maximum((self.ph['y']+1.)/2,0.01)\n",
    "        # the labels shouldn't be zero because the prediction is an output of\n",
    "        # exp. And it seems a lot of effort is wasted to make the prediction goto\n",
    "        # zero rather than match the location.\n",
    "        \n",
    "        finecost = tf.nn.l2_loss(self.finePred-self.fine_labels) \n",
    "        mrfcost = tf.nn.l2_loss(self.mrfPred-mod_labels)\n",
    "        basecost =  tf.nn.l2_loss(self.basePred-self.ph['y'])\n",
    "        self.cost = finecost + self.conf.joint_MRFweight*mrfcost\n",
    "        \n",
    "        self.createOptimizer()\n",
    "        \n",
    "        with self.env.begin() as txn,self.valenv.begin() as valtxn,tf.Session() as sess:\n",
    "\n",
    "            self.restoreBase(sess,True)\n",
    "            self.restoreMRF(sess,True)\n",
    "            self.restoreFine(sess,True)\n",
    "            self.restoreJoint(sess,restore)\n",
    "            self.initializeRemainingVars(sess)\n",
    "            self.createCursors(txn,valtxn)\n",
    "            \n",
    "            for step in range(self.jointstartat,self.conf.joint_training_iters+1):\n",
    "                self.doOpt(sess,step,self.conf.joint_learning_rate)\n",
    "                if step % self.conf.display_step == 0:\n",
    "                    self.updateFeedDict(self.DBType.Train)\n",
    "                    train_loss = self.computeLoss(sess,[self.cost,finecost,mrfcost,basecost])\n",
    "                    numrep = int(self.conf.numTest/self.conf.batch_size)+1\n",
    "                    val_loss = np.zeros([2,])\n",
    "                    for rep in range(numrep):\n",
    "                        self.updateFeedDict(self.DBType.Val)\n",
    "                        val_loss += np.array(self.computeLoss(sess,[self.cost,finecost,mrfcost,basecost]))\n",
    "                    val_loss = val_loss/numrep\n",
    "                    self.updateJointLoss(step,train_loss,val_loss)\n",
    "                if step % self.conf.save_step == 0:\n",
    "                    self.saveJoint(sess,step)\n",
    "            print(\"Optimization Finished!\")\n",
    "            self.saveJoint(sess,step)\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
