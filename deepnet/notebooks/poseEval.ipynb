{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import cv2\n",
    "import tempfile\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from batch_norm import *\n",
    "import myutils\n",
    "import PoseTools\n",
    "import localSetup\n",
    "import operator\n",
    "import copy\n",
    "import convNetBase as CNB\n",
    "import multiResData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(X, kernel_shape, conv_std,bias_val,doBatchNorm,trainPhase,addSummary=True):\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer(stddev=conv_std))\n",
    "    biases = tf.get_variable(\"biases\", kernel_shape[-1],\n",
    "        initializer=tf.constant_initializer(bias_val))\n",
    "    if addSummary:\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "#     PoseTools.variable_summaries(biases)\n",
    "    conv = tf.nn.conv2d(X, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if doBatchNorm:\n",
    "        conv = batch_norm(conv,trainPhase)\n",
    "    with tf.variable_scope('conv'):\n",
    "        PoseTools.variable_summaries(conv)\n",
    "    return tf.nn.relu(conv - biases)\n",
    "\n",
    "def max_pool(name, l_input, k,s):\n",
    "    return tf.nn.max_pool(\n",
    "        l_input, ksize=[1, k, k, 1], strides=[1, s, s, 1], \n",
    "        padding='SAME', name=name)\n",
    "\n",
    "def createPlaceHolders(conf):\n",
    "#     imsz = conf.imsz\n",
    "    # tf Graph input\n",
    "    imsz = conf.imsz\n",
    "    rescale = conf.rescale\n",
    "    scale = conf.scale\n",
    "    pool_scale = conf.pool_scale\n",
    "    n_classes = conf.n_classes\n",
    "    imgDim = conf.imgDim\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32,name='dropout') # dropout(keep probability)\n",
    "    inScale = rescale*conf.eval_scale\n",
    "    \n",
    "    nex = conf.batch_size*(conf.eval_num_neg+1)\n",
    "    x0 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/inScale,\n",
    "                                     imsz[1]/inScale,imgDim],name='x0')\n",
    "    x1 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/scale/inScale,\n",
    "                                     imsz[1]/scale/inScale,imgDim],name='x1')\n",
    "    x2 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/scale/scale/inScale,\n",
    "                                     imsz[1]/scale/scale/inScale,imgDim],name='x2')\n",
    "\n",
    "    scores_scale = scale*scale*inScale\n",
    "    s0 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/scores_scale,\n",
    "                                     imsz[1]/scores_scale,n_classes],name='s0')\n",
    "    s1 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/scale/scores_scale,\n",
    "                                     imsz[1]/scale/scores_scale,n_classes],name='s1')\n",
    "    s2 = tf.placeholder(tf.float32, [nex,\n",
    "                                     imsz[0]/scale/scale/scores_scale,\n",
    "                                     imsz[1]/scale/scale/scores_scale,n_classes],name='s2')\n",
    "    \n",
    "    y = tf.placeholder(tf.float32, [nex*n_classes,],'out')\n",
    "    \n",
    "    X = [x0,x1,x2]\n",
    "    S = [s0,s1,s2]\n",
    "    phase_train = tf.placeholder(tf.bool,name='phase_train')\n",
    "    learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    \n",
    "    ph = {'X':X,'S':S,'y':y,'keep_prob':keep_prob,\n",
    "          'phase_train':phase_train,'learning_rate':learning_rate}\n",
    "    return ph\n",
    "\n",
    "\n",
    "def createFeedDict(ph):\n",
    "    feed_dict = { ph['X'][0]:[],\n",
    "                  ph['X'][1]:[],\n",
    "                  ph['X'][2]:[],\n",
    "                  ph['S'][0]:[],\n",
    "                  ph['S'][1]:[],\n",
    "                  ph['S'][2]:[],\n",
    "                  ph['y']:[],\n",
    "                  ph['learning_rate']:1,\n",
    "                  ph['phase_train']:False,\n",
    "                  ph['keep_prob']:1.\n",
    "                  }\n",
    "    \n",
    "    return feed_dict\n",
    "\n",
    "def net_multi_base_named(X,nfilt,doBatchNorm,trainPhase,doPool=True,addSummary=True):\n",
    "    inDimX = X.get_shape()[3]\n",
    "    with tf.variable_scope('layer1_X'):\n",
    "        conv1 = conv_relu(X,[5, 5, inDimX, 48],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "    \n",
    "#     inDimS = S.get_shape()[3]\n",
    "#     with tf.variable_scope('layer1_S'):\n",
    "#         conv1s = conv_relu(S,[5, 5, inDimS, 48],0.01,0,doBatchNorm,trainPhase)\n",
    "            \n",
    "#     conv1_cat = tf.concat(3,[conv1x,conv1s])\n",
    "    if doPool:\n",
    "        pool1 = max_pool('pool1',conv1,k=3,s=2)\n",
    "    else:\n",
    "        pool1 = conv1\n",
    "            \n",
    "    with tf.variable_scope('layer2'):\n",
    "        conv2 = conv_relu(pool1,[3,3,48,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "    if doPool:\n",
    "        pool2 = max_pool('pool2',conv2,k=3,s=2)\n",
    "    else:\n",
    "        pool2 = conv2\n",
    "            \n",
    "    with tf.variable_scope('layer3'):\n",
    "        conv3 = conv_relu(pool2,[3,3,nfilt,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "    with tf.variable_scope('layer4'):\n",
    "        conv4 = conv_relu(conv3,[3,3,nfilt,nfilt],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "    with tf.variable_scope('layer5'):\n",
    "        conv5 = conv_relu(conv4,[3,3,nfilt,nfilt/4],0.01,0,doBatchNorm,trainPhase,addSummary)\n",
    "        \n",
    "    out_dict = {'conv1':conv1,'conv2':conv2,'conv3':conv3,\n",
    "                'conv4':conv4,'conv5':conv5}\n",
    "    return conv5,out_dict\n",
    "        \n",
    "\n",
    "def net_multi_conv(ph,conf):\n",
    "    X = ph['X']\n",
    "    S = ph['S']\n",
    "    X0,X1,X2 = X\n",
    "    S0,S1,S2 = S\n",
    "    \n",
    "    trainPhase = ph['phase_train']\n",
    "    _dropout = ph['keep_prob']\n",
    "    \n",
    "    imsz = conf.imsz; rescale = conf.rescale\n",
    "    pool_scale = conf.pool_scale\n",
    "    nfilt = conf.nfilt\n",
    "    doBatchNorm = conf.doBatchNorm\n",
    "    \n",
    "    #     conv5_0,base_dict_0 = net_multi_base(X0,_weights['base0'])\n",
    "    #     conv5_1,base_dict_1 = net_multi_base(X1,_weights['base1'])\n",
    "    #     conv5_2,base_dict_2 = net_multi_base(X2,_weights['base2'])\n",
    "    with tf.variable_scope('scale0'):\n",
    "        conv5_0,base_dict_0 = net_multi_base_named(X0,nfilt,doBatchNorm,trainPhase,True,True)\n",
    "    with tf.variable_scope('scale1'):\n",
    "        conv5_1,base_dict_1 = net_multi_base_named(X1,nfilt,doBatchNorm,trainPhase,True,False)\n",
    "    with tf.variable_scope('scale2'):\n",
    "        conv5_2,base_dict_2 = net_multi_base_named(X2,nfilt,doBatchNorm,trainPhase,True,False)\n",
    "    with tf.variable_scope('scale0_scores'):\n",
    "        conv5s_0,base_dict_s = net_multi_base_named(S0,nfilt,doBatchNorm,trainPhase,False,True)\n",
    "    with tf.variable_scope('scale1_scores'):\n",
    "        conv5s_1,base_dict_s = net_multi_base_named(S1,nfilt,doBatchNorm,trainPhase,False,False)\n",
    "    with tf.variable_scope('scale2_scores'):\n",
    "        conv5s_2,base_dict_s = net_multi_base_named(S2,nfilt,doBatchNorm,trainPhase,False,False)\n",
    "\n",
    "#     sz0 = int(math.ceil(float(imsz[0])/pool_scale/rescale/conf.eval_scale))\n",
    "#     sz1 = int(math.ceil(float(imsz[1])/pool_scale/rescale/conf.eval_scale))\n",
    "    sz = tf.Tensor.get_shape(conv5_2).as_list()\n",
    "    sz0 = sz[1]\n",
    "    sz1 = sz[2]\n",
    "    conv5_0_down = CNB.upscale('5_0',conv5_0,[sz0,sz1])\n",
    "    conv5_1_down = CNB.upscale('5_1',conv5_1,[sz0,sz1])\n",
    "    conv5s_0_down = CNB.upscale('5s_0',conv5s_0,[sz0,sz1])\n",
    "    conv5s_1_down = CNB.upscale('5s_1',conv5s_1,[sz0,sz1])\n",
    "\n",
    "    # crop lower res layers to match higher res size\n",
    "#     conv5_0_sz = tf.Tensor.get_shape(conv5_0).as_list()\n",
    "#     conv5_1_sz = tf.Tensor.get_shape(conv5_1_up).as_list()\n",
    "#     crop_0 = int((sz0-conv5_0_sz[1])/2)\n",
    "#     crop_1 = int((sz1-conv5_0_sz[2])/2)\n",
    "\n",
    "#     curloc = [0,crop_0,crop_1,0]\n",
    "#     patchsz = tf.to_int32([-1,conv5_0_sz[1],conv5_0_sz[2],-1])\n",
    "#     conv5_1_up = tf.slice(conv5_1_up,curloc,patchsz)\n",
    "#     conv5_2_up = tf.slice(conv5_2_up,curloc,patchsz)\n",
    "#     conv5_1_final_sz = tf.Tensor.get_shape(conv5_1_up).as_list()\n",
    "\n",
    "    conv5_cat = tf.concat(3,[conv5_0_down,conv5_1_down,conv5_2,conv5s_0_down,conv5s_1_down,conv5s_2])\n",
    "    \n",
    "    nex = conf.batch_size*(conf.eval_num_neg+1)\n",
    "    conv5_reshape = tf.reshape(conv5_cat,[nex,-1])\n",
    "    conv5_dims = conv5_reshape.get_shape()[1].value\n",
    "    with tf.variable_scope('layer6'):\n",
    "#         weights = tf.get_variable(\"weights\", [conv5_dims,conf.nfcfilt/2],\n",
    "#             initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        weights = tf.get_variable(\"weights\", [conv5_dims,conf.nfcfilt/2],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt/2,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "        conv6 = tf.nn.relu(batch_norm_2D(tf.matmul(conv5_reshape, weights),trainPhase)-biases)\n",
    "        with tf.variable_scope('conv'):\n",
    "            PoseTools.variable_summaries(conv6)\n",
    "\n",
    "    with tf.variable_scope('layer7'):\n",
    "#         weights = tf.get_variable(\"weights\", [conf.nfcfilt/2,conf.nfcfilt/2],\n",
    "#             initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt/2,conf.nfcfilt/4],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt/4,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(biases)\n",
    "\n",
    "        conv7 = tf.nn.relu(batch_norm_2D(tf.matmul(conv6, weights),trainPhase)-biases)\n",
    "        with tf.variable_scope('conv'):\n",
    "            PoseTools.variable_summaries(conv7)\n",
    "\n",
    "    with tf.variable_scope('layer8'):\n",
    "        l8_weights = tf.get_variable(\"weights\", [conf.nfcfilt/4,conf.n_classes],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        l8_biases = tf.get_variable(\"biases\", conf.n_classes,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        out = tf.matmul(conv7, l8_weights) - l8_biases\n",
    "        with tf.variable_scope('weights'):\n",
    "            PoseTools.variable_summaries(l8_weights)\n",
    "        with tf.variable_scope('biases'):\n",
    "            PoseTools.variable_summaries(l8_biases)\n",
    "        with tf.variable_scope('out'):\n",
    "            PoseTools.variable_summaries(out)\n",
    "        nex = conf.batch_size*(conf.eval_num_neg+1)\n",
    "        out = tf.reshape(out,[nex*conf.n_classes]) \n",
    "        #this should keep all the outputs of an example together\n",
    "\n",
    "    out_dict = {'base_dict_0':base_dict_0,\n",
    "                'base_dict_1':base_dict_1,\n",
    "                'base_dict_2':base_dict_2,\n",
    "                'conv6':conv6,\n",
    "                'conv7':conv7,\n",
    "               }\n",
    "    \n",
    "    return out,out_dict\n",
    "\n",
    "def openDBs(conf,trainType=0):\n",
    "        if trainType == 0:\n",
    "            trainfilename =os.path.join(conf.cachedir,conf.trainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(conf.cachedir,conf.valfilename) + '.tfrecords'\n",
    "            train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            val_queue = tf.train.string_input_producer([valfilename])\n",
    "        else:\n",
    "            trainfilename =os.path.join(conf.cachedir,conf.fulltrainfilename) + '.tfrecords'\n",
    "            valfilename =os.path.join(conf.cachedir,conf.fulltrainfilename) + '.tfrecords'\n",
    "            train_queue = tf.train.string_input_producer([trainfilename])\n",
    "            val_queue = tf.train.string_input_producer([valfilename])\n",
    "        return [train_queue,val_queue]\n",
    "\n",
    "def createCursors(sess,queue,conf):\n",
    "            \n",
    "        train_queue,val_queue = queue\n",
    "        train_ims,train_locs,temp = multiResData.read_and_decode(train_queue,conf)\n",
    "        val_ims,val_locs,temp = multiResData.read_and_decode(val_queue,conf)\n",
    "        train_data = [train_ims,train_locs]\n",
    "        val_data = [val_ims,val_locs]\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "        return [train_data,val_data],coord,threads\n",
    "        \n",
    "\n",
    "def readImages(conf,dbType,distort,sess,data):\n",
    "    train_data,val_data = data\n",
    "    cur_data = val_data if (dbType=='val') \\\n",
    "            else train_data\n",
    "    xs = []; locs = []\n",
    "    \n",
    "    for ndx in range(conf.batch_size):\n",
    "        [curxs,curlocs] = sess.run(cur_data)\n",
    "        if np.ndim(curxs)<3:\n",
    "            xs.append(curxs[np.newaxis,:,:])\n",
    "        else:\n",
    "            xs.append(curxs)\n",
    "        locs.append(curlocs)\n",
    "    xs = np.array(xs)    \n",
    "    locs = np.array(locs)\n",
    "    locs = multiResData.sanitizelocs(locs)\n",
    "    if distort:\n",
    "        if conf.horzFlip:\n",
    "            xs,locs = PoseTools.randomlyFlipLR(xs,locs)\n",
    "        if conf.vertFlip:\n",
    "            xs,locs = PoseTools.randomlyFlipUD(xs,locs)\n",
    "        xs,locs = PoseTools.randomlyRotate(xs,locs,conf)\n",
    "        xs = PoseTools.randomlyAdjust(xs,conf)\n",
    "\n",
    "    return xs,locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateFeedDict(conf,dbType,distort,sess,data,feed_dict,ph):\n",
    "    xs,locs = readImages(conf,dbType,distort,sess,data)\n",
    "    labelims = PoseTools.createLabelImages(locs,\n",
    "                               conf.imsz,\n",
    "                               conf.rescale*conf.eval_scale,\n",
    "                               conf.label_blur_rad)\n",
    "    \n",
    "    nlocs = genNegSamples(labelims,locs,conf,nsamples=1,minlen=conf.eval_minlen,N=conf.N2move4neg)\n",
    "    \n",
    "   \n",
    "#     alllocs = np.concatenate([locs[...,np.newaxis],nlocs],axis=-1)\n",
    "    alllocs = nlocs\n",
    "    dd = alllocs-locs[...,np.newaxis] # distance of neg points to actual locations\n",
    "    ddist = np.sqrt(np.sum(dd**2,axis=2))\n",
    "    ind_labels = (ddist - conf.eval_minlen/2 )/conf.eval_minlen\n",
    "    ind_labels = ind_labels.clip(min=0,max=1)\n",
    "    ind_labels = np.transpose(ind_labels,[0,2,1])\n",
    "    ind_labels = ind_labels.reshape((-1,))\n",
    "#     ind_labels = np.concatenate([ind_labels,1-ind_labels],axis=1)\n",
    "    alllocs = alllocs.transpose([0,3,1,2])\n",
    "    alllocs = alllocs.reshape((-1,)+alllocs.shape[2:])\n",
    "    \n",
    "    allxs = np.tile(xs[...,np.newaxis],conf.eval_num_neg+1)\n",
    "    allxs = allxs.transpose([0,4,1,2,3])\n",
    "    allxs = np.reshape(allxs,[-1,allxs.shape[-3],allxs.shape[-2],allxs.shape[-1]])\n",
    "    \n",
    "    x0,x1,x2 = PoseTools.multiScaleImages(allxs.transpose([0,2,3,1]),\n",
    "                                          conf.rescale*conf.eval_scale,\n",
    "                                          conf.scale, conf.l1_cropsz,conf)\n",
    "\n",
    "\n",
    "    scores_scale = conf.rescale*conf.eval_scale*conf.scale*conf.scale\n",
    "    s0 = PoseTools.createLabelImages(alllocs,\n",
    "                               conf.imsz,\n",
    "                               scores_scale,\n",
    "                               conf.label_blur_rad)\n",
    "    s1 = PoseTools.createLabelImages(alllocs,\n",
    "                               conf.imsz,\n",
    "                               scores_scale*conf.scale,\n",
    "                               conf.label_blur_rad)\n",
    "    s2 = PoseTools.createLabelImages(alllocs,\n",
    "                               conf.imsz,\n",
    "                               scores_scale*conf.scale*conf.scale,\n",
    "                               conf.label_blur_rad)\n",
    "    \n",
    "#     s0,s1,s2 = PoseTools.multiScaleLabelImages(labelims,1,conf.scale,[])\n",
    "    \n",
    "#     y = np.zeros([xs.shape[0],nlocs.shape[-1]+1,2])\n",
    "#     y[:,:1,0] = 1. \n",
    "#     y[:,1:,1] = 1.\n",
    "#     y = np.reshape(y,[-1,y.shape[-1]])\n",
    " \n",
    "    feed_dict[ph['X'][0]] = x0\n",
    "    feed_dict[ph['X'][1]] = x1\n",
    "    feed_dict[ph['X'][2]] = x2\n",
    "    feed_dict[ph['S'][0]] = s0\n",
    "    feed_dict[ph['S'][1]] = s1\n",
    "    feed_dict[ph['S'][2]] = s2\n",
    "    feed_dict[ph['y']] = ind_labels\n",
    "    return alllocs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreEval(sess,evalsaver,restore,conf,feed_dict):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.evaloutname)\n",
    "    latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                        latest_filename = conf.evalckptname)\n",
    "    if not latest_ckpt or not restore:\n",
    "        evalstartat = 0\n",
    "        sess.run(tf.variables_initializer(PoseTools.getvars('eval')),feed_dict=feed_dict)\n",
    "        print(\"Not loading Eval variables. Initializing them\")\n",
    "    else:\n",
    "        evalsaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "        matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "        evalstartat = int(matchObj.group(1))+1\n",
    "        print(\"Loading eval variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "    return  evalstartat\n",
    "    \n",
    "\n",
    "def saveEval(sess,evalsaver,step,conf):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.evaloutname)\n",
    "    evalsaver.save(sess,outfilename,global_step=step,\n",
    "               latest_filename = conf.evalckptname)\n",
    "    print('Saved state to %s-%d' %(outfilename,step))\n",
    "\n",
    "def createEvalSaver(conf):\n",
    "    evalsaver = tf.train.Saver(var_list = PoseTools.getvars('eval'),\n",
    "                                    max_to_keep=conf.maxckpt)\n",
    "    return evalsaver\n",
    "\n",
    "def initializeRemainingVars(sess,feed_dict):\n",
    "    varlist = tf.global_variables()\n",
    "    for var in varlist:\n",
    "        try:\n",
    "            sess.run(tf.assert_variables_initialized([var]))\n",
    "        except tf.errors.FailedPreconditionError:\n",
    "            sess.run(tf.variables_initializer([var]))\n",
    "            print('Initializing variable:%s'%var.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalNetInit(conf):\n",
    "    \n",
    "    ph = createPlaceHolders(conf)\n",
    "    feed_dict = createFeedDict(ph)\n",
    "    with tf.variable_scope('eval'):\n",
    "        out,out_dict = net_multi_conv(ph,conf)\n",
    "    trainType = 0\n",
    "    queue = openDBs(conf,trainType=trainType)\n",
    "    if trainType == 1:\n",
    "        print \"Training with all the data!\"\n",
    "        print \"Validation data is same as training data!!!! \"\n",
    "    return ph,feed_dict,out,queue,out_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalTrain(conf,restore=True):\n",
    "    ph,feed_dict,out,queue,_ = poseEvalNetInit(conf)\n",
    "    feed_dict[ph['phase_train']] = True\n",
    "    feed_dict[ph['keep_prob']] = 1.\n",
    "    evalSaver = createEvalSaver(conf) \n",
    "    pwts = conf.eval_num_neg\n",
    "    \n",
    "    # For classification\n",
    "#     weights = ph['y'][:,0]*(pwts-1)+1\n",
    "#     cross_entropy = tf.nn.softmax_cross_entropy_with_logits(out,ph['y'])\n",
    "#     loss = tf.reduce_mean(cross_entropy)\n",
    "#     correct_pred = tf.cast(tf.equal(tf.argmax(out,1),tf.argmax(ph['y'],1)),tf.float32)\n",
    "#     correct_pred_weighted = tf.mul(correct_pred)\n",
    "#     accuracy_weighted = tf.reduce_sum(correct_pred_weighted)/tf.reduce_sum(weights)\n",
    "\n",
    "    # for regression.\n",
    "    loss = tf.nn.l2_loss(out-ph['y'])\n",
    "    correct_pred = tf.cast(tf.equal(out>0.5,ph['y']>0.5),tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_pred)\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy',loss)\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate= \\\n",
    "                      ph['learning_rate']).minimize(loss)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(conf.cachedir + '/eval_train_summary',sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(conf.cachedir + '/eval_test_summary',sess.graph)\n",
    "        data,coord,threads = createCursors(sess,queue,conf)\n",
    "        updateFeedDict(conf,'train',distort=True,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "        evalstartat = restoreEval(sess,evalSaver,restore,conf,feed_dict)\n",
    "        initializeRemainingVars(sess,feed_dict)\n",
    "        for step in range(evalstartat,conf.eval_training_iters+1):\n",
    "            excount = step*conf.batch_size\n",
    "            cur_lr = conf.eval_learning_rate\n",
    "                        #* conf.gamma**math.floor(excount/conf.step_size)\n",
    "            feed_dict[ph['learning_rate']] = cur_lr\n",
    "            feed_dict[ph['keep_prob']] = 1.\n",
    "            feed_dict[ph['phase_train']] = True\n",
    "            updateFeedDict(conf,'train',distort=True,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "            train_summary,_ = sess.run([merged,opt], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(train_summary,step)\n",
    "\n",
    "            if step % conf.display_step == 0:\n",
    "                updateFeedDict(conf,'train',sess=sess,distort=True,data=data,feed_dict=feed_dict,ph=ph)\n",
    "                feed_dict[ph['keep_prob']] = 1.\n",
    "                feed_dict[ph['phase_train']] = False\n",
    "                train_loss,train_acc = sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "                numrep = int(conf.numTest/conf.batch_size)+1\n",
    "                val_loss = 0.\n",
    "                val_acc = 0.\n",
    "#                 val_acc_wt = 0.\n",
    "                for rep in range(numrep):\n",
    "                    updateFeedDict(conf,'val',distort=False,sess=sess,data=data,feed_dict=feed_dict,ph=ph)\n",
    "                    vloss,vacc = sess.run([loss,accuracy],feed_dict=feed_dict)\n",
    "                    val_loss += vloss\n",
    "                    val_acc += vacc\n",
    "#                     val_acc_wt += vacc_wt\n",
    "                val_loss = val_loss/numrep\n",
    "                val_acc = val_acc/numrep\n",
    "#                 val_acc_wt /= numrep\n",
    "                test_summary,_ = sess.run([merged,loss],feed_dict=feed_dict)\n",
    "                test_writer.add_summary(test_summary,step)\n",
    "                print 'Val -- Acc:{:.4f} Loss:{:.4f} Train Acc:{:.4f} Loss:{:.4f} Iter:{}'.format(\n",
    "                    val_acc,val_loss,train_acc,train_loss,step)\n",
    "            if step % conf.save_step == 0:\n",
    "                saveEval(sess,evalSaver,step,conf)\n",
    "        print(\"Optimization Done!\")\n",
    "        saveEval(sess,evalSaver,step,conf)\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseEvalNetTiny(lin,locs,conf,trainPhase,dropout):\n",
    "\n",
    "    lin_sz = tf.Tensor.get_shape(lin).as_list()\n",
    "    lin_numel = reduce(operator.mul, lin_sz[1:], 1)\n",
    "    lin_re = tf.reshape(lin,[-1,lin_numel])\n",
    "    lin_re = tf.nn.dropout(lin_re,dropout)\n",
    "    with tf.variable_scope('lin_fc'):\n",
    "        weights = tf.get_variable(\"weights\", [lin_numel, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        lin_fc = tf.nn.relu(batch_norm_2D(tf.matmul(lin_re,weights)+biases,trainPhase))\n",
    "\n",
    "        \n",
    "    loc_sz = tf.Tensor.get_shape(locs).as_list()\n",
    "    loc_numel = reduce(operator.mul, loc_sz[1:], 1)\n",
    "    loc_re = tf.reshape(locs,[-1,loc_numel])\n",
    "    with tf.variable_scope('loc_fc'):\n",
    "        weights = tf.get_variable(\"weights\", [loc_numel, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        loc_fc = tf.nn.relu(batch_norm_2D(tf.matmul(loc_re,weights)+biases,trainPhase))\n",
    "        \n",
    "    joint_fc = tf.concat(1,[lin_fc,loc_fc])\n",
    "    \n",
    "    with tf.variable_scope('fc1'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt*2, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc1 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc,weights)+biases,trainPhase))\n",
    "\n",
    "    with tf.variable_scope('fc2'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc2 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc1,weights)+biases,trainPhase))\n",
    "        \n",
    "    with tf.variable_scope('out'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, 2],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.005))\n",
    "        biases = tf.get_variable(\"biases\", 2,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        out = tf.matmul(joint_fc2,weights)+biases\n",
    "        \n",
    "    layer_dict = {'lin_fc':lin_fc,\n",
    "                  'loc_fc':loc_fc,\n",
    "                  'joint_fc1':joint_fc1,\n",
    "                  'joint_fc2':joint_fc2,\n",
    "                  'out':out\n",
    "                 }\n",
    "    return out,layer_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createEvalPH(conf):\n",
    "    lin = tf.placeholder(tf.float32,[None,conf.n_classes,conf.nfcfilt])\n",
    "    locs = tf.placeholder(tf.float32,[None,conf.n_classes,2])\n",
    "    learning_rate_ph = tf.placeholder(tf.float32,shape=[])\n",
    "    y = tf.placeholder(tf.float32,[None,2])\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')                 \n",
    "    dropout = tf.placeholder(tf.float32, shape=[])                 \n",
    "    phDict = {'lin':lin,'locs':locs,'learning_rate':learning_rate_ph,\n",
    "              'y':y,'phase_train':phase_train,'dropout':dropout}\n",
    "    return phDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def createFeedDict(phDict):\n",
    "#     feed_dict = {phDict['lin']:[],\n",
    "#                  phDict['locs']:[],\n",
    "#                  phDict['y']:[],\n",
    "#                  phDict['learning_rate']:1.,\n",
    "#                  phDict['phase_train']:False,\n",
    "#                  phDict['dropout']:1.\n",
    "#                 }\n",
    "#     return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genLabels(rlocs,locs,conf):\n",
    "    d2locs = np.sqrt(((rlocs-locs[...,np.newaxis])**2).sum(-2))\n",
    "    ll = np.arange(1,conf.n_classes+1)\n",
    "    labels = np.tile(ll[:,np.newaxis],[d2locs.shape[0],1,d2locs.shape[2]])\n",
    "    labels[d2locs>conf.poseEvalNegDist] = -1.\n",
    "    labels[d2locs<conf.poseEvalNegDist] = 1.\n",
    "    labels = np.concatenate([labels[:,np.newaxis],1-labels[:,np.newaxis]],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genRandomNegSamples(bout,l7out,locs,conf,nsamples=10):\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "    rlocs[:,:,0,:] = np.random.randint(sz[1],size=locs.shape[0:2]+(nsamples,))\n",
    "    rlocs[:,:,1,:] = np.random.randint(sz[0],size=locs.shape[0:2]+(nsamples,))\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianPosSamples(bout,l7out,locs,conf,nsamples=10,maxlen = 4):\n",
    "    scale = conf.rescale*conf.pool_scale\n",
    "    sigma = float(maxlen)*0.5*scale\n",
    "    sz = (np.array(l7out.shape[1:3])-1)*scale\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(15*nsamples,))*sigma)\n",
    "    # remove rlocs that are far away.\n",
    "    dlocs = np.all( np.sqrt( (rlocs**2).sum(2))< (maxlen*scale),1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:])[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genGaussianNegSamples(bout,locs,conf,nsamples=10,minlen = 8):\n",
    "    sigma = minlen\n",
    "#     sz = (np.array(bout.shape[1:3])-1)*scale\n",
    "    sz = np.array(bout.shape[1:3])-1\n",
    "    bsize = conf.batch_size\n",
    "    rlocs = np.round(np.random.normal(size=locs.shape+(5*nsamples,))*sigma)\n",
    "    # remove rlocs that are small.\n",
    "    dlocs = np.sqrt( (rlocs**2).sum(2)).sum(1)\n",
    "    clocs = np.zeros(locs.shape+(nsamples,))\n",
    "    for ii in range(dlocs.shape[0]):\n",
    "        ndx = np.where(dlocs[ii,:]> (minlen*conf.n_classes) )[0][:nsamples]\n",
    "        clocs[ii,:,:,:] = rlocs[ii,:,:,ndx].transpose([1,2,0])\n",
    "\n",
    "    rlocs = locs[...,np.newaxis] + clocs\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genMovedNegSamples(bout,locs,conf,nsamples=10,minlen=8):\n",
    "    # Add same x and y to locs\n",
    "    \n",
    "    minlen = float(minlen)/2\n",
    "    maxlen = 2*minlen\n",
    "    rlocs = np.zeros(locs.shape + (nsamples,))\n",
    "#     sz = (np.array(bout.shape[1:3])-1)*conf.rescale*conf.pool_scale\n",
    "    sz = np.array(bout.shape[1:3])-1\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        rx = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "        ry = np.round(np.random.rand(nsamples)*(maxlen-minlen) + minlen)*\\\n",
    "            np.sign(np.random.rand(nsamples)-0.5)\n",
    "\n",
    "        rlocs[curi,:,0,:] = locs[curi,:,0,np.newaxis] + rx\n",
    "        rlocs[curi,:,1,:] = locs[curi,:,1,np.newaxis] + ry\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genNMovedNegSamples(bout,locs,N,conf,nsamples=10,minlen=8):\n",
    "    # Move one of the points.\n",
    "    minlen = float(minlen)\n",
    "    maxlen = 2*minlen\n",
    "    \n",
    "    rlocs = np.tile(locs[...,np.newaxis],[1,1,1,nsamples])\n",
    "    sz = np.array(bout.shape[1:3])-1\n",
    "\n",
    "    for curi in range(locs.shape[0]):\n",
    "        for curs in range(nsamples):\n",
    "            curN = np.random.randint(conf.n_classes)\n",
    "            for rand_point in np.random.choice(conf.n_classes,size=[curN,],replace=False):\n",
    "                rx = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                    np.sign(np.random.rand()-0.5)\n",
    "                ry = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                    np.sign(np.random.rand()-0.5)\n",
    "\n",
    "                rlocs[curi,rand_point,0,curs] = locs[curi,rand_point,0] + rx\n",
    "                rlocs[curi,rand_point,1,curs] = locs[curi,rand_point,1] + ry\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[:,:,0,:]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[:,:,0,:] = xlocs\n",
    "    ylocs = rlocs[:,:,1,:]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[:,:,1,:] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genNegSamples(bout,locs,conf,nsamples=10,minlen=8,N=1):\n",
    "    rlocs = np.concatenate([\n",
    "#                           genRandomNegSamples(bout,l7out,locs,conf,nsamples),\n",
    "#                           genGaussianNegSamples(bout,locs,conf,nsamples,minlen),\n",
    "#                           genMovedNegSamples(bout,locs,conf,nsamples,minlen), \n",
    "                          genNMovedNegSamples(bout,locs,N,conf,nsamples,minlen)], \n",
    "                          axis=3)\n",
    "#     rlabels = genLabels(rlocs,locs,conf)\n",
    "    return rlocs#,rlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genData(l7out,inlocs,conf):\n",
    "    locs = np.round(inlocs/(conf.rescale*conf.pool_scale))\n",
    "    dd = np.zeros(locs.shape[0:2]+l7out.shape[-1:]+locs.shape[-1:])\n",
    "    for curi in range(locs.shape[0]):\n",
    "        for pp in range(locs.shape[1]):\n",
    "            for s in range(locs.shape[3]):\n",
    "                dd[curi,pp,:,s] = l7out[curi,int(locs[curi,pp,1,s]),int(locs[curi,pp,0,s]),:]\n",
    "    return dd        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareOpt(baseNet,dbtype,feed_dict,sess,conf,phDict,distort):\n",
    "    nsamples = 10\n",
    "    npos = 2\n",
    "    baseNet.updateFeedDict(dbtype,distort)\n",
    "    locs = baseNet.locs\n",
    "    l7 = baseNet.baseLayers['conv7']\n",
    "    [bout,l7out] = sess.run([baseNet.basePred,l7],feed_dict=baseNet.feed_dict)\n",
    "    neglocs = genNegSamples(bout,l7out,locs,conf,nsamples=nsamples)\n",
    "#     replocs = np.tile(locs[...,np.newaxis],npos*nsamples)\n",
    "    replocs = genGaussianPosSamples(bout,l7out,locs,conf,nsamples*npos,maxlen=4)\n",
    "    alllocs = np.concatenate([neglocs,replocs],axis=-1)\n",
    "    alllocs = alllocs.transpose([0,3,1,2])\n",
    "    alllocs = alllocs.reshape((-1,)+alllocs.shape[2:])\n",
    "    retlocs = copy.deepcopy(alllocs)\n",
    "    alllocs_m = alllocs.mean(1)\n",
    "    alllocs = alllocs-alllocs_m[:,np.newaxis,:]\n",
    "    \n",
    "#    poslabels = np.ones(replocs.shape[0,1,3])\n",
    "#    alllabels = np.concatenate([neglabels,poslabels],axis=-2)\n",
    "#    alllabels = alllabels.transpose([0,2,1,3])\n",
    "#    alllabels = alllabels.reshape((-1,)+alllabels.shape[-1])\n",
    "\n",
    "    negdd = genData(l7out,neglocs,conf)\n",
    "    posdd = genData(l7out,replocs,conf)\n",
    "    alldd = np.concatenate([negdd,posdd],axis=-1)\n",
    "    alldd = alldd.transpose([0,3,1,2])\n",
    "    alldd = np.reshape(alldd,[-1,alldd.shape[-2],alldd.shape[-1]])\n",
    "\n",
    "#     y = alllabels\n",
    "    y = np.zeros([l7out.shape[0],neglocs.shape[-1]+replocs.shape[-1],2])\n",
    "    y[:,:-nsamples*npos,0] = 1. \n",
    "    y[:,-nsamples*npos:,1] = 1.\n",
    "    y = np.reshape(y,[-1,y.shape[-1]])\n",
    "        \n",
    "#     excount = step*conf.batch_size\n",
    "#     cur_lr = learning_rate * \\\n",
    "#             conf.gamma**math.floor(excount/conf.step_size)\n",
    "#     feed_dict[phDict['learning_rate']] = cur_lr\n",
    "    feed_dict[phDict['y']] = y\n",
    "    feed_dict[phDict['lin']] = alldd\n",
    "    feed_dict[phDict['locs']] = alllocs\n",
    "    return retlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(conf,restore=True):\n",
    "    \n",
    "    phDict = createEvalPH(conf)\n",
    "    feed_dict = createFeedDict(phDict)\n",
    "    feed_dict[phDict['phase_train']] = True\n",
    "    feed_dict[phDict['dropout']] = 0.5\n",
    "    with tf.variable_scope('poseEval'):\n",
    "        out,layer_dict = poseEvalNet(phDict['lin'],phDict['locs'],\n",
    "                                     conf,phDict['phase_train'],\n",
    "                                     phDict['dropout'])\n",
    "        \n",
    "    evalSaver = createEvalSaver(conf)\n",
    "    y = phDict['y']\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(out,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    baseNet = PoseTools.createNetwork(conf,1)\n",
    "    baseNet.openDBs()\n",
    "    \n",
    "    with baseNet.env.begin() as txn,baseNet.valenv.begin() as valtxn,tf.Session() as sess:\n",
    "\n",
    "        baseNet.createCursors()\n",
    "        baseNet.restoreBase(sess,True)\n",
    "        didRestore,startat = restoreEval(sess,conf,evalSaver,restore)\n",
    "        baseNet.initializeRemainingVars(sess)\n",
    "        for step in range(startat,conf.eval_training_iters+1):\n",
    "            prepareOpt(baseNet,baseNet.DBType.Train,feed_dict,sess,conf,\n",
    "                       phDict,distort=True)\n",
    "#             baseNet.feed_dict[baseNet.ph['keep_prob']] = 0.5\n",
    "            feed_dict[phDict['phase_train']] = True\n",
    "            feed_dict[phDict['dropout']] = 0.5\n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "            if step % 25 == 0:\n",
    "                prepareOpt(baseNet,baseNet.DBType.Train,feed_dict,\n",
    "                           sess,conf,phDict,distort=False)\n",
    "#                 baseNet.feed_dict[baseNet.ph['keep_prob']] = 1\n",
    "                feed_dict[phDict['phase_train']] = False\n",
    "                feed_dict[phDict['dropout']] = 1\n",
    "                train_cross_ent = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                test_cross_ent = 0\n",
    "                test_acc = 0\n",
    "                pos_acc = 0\n",
    "                pred_acc_pos = 0\n",
    "                pred_acc_pred = 0\n",
    "                \n",
    "                #generate blocks for different neg types\n",
    "                nsamples = 10\n",
    "                ntypes = 3\n",
    "                npos = 2\n",
    "                blk = np.arange(nsamples)\n",
    "                inter = np.arange(0,conf.batch_size*nsamples*(ntypes+npos),nsamples*(ntypes+npos))\n",
    "                n_inters = blk + inter[:,np.newaxis]\n",
    "                n_inters = n_inters.flatten()\n",
    "                nacc = np.zeros(ntypes)\n",
    "                nclose = 0 \n",
    "                in_locs = np.array([])\n",
    "                nrep = 40\n",
    "                for rep in range(nrep):\n",
    "                    prepareOpt(baseNet,baseNet.DBType.Val,feed_dict,sess,conf,\n",
    "                               phDict,distort=False)\n",
    "                    test_cross_ent += sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                    test_acc += sess.run(accuracy, feed_dict = feed_dict)\n",
    "                    tout = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "                    labels = feed_dict[phDict['y']]\n",
    "                    pos_acc += float(np.count_nonzero(tout[labels[:,1]>0.5]))/nsamples\n",
    "                    for nt in range(ntypes):\n",
    "                        nacc[nt] += float(np.count_nonzero(tout[n_inters+nt*nsamples]))/nsamples\n",
    "                    \n",
    "                    tdd = feed_dict[phDict['lin']]\n",
    "                    tlocs = feed_dict[phDict['locs']]\n",
    "                    ty = feed_dict[phDict['y']]\n",
    "                    # \n",
    "                    l7 = baseNet.baseLayers['conv7']\n",
    "                    curpred = sess.run([baseNet.basePred,l7], feed_dict=baseNet.feed_dict)\n",
    "                    baseLocs = PoseTools.getBasePredLocs(curpred[0],conf)\n",
    "\n",
    "                    neglocs = baseLocs[:,:,:,np.newaxis]\n",
    "                    locs = np.array(baseNet.locs)[...,np.newaxis]\n",
    "                    d2locs = np.sqrt( np.sum((neglocs-locs)**2,axis=(1,2,3)))\n",
    "                    alllocs = np.concatenate([neglocs,locs],axis=3)\n",
    "                    alldd = genData(curpred[1],alllocs,conf)\n",
    "                    alllocs = alllocs.transpose([0,3,1,2])\n",
    "                    alllocs = alllocs.reshape((-1,)+alllocs.shape[2:])\n",
    "                    alllocs_m = alllocs.mean(1)\n",
    "                    alllocs = alllocs-alllocs_m[:,np.newaxis,:]\n",
    "\n",
    "                    alldd = alldd.transpose([0,3,1,2])\n",
    "                    alldd = np.reshape(alldd,[-1,alldd.shape[-2],alldd.shape[-1]])\n",
    "\n",
    "                    y = np.zeros([curpred[0].shape[0],alllocs.shape[-1],2])\n",
    "                    y[d2locs>=25,:-1,0] = 1. \n",
    "                    y[d2locs<25,:-1,1] = 1. \n",
    "                    y[:,-1,1] = 1.\n",
    "                    y = np.reshape(y,[-1,y.shape[-1]])\n",
    "\n",
    "                    feed_dict[phDict['y']] = y\n",
    "                    feed_dict[phDict['lin']] = alldd\n",
    "                    feed_dict[phDict['locs']] = alllocs\n",
    "\n",
    "                    corrpred = sess.run(correct_prediction,feed_dict=feed_dict)\n",
    "                    pred_acc_pos += np.count_nonzero(corrpred[1::2])\n",
    "                    pred_acc_pred += np.count_nonzero(corrpred[0::2])\n",
    "                    nclose += np.count_nonzero(d2locs<25)\n",
    "                    er_locs = ~corrpred[0::2]\n",
    "                    in_locs = np.append(in_locs,d2locs[er_locs])\n",
    "                print \"Iter:{:d}, train:{:.4f} test:{:.4f} acc:{:.2f} posacc:{:.2f}\".format(step,\n",
    "                                                 train_cross_ent,test_cross_ent/nrep,\n",
    "                                                 test_acc/nrep,pos_acc/nrep/conf.batch_size/npos)\n",
    "                print \"Neg:{}\".format(nacc/nrep/conf.batch_size)\n",
    "                print \"Pred Acc Pos:{},Pred Acc Pred:{},numclose:{}\".format(float(pred_acc_pos)/nrep/conf.batch_size,\n",
    "                                                                            float(pred_acc_pred)/nrep/conf.batch_size,\n",
    "                                                                            float(nclose)/nrep/conf.batch_size)\n",
    "                print 'Distance of incorrect predictions:{}'.format(in_locs)\n",
    "                \n",
    "            if step % 100 == 0:\n",
    "                saveEval(sess,step,evalSaver,conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
