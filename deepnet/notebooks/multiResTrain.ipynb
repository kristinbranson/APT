{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Mayank Feb 3 2016\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import tempfile,copy,re\n",
    "\n",
    "sys.path.append('/home/mayank/work/caffe/python')\n",
    "sys.path.append('/home/mayank/work/pyutils')\n",
    "\n",
    "import caffe,lmdb\n",
    "import caffe.proto.caffe_pb2\n",
    "from caffe.io import datum_to_array\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as manimation\n",
    "import math,cv2,scipy,time,pickle\n",
    "from cvc import cvc\n",
    "import numpy as np\n",
    "\n",
    "import multiPawTools,myutils,multiResData\n",
    "\n",
    "from convNetBase import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainBase(conf,resume=True):\n",
    "    # Parameters\n",
    "    learning_rate = conf.base_learning_rate;  training_iters = conf.base_training_iters\n",
    "    batch_size = conf.batch_size;        display_step = conf.display_step\n",
    "\n",
    "    # Network Parameters\n",
    "    n_input = conf.psz; n_classes = conf.n_classes; dropout = conf.dropout \n",
    "    imsz = conf.imsz;   rescale = conf.rescale;     scale = conf.scale\n",
    "    pool_scale = conf.pool_scale\n",
    "    \n",
    "    x0,x1,x2,y,keep_prob = createPlaceHolders(imsz,\n",
    "                              rescale,scale,pool_scale,n_classes)\n",
    "    learning_rate_ph = tf.placeholder(tf.float32,shape=[])\n",
    "    \n",
    "    weights = initNetConvWeights(conf)\n",
    "    # Construct model\n",
    "    pred,layers = net_multi_conv(x0,x1,x2, weights, keep_prob,\n",
    "                          imsz,rescale,pool_scale)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.l2_loss(pred- y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_ph).minimize(cost)\n",
    "\n",
    "    # training data stuff\n",
    "    lmdbfilename =os.path.join(conf.cachedir,conf.trainfilename)\n",
    "    vallmdbfilename =os.path.join(conf.cachedir,conf.valfilename)\n",
    "    env = lmdb.open(lmdbfilename, readonly = True)\n",
    "    valenv = lmdb.open(vallmdbfilename, readonly = True)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    saver = tf.train.Saver(max_to_keep=conf.maxckpt)\n",
    "\n",
    "    trainData = {'train_err':[],'val_err':[],'step_no':[]}\n",
    "    with env.begin() as txn,valenv.begin() as valtxn:\n",
    "        train_cursor = txn.cursor(); val_cursor = valtxn.cursor()\n",
    "        nTrain = txn.stat()['entries']\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            outfilename = os.path.join(conf.cachedir,conf.outname)\n",
    "            traindatafilename = os.path.join(conf.cachedir,conf.databasename)\n",
    "            latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                                latest_filename = conf.ckptbasename)\n",
    "            if not latest_ckpt or not resume:\n",
    "                sess.run(init)\n",
    "                startat = 0\n",
    "            else:\n",
    "                saver.restore(latest_ckpt.model_checkpoint_path)\n",
    "                matchObj = re.match(outfilename + '-(\\d*)',ckpt.model_checkpoint_path)\n",
    "                startat = int(matchObj.group(1)+1)\n",
    "                tdfile = open(traindatafilename,'rb')\n",
    "                trainData = pickle.load(tdfile)\n",
    "                tdfile.close()\n",
    "                \n",
    "            read_time = 0.; proc_time = 0.; opt_time = 0.\n",
    "            # Keep training until reach max iterations\n",
    "            for step in range(startat,training_iters):\n",
    "                excount = step*batch_size\n",
    "                cur_lr = learning_rate * \\\n",
    "                        conf.gamma**math.floor(excount/conf.step_size)\n",
    "                \n",
    "                r_start = time.clock()\n",
    "                batch_xs, locs = multiPawTools.readLMDB(train_cursor,\n",
    "                                        batch_size,imsz,multiResData)\n",
    "                r_end = time.clock()\n",
    "                \n",
    "                locs = multiResData.sanitizelocs(locs)\n",
    "                \n",
    "                x0_in,x1_in,x2_in = multiPawTools.multiScaleImages(\n",
    "                    batch_xs.transpose([0,2,3,1]),rescale,scale)\n",
    "                \n",
    "                labelims = multiPawTools.createLabelImages(locs,\n",
    "                                   conf.imsz,conf.pool_scale*conf.rescale,\n",
    "                                   conf.label_blur_rad) \n",
    "                p_end = time.clock()\n",
    "                sess.run(optimizer, \n",
    "                         feed_dict={x0: x0_in, x1: x1_in,\n",
    "                                    x2: x2_in, y: labelims, \n",
    "                                    keep_prob: dropout, learning_rate_ph:cur_lr})\n",
    "                o_end = time.clock()\n",
    "                \n",
    "                read_time += r_end-r_start\n",
    "                proc_time += p_end-r_end\n",
    "                opt_time += o_end-p_end\n",
    "\n",
    "                if step % display_step == 0:\n",
    "                    train_loss = sess.run(cost, feed_dict={x0:x0_in,\n",
    "                                                     x1:x1_in,\n",
    "                                                     x2:x2_in,\n",
    "                                               y: labelims, keep_prob: 1.})\n",
    "                    train_loss /= batch_size\n",
    "                    numrep = int(conf.numTest/conf.batch_size)+1\n",
    "                    acc = 0; loss = 0\n",
    "                    for rep in range(numrep):\n",
    "                        val_xs, locs = multiPawTools.readLMDB(val_cursor,\n",
    "                                          batch_size,imsz,multiResData)\n",
    "                        x0_in,x1_in,x2_in = multiPawTools.multiScaleImages(\n",
    "                            val_xs.transpose([0,2,3,1]),rescale,scale)\n",
    "\n",
    "                        labelims = multiPawTools.createLabelImages(locs,\n",
    "                                                   conf.imsz,conf.pool_scale*conf.rescale,\n",
    "                                                   conf.label_blur_rad)\n",
    "                        loss += sess.run(cost, feed_dict={x0:x0_in,\n",
    "                                                         x1:x1_in,\n",
    "                                                         x2:x2_in,\n",
    "                                                   y: labelims, keep_prob: 1.})\n",
    "                    loss = (loss/numrep)/batch_size\n",
    "                    print \"Iter \" + str(step)\n",
    "                    print \"  Training Loss= \" + \"{:.6f}\".format(train_loss) + \\\n",
    "                         \", Minibatch Loss= \" + \"{:.6f}\".format(loss) \n",
    "                    print \"  Read Time:\" + \"{:.2f}, \".format(read_time/(step+1)) + \\\n",
    "                          \"Proc Time:\" + \"{:.2f}, \".format(proc_time/(step+1)) + \\\n",
    "                          \"Opt Time:\" + \"{:.2f}\".format(opt_time/(step+1)) \n",
    "                    trainData['train_err'].append(train_loss)        \n",
    "                    trainData['val_err'].append(loss)        \n",
    "                    trainData['step_no'].append(step)        \n",
    "                    \n",
    "                if step % conf.save_step == 0:\n",
    "                    saver.save(sess,outfilename,global_step=step,\n",
    "                               latest_filename = conf.ckptbasename)\n",
    "                    print('Saved state to %s-%d' %(outfilename,step))\n",
    "                    tdfile = open(traindatafilename,'wb')\n",
    "                    pickle.dump(trainData,tdfile)\n",
    "                    tdfile.close()\n",
    "                    \n",
    "                step += 1\n",
    "            print \"Optimization Finished!\"\n",
    "            saver.save(sess,outfilename,global_step=step,\n",
    "                       latest_filename = conf.ckptbasename)\n",
    "            print('Saved state to %s-%d' %(outfilename,step))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainFine(conf,jointTrain=False,resume=True):\n",
    "    # Parameters\n",
    "    learning_rate = conf.fine_learning_rate;  \n",
    "    batch_size = conf.fine_batch_size;        display_step = conf.display_step\n",
    "    n_input = conf.psz; n_classes = conf.n_classes; dropout = conf.dropout \n",
    "    imsz = conf.imsz;   rescale = conf.rescale;     scale = conf.scale\n",
    "    pool_scale = conf.pool_scale\n",
    "    \n",
    "    x0,x1,x2,y,keep_prob = createPlaceHolders(imsz,rescale,scale,pool_scale,n_classes)\n",
    "    locs_ph = tf.placeholder(tf.float32,[conf.batch_size,n_classes,2])\n",
    "    learning_rate_ph = tf.placeholder(tf.float32,shape=[])\n",
    "\n",
    "    weights = initNetConvWeights(conf)\n",
    "    pred_gradient,layers = net_multi_conv(x0,x1,x2, weights, keep_prob,\n",
    "                          imsz,rescale,pool_scale)\n",
    "    \n",
    "    baseoutname = '%s_%d.ckpt'%(conf.outname,conf.base_training_iters)\n",
    "    basemodelfile = os.path.join(conf.cachedir,baseoutname)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    pred = tf.stop_gradient(pred_gradient)\n",
    "    training_iters = conf.fine_training_iters\n",
    "    outname = conf.fineoutname\n",
    "    print(\"Restoring base model from:\" + basemodelfile)\n",
    "    saver.restore(sess, basemodelfile)\n",
    "        \n",
    "    # Construct fine model\n",
    "    labelT  = multiPawTools.createFineLabelTensor(conf)\n",
    "    layer1_1 = tf.stop_gradient(layers['base_dict_0']['conv1'])\n",
    "    layer1_2 = tf.stop_gradient(layers['base_dict_0']['conv2'])\n",
    "    layer2_1 = tf.stop_gradient(layers['base_dict_1']['conv1'])\n",
    "    layer2_2 = tf.stop_gradient(layers['base_dict_1']['conv2'])\n",
    "    curfine1_1 = extractPatches(layer1_1,pred,conf,1,4)\n",
    "    curfine1_2 = extractPatches(layer1_2,pred,conf,2,2)\n",
    "    curfine2_1 = extractPatches(layer2_1,pred,conf,2,2)\n",
    "    curfine2_2 = extractPatches(layer2_2,pred,conf,4,1)\n",
    "    curfine1_1u = tf.unpack(tf.transpose(curfine1_1,[1,0,2,3,4]))\n",
    "    curfine1_2u = tf.unpack(tf.transpose(curfine1_2,[1,0,2,3,4]))\n",
    "    curfine2_1u = tf.unpack(tf.transpose(curfine2_1,[1,0,2,3,4]))\n",
    "    curfine2_2u = tf.unpack(tf.transpose(curfine2_2,[1,0,2,3,4]))\n",
    "    finepred = fineOut(curfine1_1u,curfine1_2u,curfine2_1u,curfine2_2u,conf)    \n",
    "    limgs = multiPawTools.createFineLabelImages(locs_ph,pred,conf,labelT)\n",
    "\n",
    "    # training data stuff\n",
    "    lmdbfilename =os.path.join(conf.cachedir,conf.trainfilename)\n",
    "    vallmdbfilename =os.path.join(conf.cachedir,conf.valfilename)\n",
    "    env = lmdb.open(lmdbfilename, readonly = True)\n",
    "    valenv = lmdb.open(vallmdbfilename, readonly = True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    costFine = tf.reduce_mean(tf.nn.l2_loss(finepred- tf.to_float(limgs)))\n",
    "    costBase =  tf.reduce_mean(tf.nn.l2_loss(pred- y))\n",
    "\n",
    "    cost = costFine\n",
    "\n",
    "    saver1 = tf.train.Saver(max_to_keep=conf.maxckpt)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_ph).minimize(cost)\n",
    "\n",
    "    outfilename = os.path.join(conf.cachedir,conf.fineoutname)\n",
    "    traindatafilename = os.path.join(conf.cachedir,conf.datafinename)\n",
    "    latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                        latest_filename = conf.ckptfinename)\n",
    "    \n",
    "    if not latest_ckpt or not resume:\n",
    "        startat = 0\n",
    "        trainData = {'train_err':[],'val_err':[],'step_no':[]}\n",
    "        varlist = tf.all_variables()\n",
    "        for var in varlist:\n",
    "            try:\n",
    "                sess.run(tf.assert_variables_initialized([var]))\n",
    "            except tf.errors.FailedPreconditionError:\n",
    "                sess.run(tf.initialize_variables([var]))\n",
    "\n",
    "    else:\n",
    "        saver1.restore(latest_ckpt.model_checkpoint_path)\n",
    "        matchObj = re.match(outfilename + '-(\\d*)',ckpt.model_checkpoint_path)\n",
    "        startat = int(matchObj.group(1)+1)\n",
    "        tdfile = open(traindatafilename,'rb')\n",
    "        trainData = pickle.load(tdfile)\n",
    "        tdfile.close()\n",
    "\n",
    "\n",
    "#             print('Initializing variable %s'%var.name)\n",
    "            \n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "\n",
    "    with env.begin() as txn,valenv.begin() as valtxn:\n",
    "        train_cursor = txn.cursor(); val_cursor = valtxn.cursor()\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for step in range(startat,training_iters):\n",
    "            excount = step*batch_size\n",
    "            cur_lr = learning_rate * \\\n",
    "                    conf.gamma**math.floor(excount/conf.step_size)\n",
    "\n",
    "            batch_xs, locs = multiPawTools.readLMDB(train_cursor,\n",
    "                                    batch_size,imsz,multiResData)\n",
    "\n",
    "            locs = multiResData.sanitizelocs(locs)\n",
    "\n",
    "            x0_in,x1_in,x2_in = multiPawTools.iScaleImages(\n",
    "                batch_xs.transpose([0,2,3,1]),rescale,scale)\n",
    "\n",
    "            labelims = multiPawTools.createLabelImages(locs,\n",
    "                               conf.imsz,conf.pool_scale*conf.rescale,\n",
    "                               conf.label_blur_rad) \n",
    "            feed_dict={x0: x0_in,x1: x1_in,x2: x2_in,\n",
    "                y: labelims, keep_prob: dropout,locs_ph:np.array(locs),\n",
    "                learning_rate_ph:cur_lr}\n",
    "            sess.run(optimizer, feed_dict = feed_dict)\n",
    "\n",
    "            if step % display_step == 0:\n",
    "                feed_dict={x0: x0_in,x1: x1_in,x2: x2_in,\n",
    "                    y: labelims, keep_prob: 1.,locs_ph:np.array(locs)}\n",
    "                train_loss = sess.run([cost,costBase], feed_dict=feed_dict)\n",
    "\n",
    "                numrep = int(conf.numTest/conf.batch_size)+1\n",
    "                acc = 0; loss = 0\n",
    "                for rep in range(numrep):\n",
    "                    val_xs, locs = multiPawTools.readLMDB(val_cursor,\n",
    "                                      batch_size,imsz,multiResData)\n",
    "                    x0_in,x1_in,x2_in = multiPawTools.multiScaleImages(\n",
    "                        val_xs.transpose([0,2,3,1]),rescale,scale)\n",
    "\n",
    "                    labelims = multiPawTools.createLabelImages(locs,\n",
    "                        conf.imsz,conf.pool_scale*conf.rescale,\n",
    "                        conf.label_blur_rad)\n",
    "                    feed_dict={x0: x0_in,x1: x1_in,x2: x2_in,\n",
    "                        y: labelims, keep_prob:1.,locs_ph:np.array(locs)}\n",
    "                    loss += sess.run(cost, feed_dict=feed_dict)\n",
    "                loss = (loss/numrep)/batch_size\n",
    "                print \"Iter \" + str(step) + \\\n",
    "                \"  Minibatch Loss= \" + \"{:.3f}\".format(loss) + \\\n",
    "                 \", Training Loss= \" + \"{:.3f}\".format(train_loss[0]/batch_size) + \\\n",
    "                 \", Base Training Loss= \" + \"{:.3f}\".format(train_loss[1]/batch_size)\n",
    "                trainData['train_err'].append(train_loss[0]/batch_size)\n",
    "                trainData['val_err'].append(loss)\n",
    "                trainData['step_no'].append(step)\n",
    "\n",
    "            if step % conf.save_step == 0:\n",
    "                saver1.save(sess,outfilename,global_step=step,\n",
    "                           latest_filename = conf.ckptfinename)\n",
    "                print('Saved state to %s-%d' %(outfilename,step))\n",
    "                tdfile = open(traindatafilename,'wb')\n",
    "                pickle.dump(trainData,tdfile)\n",
    "                tdfile.close()\n",
    "#             if step % conf.save_step == 0:\n",
    "#                 curoutname = '%s_%d.ckpt'% (outname,step)\n",
    "#                 outfilename = os.path.join(conf.cachedir,curoutname)\n",
    "#                 saver1.save(sess,outfilename)\n",
    "#                 print('Saved state to %s' %(outfilename))\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "        print \"Optimization Finished!\"\n",
    "        saver1.save(sess,outfilename,global_step=step,\n",
    "                   latest_filename = conf.ckptfinename)\n",
    "        print('Saved state to %s-%d' %(outfilename,step))\n",
    "        tdfile = open(traindatafilename,'wb')\n",
    "        pickle.dump(trainData,tdfile)\n",
    "        tdfile.close()\n",
    "    \n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractPatches(layer,out,conf,scale,outscale):\n",
    "    hsz = conf.fine_sz/scale/2\n",
    "    padsz = tf.constant([[0,0],[hsz, hsz],[hsz,hsz],[0,0]])\n",
    "    patchsz = tf.to_int32([conf.fine_sz/scale,conf.fine_sz/scale,-1])\n",
    "\n",
    "    patches = []\n",
    "    maxloc = multiPawTools.argmax2d(out)*outscale\n",
    "    padlayer = tf.pad(layer,padsz)\n",
    "    for inum in range(conf.batch_size):\n",
    "        curpatches = []\n",
    "        for ndx in range(conf.n_classes):\n",
    "            curloc = tf.concat(0,[tf.squeeze(maxloc[:,inum,ndx]),[0]])\n",
    "            curpatches.append(tf.slice(padlayer[inum,:,:,:],curloc,patchsz))\n",
    "        patches.append(tf.pack(curpatches))\n",
    "    return tf.pack(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initPredSession():\n",
    "    x0,x1,x2,y,keep_prob = createPlaceHolders()\n",
    "    weights = initNetConvWeights()\n",
    "    pred = paw_net_multi_conv(x0,x1,x2, weights, keep_prob)\n",
    "    saver = tf.train.Saver()\n",
    "    pholders = (x0,x1,x2,y,keep_prob)\n",
    "    return pred, saver,pholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(img,sess,pred,pholders):\n",
    "    x0_in,x1_in,x2_in = multiScaleImages(img[np.newaxis,:,:,:])\n",
    "    imsz = conf.imsz\n",
    "    lsz0 = int(math.ceil(float(imsz[0])/conf.pool_scale/conf.rescale))\n",
    "    lsz1 = int(math.ceil(float(imsz[1])/conf.pool_scale/conf.rescale))\n",
    "\n",
    "    labelim = np.zeros([1,lsz0,lsz1,1])\n",
    "\n",
    "    out = sess.run(pred,feed_dict={pholders[0]:x0_in,\n",
    "                     pholders[1]:x1_in,\n",
    "                     pholders[2]:x2_in,\n",
    "                     pholders[3]:labelim,\n",
    "                     pholders[4]: 1.})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictMovie(model_file,inmovie,outmovie):\n",
    "    pred,saver,pholders = initPredSession()\n",
    "    tdir = tempfile.mkdtemp()\n",
    "\n",
    "    cap = cv2.VideoCapture(inmovie)\n",
    "    nframes = int(cap.get(cvc.FRAME_COUNT))\n",
    "    plt.gray()\n",
    "    # with writer.saving(fig,\"test_results.mp4\",4):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_file)\n",
    "        \n",
    "        count = 0\n",
    "        for fnum in range(nframes):\n",
    "            plt.clf()\n",
    "            plt.axis('off')\n",
    "            framein = myutils.readframe(cap,fnum)\n",
    "            framein = framein[:,0:(framein.shape[1]/2),0:1]\n",
    "            out = predict(copy.copy(framein),sess,pred,pholders)\n",
    "            plt.imshow(framein[:,:,0])\n",
    "            maxndx = np.argmax(out[0,:,:,0])\n",
    "            loc = np.unravel_index(maxndx,out.shape[1:3])\n",
    "            scalefactor = conf.rescale*conf.pool_scale\n",
    "            plt.scatter(loc[1]*scalefactor,loc[0]*scalefactor,hold=True)\n",
    "\n",
    "            fname = \"test_{:06d}.png\".format(count)\n",
    "            plt.savefig(os.path.join(tdir,fname))\n",
    "            count+=1\n",
    "\n",
    "#     ffmpeg_cmd = \"ffmpeg -r 30 \" + \\\n",
    "#     \"-f image2 -i '/path/to/your/picName%d.png' -qscale 0 '/path/to/your/new/video.avi'\n",
    "\n",
    "    tfilestr = os.path.join(tdir,'test_*.png')\n",
    "    mencoder_cmd = \"mencoder mf://\" + tfilestr + \\\n",
    "    \" -frames \" + \"{:d}\".format(count) + \" -mf type=png:fps=15 -o \" + \\\n",
    "    outmovie + \" -ovc lavc -lavcopts vcodec=mpeg4:vbitrate=2000000\"\n",
    "#     print(mencoder_cmd)\n",
    "    os.system(mencoder_cmd)\n",
    "    cap.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
