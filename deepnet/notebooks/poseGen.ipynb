{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n",
      "/home/mayank/work/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.\n",
      "  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \\\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import lmdb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import cv2\n",
    "import tempfile\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from batch_norm import batch_norm_2D\n",
    "import myutils\n",
    "import PoseTools\n",
    "import localSetup\n",
    "import operator\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addDropoutLayer(ptrainObj,dropout,conf):\n",
    "    l7 = ptrainObj.baseLayers['conv7']\n",
    "    with tf.variable_scope('base/layer8') as scope:\n",
    "        scope.reuse_variables()\n",
    "        l7_do = tf.nn.dropout(l7,dropout,[conf.batch_size,1,1,conf.nfcfilt])\n",
    "        l8_weights = tf.get_variable(\"weights\", [1,1,conf.nfcfilt,conf.n_classes],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        l8_biases = tf.get_variable(\"biases\", conf.n_classes,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        l8 = tf.nn.conv2d(l7_do,l8_weights,strides=[1,1,1,1],padding='SAME')+l8_biases\n",
    "    return l8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poseGenNet(locs,scores,l8,conf,ptrainObj,trainPhase):\n",
    "\n",
    "    \n",
    "    scores_sz = tf.Tensor.get_shape(scores).as_list()\n",
    "    scores_numel = reduce(operator.mul, scores_sz[1:], 1)\n",
    "    scores_re = tf.reshape(scores,[-1,scores_numel])\n",
    "#     with tf.variable_scope('scores_fc'):\n",
    "#         weights = tf.get_variable(\"weights\", [scores_numel, conf.nfcfilt],\n",
    "#             initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "#         biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "#             initializer=tf.constant_initializer(0))\n",
    "        \n",
    "#         scores_fc = tf.nn.relu(batch_norm_2D(tf.matmul(scores_re,weights)+biases,trainPhase))\n",
    "\n",
    "        \n",
    "    loc_sz = tf.Tensor.get_shape(locs).as_list()\n",
    "    loc_numel = reduce(operator.mul, loc_sz[1:], 1)\n",
    "    loc_re = tf.reshape(locs,[-1,loc_numel])\n",
    "    joint = tf.concat(0,[scores_re,loc_re])\n",
    "    with tf.variable_scope('loc_fc'):\n",
    "        weights = tf.get_variable(\"weights\", [loc_numel, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc = tf.nn.relu(batch_norm_2D(tf.matmul(joint,weights)+biases,trainPhase))\n",
    "        \n",
    "#     joint_fc = tf.concat(1,[scores_fc,loc_fc])\n",
    "    \n",
    "    with tf.variable_scope('fc1'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt*2, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc1 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc,weights)+biases,trainPhase))\n",
    "\n",
    "    with tf.variable_scope('fc2'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, conf.nfcfilt],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "        biases = tf.get_variable(\"biases\", conf.nfcfilt,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        joint_fc2 = tf.nn.relu(batch_norm_2D(tf.matmul(joint_fc1,weights)+biases,trainPhase))\n",
    "        \n",
    "    with tf.variable_scope('out'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, conf.n_classes*2],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", conf.n_classes*2,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        out = tf.matmul(joint_fc2,weights)+biases\n",
    "        \n",
    "    with tf.variable_scope('out_m'):\n",
    "        weights = tf.get_variable(\"weights\", [conf.nfcfilt, 2],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", 2,\n",
    "            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        out_m = tf.matmul(joint_fc2,weights)+biases\n",
    "        \n",
    "    layer_dict = {'scores_fc':scores_fc,\n",
    "                  'loc_fc':loc_fc,\n",
    "                  'joint_fc1':joint_fc1,\n",
    "                  'joint_fc2':joint_fc2,\n",
    "                  'out':out,\n",
    "                  'out_m':out_m\n",
    "                 }\n",
    "    return out, out_m, layer_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createGenPH(conf):\n",
    "    scores = tf.placeholder(tf.float32,[None,conf.n_classes],name='scores')\n",
    "    locs = tf.placeholder(tf.float32,[None,conf.n_classes,2],name='locs')\n",
    "    learning_rate_ph = tf.placeholder(tf.float32,shape=[],name='learning_rate_gen')\n",
    "    y = tf.placeholder(tf.float32,[None,conf.n_classes*2],name='y')\n",
    "    y_m = tf.placeholder(tf.float32,[None,2],name='y_m')\n",
    "    phase_train = tf.placeholder(tf.bool, name='phase_train')                 \n",
    "    dropout = tf.placeholder(tf.float32, shape=[],name='gen_dropout')        \n",
    "    phDict = {'scores':scores,'locs':locs,'learning_rate':learning_rate_ph,\n",
    "              'y':y,'y_m':y_m,'phase_train':phase_train,'dropout':dropout}\n",
    "    return phDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFeedDict(phDict):\n",
    "    feed_dict = {phDict['scores']:[],\n",
    "                 phDict['locs']:[],\n",
    "                 phDict['y']:[],\n",
    "                 phDict['y_m']:[],\n",
    "                 phDict['learning_rate']:1.,\n",
    "                 phDict['phase_train']:False,\n",
    "                 phDict['dropout']:1.\n",
    "                }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createGenSaver(conf):\n",
    "    genSaver = tf.train.Saver(var_list = PoseTools.getvars('poseGen'),max_to_keep=conf.maxckpt)\n",
    "    return genSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restoreGen(sess,conf,genSaver,restore=True):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.genoutname)\n",
    "    latest_ckpt = tf.train.get_checkpoint_state(conf.cachedir,\n",
    "                                        latest_filename = conf.genckptname)\n",
    "    if not latest_ckpt or not restore:\n",
    "        startat = 0\n",
    "        sess.run(tf.initialize_variables(PoseTools.getvars('poseGen')))\n",
    "        print(\"Not loading gen variables. Initializing them\")\n",
    "        didRestore = False\n",
    "    else:\n",
    "        genSaver.restore(sess,latest_ckpt.model_checkpoint_path)\n",
    "        matchObj = re.match(outfilename + '-(\\d*)',latest_ckpt.model_checkpoint_path)\n",
    "        startat = int(matchObj.group(1))+1\n",
    "        print(\"Loading gen variables from %s\"%latest_ckpt.model_checkpoint_path)\n",
    "        didRestore = True\n",
    "        \n",
    "    return didRestore,startat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveGen(sess,step,genSaver,conf):\n",
    "    outfilename = os.path.join(conf.cachedir,conf.genoutname)\n",
    "    genSaver.save(sess,outfilename,global_step=step,\n",
    "               latest_filename = conf.genckptname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genFewMovedNegSamples(locs,conf,nmove=1):\n",
    "    # move few of the points randomly\n",
    "    \n",
    "    minlen = conf.gen_minlen\n",
    "    minlen = float(minlen)\n",
    "    maxlen = 2*minlen\n",
    "    \n",
    "    rlocs = copy.deepcopy(locs)\n",
    "\n",
    "    sz = conf.imsz\n",
    "    for curi in range(locs.shape[0]):\n",
    "        for curp in range(nmove):\n",
    "            rand_point = np.random.randint(conf.n_classes)\n",
    "            rx = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "            ry = np.round(np.random.rand()*(maxlen-minlen) + minlen)*\\\n",
    "                np.sign(np.random.rand()-0.5)\n",
    "\n",
    "            rlocs[curi,rand_point,0] = rlocs[curi,rand_point,0] + rx*conf.rescale*conf.pool_scale\n",
    "            rlocs[curi,rand_point,1] = rlocs[curi,rand_point,1] + ry*conf.rescale*conf.pool_scale\n",
    "    \n",
    "    # sanitize the locs\n",
    "    rlocs[rlocs<0] = 0\n",
    "    xlocs = rlocs[...,0]\n",
    "    xlocs[xlocs>=sz[1]] = sz[1]-1\n",
    "    rlocs[...,0] = xlocs\n",
    "    ylocs = rlocs[...,1]\n",
    "    ylocs[ylocs>=sz[0]] = sz[0]-1\n",
    "    rlocs[...,1] = ylocs\n",
    "    return rlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genLocs(locs,predlocs,conf):\n",
    "    dlocs = np.apply_over_axes(np.sum,(locs-predlocs)**2,axes=[1,2])\n",
    "    dlocs = np.sqrt(dlocs)/conf.n_classes\n",
    "    close = np.reshape(dlocs < (conf.gen_minlen/2),[-1])\n",
    "    newlocs = copy.deepcopy(predlocs)\n",
    "    newlocs[close,...] = genFewMovedNegSamples(newlocs[close,...],conf,nmove=3)\n",
    "    return newlocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareOpt(baseNet,l8,dbtype,feed_dict,sess,conf,phDict,distort,nsamples=10):\n",
    "    baseNet.updateFeedDict(dbtype,distort)\n",
    "    locs = baseNet.locs\n",
    "    bout = sess.run(l8,feed_dict=baseNet.feed_dict)\n",
    "    predlocs = PoseTools.getBasePredLocs(bout,conf)\n",
    "    \n",
    "    #repeat locs nsamples times\n",
    "    ls = locs.shape\n",
    "    locs = np.tile(locs[:,np.newaxis,:,:],[1,nsamples,1,1])\n",
    "    locs = np.reshape(locs,[ls[0]*nsamples,ls[1],ls[2]])\n",
    "    predlocs = np.tile(predlocs[:,np.newaxis,:,:],[1,nsamples,1,1])\n",
    "    predlocs = np.reshape(predlocs,[ls[0]*nsamples,ls[1],ls[2]])\n",
    "    \n",
    "    newlocs = genLocs(locs,predlocs,conf)\n",
    "    new_mean = newlocs.mean(axis=1)\n",
    "    \n",
    "    locs_mean = locs.mean(axis=1)\n",
    "    dlocs = locs-locs_mean[:,np.newaxis,:]\n",
    "    newlocs = newlocs-new_mean[:,np.newaxis,:]\n",
    "    \n",
    "    d_mean = locs_mean-new_mean\n",
    "    \n",
    "    scores = np.zeros(locs.shape[0:2])\n",
    "    scale = conf.rescale*conf.pool_scale\n",
    "    rlocs = (np.round(newlocs/scale)).astype('int')\n",
    "    for ndx in range(predlocs.shape[0]):\n",
    "        for cls in range(conf.n_classes):\n",
    "            bndx = int(math.floor(ndx/nsamples))\n",
    "            scores[ndx,cls] = bout[bndx,rlocs[ndx,cls,1],rlocs[ndx,cls,0],cls]\n",
    "\n",
    "    feed_dict[phDict['y']] = np.reshape(dlocs,[-1,2*conf.n_classes])\n",
    "    feed_dict[phDict['y_m']] = d_mean\n",
    "    feed_dict[phDict['scores']] = scores\n",
    "    feed_dict[phDict['locs']] = newlocs\n",
    "    return new_mean, locs_mean\n",
    "#     gg = 3/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(conf,restore=True):\n",
    "    \n",
    "    phDict = createGenPH(conf)\n",
    "    feed_dict = createFeedDict(phDict)\n",
    "    feed_dict[phDict['phase_train']] = True\n",
    "    feed_dict[phDict['dropout']] = 0.5\n",
    "    feed_dict[phDict['y']] = np.zeros((conf.batch_size,conf.n_classes*2))\n",
    "    baseNet = PoseTools.createNetwork(conf,1)\n",
    "    l8 = addDropoutLayer(baseNet,phDict['dropout'],conf)\n",
    "    with tf.variable_scope('poseGen'):\n",
    "        out,out_m,layer_dict = poseGenNet(phDict['locs'],phDict['scores'],l8,\n",
    "                                     conf,baseNet,phDict['phase_train'])\n",
    "        \n",
    "    genSaver = createGenSaver(conf)\n",
    "    y = phDict['y']\n",
    "    y_m = phDict['y_m']\n",
    "    ind_loss = tf.nn.l2_loss(out-y)/conf.n_classes\n",
    "    mean_loss = tf.nn.l2_loss(out_m-y_m)\n",
    "    loss = ind_loss + mean_loss\n",
    "    in_loss = tf.nn.l2_loss(phDict['y']-tf.reshape(phDict['locs'],[-1,2*conf.n_classes]))\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "    baseNet.openDBs()\n",
    "    baseNet.feed_dict[phDict['dropout']] = feed_dict[phDict['dropout']]\n",
    "    \n",
    "    with baseNet.env.begin() as txn,baseNet.valenv.begin() as valtxn,tf.Session() as sess:\n",
    "\n",
    "        baseNet.createCursors()\n",
    "        baseNet.restoreBase(sess,True)\n",
    "        didRestore,startat = restoreGen(sess,conf,genSaver,restore)\n",
    "        baseNet.initializeRemainingVars(sess)\n",
    "        for step in range(startat,conf.gen_training_iters+1):\n",
    "            prepareOpt(baseNet,l8,baseNet.DBType.Train,feed_dict,sess,conf,\n",
    "                       phDict,distort=True)\n",
    "            feed_dict[phDict['phase_train']] = True\n",
    "            sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "            if step % 25 == 0:\n",
    "                prepareOpt(baseNet,l8,baseNet.DBType.Train,feed_dict,\n",
    "                           sess,conf,phDict,distort=False)\n",
    "                feed_dict[phDict['phase_train']] = False\n",
    "                train_loss = sess.run([loss,in_loss,out,out_m,ind_loss,mean_loss], feed_dict=feed_dict)\n",
    "                train_mean_loss = np.sum((train_loss[3]-feed_dict[phDict['y_m']])**2 )/2\n",
    "                train_ind_loss = np.sum((train_loss[2]-feed_dict[phDict['y']])**2 )/2\n",
    "                test_loss = 0\n",
    "                test_in_loss = 0\n",
    "                test_ind_loss = 0 \n",
    "                test_mean_loss = 0 \n",
    "                \n",
    "                \n",
    "                nrep = 10\n",
    "                for rep in range(nrep):\n",
    "                    prepareOpt(baseNet,l8,baseNet.DBType.Val,feed_dict,sess,conf,\n",
    "                               phDict,distort=False)\n",
    "                    tloss = sess.run([loss,in_loss,out,out_m], feed_dict=feed_dict)\n",
    "                    test_loss += tloss[0]\n",
    "                    test_in_loss += tloss[1]\n",
    "                    test_mean_loss += np.sum((tloss[3]-feed_dict[phDict['y_m']])**2 )/2\n",
    "                    test_ind_loss += np.sum((tloss[2]-feed_dict[phDict['y']])**2 )/2\n",
    "\n",
    "                print \"Iter:{:d}, train:{:.4f},mean:{:.4f},ind:{:.4f} test:{:.4f},mean:{:.4f},ind:{:.4f} \".format(step, \n",
    "                      np.sqrt(train_loss[0]/conf.batch_size),\n",
    "                      np.sqrt(train_mean_loss/conf.batch_size),\n",
    "                      np.sqrt((train_ind_loss/conf.batch_size)/conf.n_classes),\n",
    "                      np.sqrt((test_loss/nrep)/conf.batch_size),\n",
    "                      np.sqrt((test_mean_loss/nrep)/conf.batch_size),\n",
    "                      np.sqrt(((test_ind_loss/nrep)/conf.batch_size)/conf.n_classes))\n",
    "                \n",
    "            if step % 100 == 0:\n",
    "                saveGen(sess,step,genSaver,conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
